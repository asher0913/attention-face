bash run_exp1.sh
Logging to: /home/unnc/zhang/attention-face/log/20251224_014113_slot.txt
2025-12-24 01:41:15.461750: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-24 01:41:15.504808: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-24 01:41:16.271879: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
Split Learning Scheme: Overall Cutting_layer 4/13
Total number of batches per epoch for each client is  162
64
original channel size of smashed-data is 128
added bottleneck, new channel size of smashed-data is 8
local:
Sequential(
  (0): ResizeLayer()
  (1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): ReLU(inplace=True)
  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): ReLU(inplace=True)
  (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (9): Conv2d(128, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (10): LCALayer()
  (11): Sigmoid()
)
cloud:
Sequential(
  (0): Conv2d(8, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): ReLU(inplace=True)
  (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (6): ReLU(inplace=True)
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (10): ReLU(inplace=True)
  (11): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (12): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (13): ReLU(inplace=True)
  (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (17): ReLU(inplace=True)
  (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (20): ReLU(inplace=True)
  (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
classifier:
Sequential(
  (0): Dropout(p=0.5, inplace=False)
  (1): Linear(in_features=512, out_features=512, bias=True)
  (2): ReLU(inplace=True)
  (3): Dropout(p=0.5, inplace=False)
  (4): Linear(in_features=512, out_features=512, bias=True)
  (5): ReLU(inplace=True)
  (6): Linear(in_features=512, out_features=530, bias=True)
)
Namespace(arch='vgg11_bn_sgm', cutlayer=4, batch_size=256, filename='pretrain_False_lambd_16_noise_0.025_epoch_240_bottleneck_noRELU_C8S1_log_1_ATstrength_0.3_lr_0.05_varthres_0.125', folder='saves/facescrub/SCA_new_slotatt_opt_lg1_thre0.125', num_client=1, num_epochs=240, learning_rate=0.05, lambd=16.0, dataset_portion=1.0, client_sample_ratio=1.0, noniid=1.0, local_lr=-1.0, dataset='facescrub', scheme='V2_epoch', regularization='Gaussian_kl', regularization_strength=0.025, var_threshold=0.125, AT_regularization='SCA_new', AT_regularization_strength=0.3, log_entropy=1.0, ssim_threshold=0.5, gan_AE_type='res_normN4C64', gan_loss_type='SSIM', bottleneck_option='noRELU_C8S1', optimize_computation=1, decoder_sync=False, load_from_checkpoint=False, load_from_checkpoint_server=False, transfer_source_task='cifar100', finetune_freeze_bn=False, save_more_checkpoints=False, initialize_different=False, random_seed=125)
Model's smashed-data size is torch.Size([1, 8, 16, 16])
Real Train Phase: done by all clients, for total 240 epochs
GAN training interval N (once every N step) is set to 1!
Train in V2_epoch style
log--[1/240][0/162][client-0] train loss: 6.2841 cross-entropy loss: 6.2841
log--[1/240][161/162][client-0] train loss: 6.2907 cross-entropy loss: 6.2907
Epoch 0	Test (client-0):	Loss 6.2786 (6.2786)	Prec@1 0.000 (0.000)
 * Prec@1 0.208
best model saved at: 1
lambd value is: 0.32 learning rate is: 0.05
/home/unnc/miniconda3/lib/python3.13/site-packages/torch/optim/lr_scheduler.py:209: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Train in V2_epoch style
log--[2/240][0/162][client-0] train loss: 6.3034 cross-entropy loss: 6.3034
log--[2/240][161/162][client-0] train loss: 6.1282 cross-entropy loss: 6.1282
Epoch 0	Test (client-0):	Loss 6.5895 (6.5895)	Prec@1 0.000 (0.000)
 * Prec@1 0.415
best model saved at: 2
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[3/240][0/162][client-0] train loss: 5.7545 cross-entropy loss: 5.7545
log--[3/240][161/162][client-0] train loss: 5.4719 cross-entropy loss: 5.4719
Epoch 0	Test (client-0):	Loss 5.4904 (5.4904)	Prec@1 0.781 (0.781)
 * Prec@1 1.569
best model saved at: 3
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[4/240][0/162][client-0] train loss: 5.3862 cross-entropy loss: 5.3862
log--[4/240][161/162][client-0] train loss: 5.2422 cross-entropy loss: 5.2422
Epoch 0	Test (client-0):	Loss 5.0366 (5.0366)	Prec@1 0.000 (0.000)
 * Prec@1 2.030
best model saved at: 4
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[5/240][0/162][client-0] train loss: 5.1542 cross-entropy loss: 5.1542
log--[5/240][161/162][client-0] train loss: 5.0834 cross-entropy loss: 5.0834
Epoch 0	Test (client-0):	Loss 4.9966 (4.9966)	Prec@1 3.125 (3.125)
 * Prec@1 2.537
best model saved at: 5
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[6/240][0/162][client-0] train loss: 4.9049 cross-entropy loss: 4.9049
log--[6/240][161/162][client-0] train loss: 4.9028 cross-entropy loss: 4.9028
Epoch 0	Test (client-0):	Loss 4.5517 (4.5517)	Prec@1 8.594 (8.594)
 * Prec@1 3.599
best model saved at: 6
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[7/240][0/162][client-0] train loss: 4.8658 cross-entropy loss: 4.8658
log--[7/240][161/162][client-0] train loss: 4.7172 cross-entropy loss: 4.7172
Epoch 0	Test (client-0):	Loss 4.4891 (4.4891)	Prec@1 2.734 (2.734)
 * Prec@1 4.544
best model saved at: 7
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[8/240][0/162][client-0] train loss: 4.6889 cross-entropy loss: 4.6889
log--[8/240][161/162][client-0] train loss: 4.4543 cross-entropy loss: 4.4543
Epoch 0	Test (client-0):	Loss 4.1324 (4.1324)	Prec@1 11.328 (11.328)
 * Prec@1 9.458
best model saved at: 8
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[9/240][0/162][client-0] train loss: 4.2822 cross-entropy loss: 4.2822
log--[9/240][161/162][client-0] train loss: 4.2083 cross-entropy loss: 4.2083
Epoch 0	Test (client-0):	Loss 4.2983 (4.2983)	Prec@1 9.375 (9.375)
 * Prec@1 5.790
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[10/240][0/162][client-0] train loss: 4.0640 cross-entropy loss: 4.0640
log--[10/240][161/162][client-0] train loss: 4.0616 cross-entropy loss: 4.0616
Epoch 0	Test (client-0):	Loss 4.3073 (4.3073)	Prec@1 8.203 (8.203)
 * Prec@1 10.727
best model saved at: 10
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[11/240][0/162][client-0] train loss: 3.9098 cross-entropy loss: 3.9098
log--[11/240][161/162][client-0] train loss: 3.9093 cross-entropy loss: 3.9093
Epoch 0	Test (client-0):	Loss 3.6450 (3.6450)	Prec@1 16.797 (16.797)
 * Prec@1 11.811
best model saved at: 11
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[12/240][0/162][client-0] train loss: 3.8987 cross-entropy loss: 3.8987
log--[12/240][161/162][client-0] train loss: 3.7891 cross-entropy loss: 3.7891
Epoch 0	Test (client-0):	Loss 3.5937 (3.5937)	Prec@1 18.359 (18.359)
 * Prec@1 16.471
best model saved at: 12
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[13/240][0/162][client-0] train loss: 3.7920 cross-entropy loss: 3.7920
log--[13/240][161/162][client-0] train loss: 3.6957 cross-entropy loss: 3.6957
Epoch 0	Test (client-0):	Loss 3.4312 (3.4312)	Prec@1 24.219 (24.219)
 * Prec@1 16.563
best model saved at: 13
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[14/240][0/162][client-0] train loss: 3.5879 cross-entropy loss: 3.5879
log--[14/240][161/162][client-0] train loss: 3.5938 cross-entropy loss: 3.5938
Epoch 0	Test (client-0):	Loss 3.4304 (3.4304)	Prec@1 23.828 (23.828)
 * Prec@1 16.494
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[15/240][0/162][client-0] train loss: 3.2835 cross-entropy loss: 3.2835
log--[15/240][161/162][client-0] train loss: 3.5107 cross-entropy loss: 3.5107
Epoch 0	Test (client-0):	Loss 3.2233 (3.2233)	Prec@1 23.828 (23.828)
 * Prec@1 19.285
best model saved at: 15
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[16/240][0/162][client-0] train loss: 3.3798 cross-entropy loss: 3.3798
log--[16/240][161/162][client-0] train loss: 3.4222 cross-entropy loss: 3.4222
Epoch 0	Test (client-0):	Loss 3.4242 (3.4242)	Prec@1 21.875 (21.875)
 * Prec@1 19.077
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[17/240][0/162][client-0] train loss: 3.3143 cross-entropy loss: 3.3143
log--[17/240][161/162][client-0] train loss: 3.3274 cross-entropy loss: 3.3274
Epoch 0	Test (client-0):	Loss 3.0661 (3.0661)	Prec@1 24.609 (24.609)
 * Prec@1 20.208
best model saved at: 17
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[18/240][0/162][client-0] train loss: 3.1460 cross-entropy loss: 3.1460
log--[18/240][161/162][client-0] train loss: 3.2540 cross-entropy loss: 3.2540
Epoch 0	Test (client-0):	Loss 2.8306 (2.8306)	Prec@1 30.078 (30.078)
 * Prec@1 25.444
best model saved at: 18
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[19/240][0/162][client-0] train loss: 3.2366 cross-entropy loss: 3.2366
log--[19/240][161/162][client-0] train loss: 3.1814 cross-entropy loss: 3.1814
Epoch 0	Test (client-0):	Loss 2.6753 (2.6753)	Prec@1 32.812 (32.812)
 * Prec@1 25.398
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[20/240][0/162][client-0] train loss: 2.9654 cross-entropy loss: 2.9654
log--[20/240][161/162][client-0] train loss: 3.1164 cross-entropy loss: 3.1164
Epoch 0	Test (client-0):	Loss 2.7105 (2.7105)	Prec@1 28.906 (28.906)
 * Prec@1 29.366
best model saved at: 20
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[21/240][0/162][client-0] train loss: 2.7496 cross-entropy loss: 2.7496
log--[21/240][161/162][client-0] train loss: 3.0289 cross-entropy loss: 3.0289
Epoch 0	Test (client-0):	Loss 2.7091 (2.7091)	Prec@1 33.594 (33.594)
 * Prec@1 28.604
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[22/240][0/162][client-0] train loss: 2.9372 cross-entropy loss: 2.9372
log--[22/240][161/162][client-0] train loss: 2.9552 cross-entropy loss: 2.9552
Epoch 0	Test (client-0):	Loss 2.6504 (2.6504)	Prec@1 36.328 (36.328)
 * Prec@1 28.835
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[23/240][0/162][client-0] train loss: 2.9776 cross-entropy loss: 2.9776
log--[23/240][161/162][client-0] train loss: 2.8952 cross-entropy loss: 2.8952
Epoch 0	Test (client-0):	Loss 2.7078 (2.7078)	Prec@1 31.250 (31.250)
 * Prec@1 32.295
best model saved at: 23
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[24/240][0/162][client-0] train loss: 2.8851 cross-entropy loss: 2.8851
log--[24/240][161/162][client-0] train loss: 2.8248 cross-entropy loss: 2.8248
Epoch 0	Test (client-0):	Loss 2.4452 (2.4452)	Prec@1 34.766 (34.766)
 * Prec@1 33.795
best model saved at: 24
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[25/240][0/162][client-0] train loss: 2.7850 cross-entropy loss: 2.7850
log--[25/240][161/162][client-0] train loss: 2.7637 cross-entropy loss: 2.7637
Epoch 0	Test (client-0):	Loss 2.2633 (2.2633)	Prec@1 44.141 (44.141)
 * Prec@1 37.116
best model saved at: 25
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[26/240][0/162][client-0] train loss: 2.4962 cross-entropy loss: 2.4962
log--[26/240][161/162][client-0] train loss: 2.6809 cross-entropy loss: 2.6809
Epoch 0	Test (client-0):	Loss 2.8642 (2.8642)	Prec@1 33.203 (33.203)
 * Prec@1 28.328
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[27/240][0/162][client-0] train loss: 2.5130 cross-entropy loss: 2.5130
log--[27/240][161/162][client-0] train loss: 2.6236 cross-entropy loss: 2.6236
Epoch 0	Test (client-0):	Loss 2.8197 (2.8197)	Prec@1 35.547 (35.547)
 * Prec@1 30.265
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[28/240][0/162][client-0] train loss: 2.4280 cross-entropy loss: 2.4280
log--[28/240][161/162][client-0] train loss: 2.5669 cross-entropy loss: 2.5669
Epoch 0	Test (client-0):	Loss 2.3264 (2.3264)	Prec@1 44.922 (44.922)
 * Prec@1 41.984
best model saved at: 28
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[29/240][0/162][client-0] train loss: 2.4715 cross-entropy loss: 2.4715
log--[29/240][161/162][client-0] train loss: 2.4861 cross-entropy loss: 2.4861
Epoch 0	Test (client-0):	Loss 2.2338 (2.2338)	Prec@1 48.047 (48.047)
 * Prec@1 40.392
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[30/240][0/162][client-0] train loss: 2.1727 cross-entropy loss: 2.1727
log--[30/240][161/162][client-0] train loss: 2.4295 cross-entropy loss: 2.4295
Epoch 0	Test (client-0):	Loss 2.3410 (2.3410)	Prec@1 42.969 (42.969)
 * Prec@1 36.217
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[31/240][0/162][client-0] train loss: 2.4171 cross-entropy loss: 2.4171
log--[31/240][161/162][client-0] train loss: 2.3783 cross-entropy loss: 2.3783
Epoch 0	Test (client-0):	Loss 2.1913 (2.1913)	Prec@1 47.266 (47.266)
 * Prec@1 39.146
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[32/240][0/162][client-0] train loss: 2.3531 cross-entropy loss: 2.3531
log--[32/240][161/162][client-0] train loss: 2.3165 cross-entropy loss: 2.3165
Epoch 0	Test (client-0):	Loss 2.1214 (2.1214)	Prec@1 48.438 (48.438)
 * Prec@1 43.737
best model saved at: 32
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[33/240][0/162][client-0] train loss: 2.0898 cross-entropy loss: 2.0898
log--[33/240][161/162][client-0] train loss: 2.2723 cross-entropy loss: 2.2723
Epoch 0	Test (client-0):	Loss 2.3577 (2.3577)	Prec@1 42.578 (42.578)
 * Prec@1 41.200
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[34/240][0/162][client-0] train loss: 2.3501 cross-entropy loss: 2.3501
log--[34/240][161/162][client-0] train loss: 2.2393 cross-entropy loss: 2.2393
Epoch 0	Test (client-0):	Loss 1.7647 (1.7647)	Prec@1 56.250 (56.250)
 * Prec@1 48.397
best model saved at: 34
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[35/240][0/162][client-0] train loss: 1.9671 cross-entropy loss: 1.9671
log--[35/240][161/162][client-0] train loss: 2.1827 cross-entropy loss: 2.1827
Epoch 0	Test (client-0):	Loss 1.8890 (1.8890)	Prec@1 58.594 (58.594)
 * Prec@1 47.474
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[36/240][0/162][client-0] train loss: 2.0660 cross-entropy loss: 2.0660
log--[36/240][161/162][client-0] train loss: 2.1456 cross-entropy loss: 2.1456
Epoch 0	Test (client-0):	Loss 2.3495 (2.3495)	Prec@1 44.922 (44.922)
 * Prec@1 41.638
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[37/240][0/162][client-0] train loss: 2.1527 cross-entropy loss: 2.1527
log--[37/240][161/162][client-0] train loss: 2.1175 cross-entropy loss: 2.1175
Epoch 0	Test (client-0):	Loss 1.9537 (1.9537)	Prec@1 51.562 (51.562)
 * Prec@1 47.082
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[38/240][0/162][client-0] train loss: 1.9444 cross-entropy loss: 1.9444
log--[38/240][161/162][client-0] train loss: 2.0890 cross-entropy loss: 2.0890
Epoch 0	Test (client-0):	Loss 2.1215 (2.1215)	Prec@1 43.359 (43.359)
 * Prec@1 44.544
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[39/240][0/162][client-0] train loss: 1.9737 cross-entropy loss: 1.9737
log--[39/240][161/162][client-0] train loss: 2.0615 cross-entropy loss: 2.0615
Epoch 0	Test (client-0):	Loss 2.2350 (2.2350)	Prec@1 44.922 (44.922)
 * Prec@1 44.821
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[40/240][0/162][client-0] train loss: 1.9810 cross-entropy loss: 1.9810
log--[40/240][161/162][client-0] train loss: 2.0270 cross-entropy loss: 2.0270
Epoch 0	Test (client-0):	Loss 1.8832 (1.8832)	Prec@1 52.344 (52.344)
 * Prec@1 50.819
best model saved at: 40
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[41/240][0/162][client-0] train loss: 1.9456 cross-entropy loss: 1.9456
log--[41/240][161/162][client-0] train loss: 2.0112 cross-entropy loss: 2.0112
Epoch 0	Test (client-0):	Loss 1.7742 (1.7742)	Prec@1 58.594 (58.594)
 * Prec@1 52.549
best model saved at: 41
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[42/240][0/162][client-0] train loss: 1.7843 cross-entropy loss: 1.7843
log--[42/240][161/162][client-0] train loss: 1.9754 cross-entropy loss: 1.9754
Epoch 0	Test (client-0):	Loss 1.9371 (1.9371)	Prec@1 55.859 (55.859)
 * Prec@1 46.667
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[43/240][0/162][client-0] train loss: 1.9480 cross-entropy loss: 1.9480
log--[43/240][161/162][client-0] train loss: 1.9507 cross-entropy loss: 1.9507
Epoch 0	Test (client-0):	Loss 2.2537 (2.2537)	Prec@1 45.703 (45.703)
 * Prec@1 42.745
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[44/240][0/162][client-0] train loss: 1.8515 cross-entropy loss: 1.8515
log--[44/240][161/162][client-0] train loss: 1.9293 cross-entropy loss: 1.9293
Epoch 0	Test (client-0):	Loss 1.9982 (1.9982)	Prec@1 51.172 (51.172)
 * Prec@1 47.612
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[45/240][0/162][client-0] train loss: 1.8120 cross-entropy loss: 1.8120
log--[45/240][161/162][client-0] train loss: 1.8995 cross-entropy loss: 1.8995
Epoch 0	Test (client-0):	Loss 1.9074 (1.9074)	Prec@1 57.031 (57.031)
 * Prec@1 51.257
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[46/240][0/162][client-0] train loss: 2.0711 cross-entropy loss: 2.0711
log--[46/240][161/162][client-0] train loss: 1.8822 cross-entropy loss: 1.8822
Epoch 0	Test (client-0):	Loss 2.1772 (2.1772)	Prec@1 47.266 (47.266)
 * Prec@1 45.675
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[47/240][0/162][client-0] train loss: 1.7776 cross-entropy loss: 1.7776
log--[47/240][161/162][client-0] train loss: 1.8581 cross-entropy loss: 1.8581
Epoch 0	Test (client-0):	Loss 2.3534 (2.3534)	Prec@1 46.094 (46.094)
 * Prec@1 45.582
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[48/240][0/162][client-0] train loss: 1.6759 cross-entropy loss: 1.6759
log--[48/240][161/162][client-0] train loss: 1.8480 cross-entropy loss: 1.8480
Epoch 0	Test (client-0):	Loss 1.7650 (1.7650)	Prec@1 60.938 (60.938)
 * Prec@1 51.603
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[49/240][0/162][client-0] train loss: 1.8328 cross-entropy loss: 1.8328
log--[49/240][161/162][client-0] train loss: 1.8144 cross-entropy loss: 1.8144
Epoch 0	Test (client-0):	Loss 1.7367 (1.7367)	Prec@1 57.031 (57.031)
 * Prec@1 48.950
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[50/240][0/162][client-0] train loss: 1.6869 cross-entropy loss: 1.6869
log--[50/240][161/162][client-0] train loss: 1.7737 cross-entropy loss: 1.7737
Epoch 0	Test (client-0):	Loss 1.8396 (1.8396)	Prec@1 56.641 (56.641)
 * Prec@1 48.074
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[51/240][0/162][client-0] train loss: 1.7839 cross-entropy loss: 1.7839
log--[51/240][161/162][client-0] train loss: 1.7699 cross-entropy loss: 1.7699
Epoch 0	Test (client-0):	Loss 1.8310 (1.8310)	Prec@1 57.422 (57.422)
 * Prec@1 52.226
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[52/240][0/162][client-0] train loss: 1.5697 cross-entropy loss: 1.5697
log--[52/240][161/162][client-0] train loss: 1.7343 cross-entropy loss: 1.7343
Epoch 0	Test (client-0):	Loss 1.6297 (1.6297)	Prec@1 60.156 (60.156)
 * Prec@1 53.010
best model saved at: 52
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[53/240][0/162][client-0] train loss: 1.7601 cross-entropy loss: 1.7601
log--[53/240][161/162][client-0] train loss: 1.7197 cross-entropy loss: 1.7197
Epoch 0	Test (client-0):	Loss 2.3166 (2.3166)	Prec@1 47.266 (47.266)
 * Prec@1 43.322
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[54/240][0/162][client-0] train loss: 1.7005 cross-entropy loss: 1.7005
log--[54/240][161/162][client-0] train loss: 1.6936 cross-entropy loss: 1.6936
Epoch 0	Test (client-0):	Loss 1.5039 (1.5039)	Prec@1 62.891 (62.891)
 * Prec@1 56.194
best model saved at: 54
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[55/240][0/162][client-0] train loss: 1.6033 cross-entropy loss: 1.6033
log--[55/240][161/162][client-0] train loss: 1.6711 cross-entropy loss: 1.6711
Epoch 0	Test (client-0):	Loss 1.9579 (1.9579)	Prec@1 55.469 (55.469)
 * Prec@1 52.318
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[56/240][0/162][client-0] train loss: 1.5254 cross-entropy loss: 1.5254
log--[56/240][161/162][client-0] train loss: 1.6602 cross-entropy loss: 1.6602
Epoch 0	Test (client-0):	Loss 1.6641 (1.6641)	Prec@1 58.594 (58.594)
 * Prec@1 55.617
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[57/240][0/162][client-0] train loss: 1.5905 cross-entropy loss: 1.5905
log--[57/240][161/162][client-0] train loss: 1.6457 cross-entropy loss: 1.6457
Epoch 0	Test (client-0):	Loss 1.7473 (1.7473)	Prec@1 59.766 (59.766)
 * Prec@1 54.948
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[58/240][0/162][client-0] train loss: 1.3831 cross-entropy loss: 1.3831
log--[58/240][161/162][client-0] train loss: 1.6275 cross-entropy loss: 1.6275
Epoch 0	Test (client-0):	Loss 1.6479 (1.6479)	Prec@1 60.156 (60.156)
 * Prec@1 53.080
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[59/240][0/162][client-0] train loss: 1.4643 cross-entropy loss: 1.4643
log--[59/240][161/162][client-0] train loss: 1.5879 cross-entropy loss: 1.5879
Epoch 0	Test (client-0):	Loss 1.8659 (1.8659)	Prec@1 60.547 (60.547)
 * Prec@1 50.957
lambd value is: 0.32 learning rate is: 0.05
Train in V2_epoch style
log--[60/240][0/162][client-0] train loss: 1.5349 cross-entropy loss: 1.5349
log--[60/240][161/162][client-0] train loss: 1.1226 cross-entropy loss: 1.1226
Epoch 0	Test (client-0):	Loss 0.9925 (0.9925)	Prec@1 76.172 (76.172)
 * Prec@1 71.050
best model saved at: 60
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[61/240][0/162][client-0] train loss: 1.1462 cross-entropy loss: 1.1462
log--[61/240][161/162][client-0] train loss: 0.9397 cross-entropy loss: 0.9397
Epoch 0	Test (client-0):	Loss 0.9284 (0.9284)	Prec@1 78.125 (78.125)
 * Prec@1 72.203
best model saved at: 61
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[62/240][0/162][client-0] train loss: 0.8812 cross-entropy loss: 0.8812
log--[62/240][161/162][client-0] train loss: 0.8735 cross-entropy loss: 0.8735
Epoch 0	Test (client-0):	Loss 0.9517 (0.9517)	Prec@1 78.125 (78.125)
 * Prec@1 71.511
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[63/240][0/162][client-0] train loss: 0.8700 cross-entropy loss: 0.8700
log--[63/240][161/162][client-0] train loss: 0.8221 cross-entropy loss: 0.8221
Epoch 0	Test (client-0):	Loss 0.9947 (0.9947)	Prec@1 77.344 (77.344)
 * Prec@1 71.626
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[64/240][0/162][client-0] train loss: 0.6882 cross-entropy loss: 0.6882
log--[64/240][161/162][client-0] train loss: 0.7846 cross-entropy loss: 0.7846
Epoch 0	Test (client-0):	Loss 0.9217 (0.9217)	Prec@1 80.078 (80.078)
 * Prec@1 72.226
best model saved at: 64
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[65/240][0/162][client-0] train loss: 0.7481 cross-entropy loss: 0.7481
log--[65/240][161/162][client-0] train loss: 0.7569 cross-entropy loss: 0.7569
Epoch 0	Test (client-0):	Loss 0.9658 (0.9658)	Prec@1 78.906 (78.906)
 * Prec@1 72.803
best model saved at: 65
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[66/240][0/162][client-0] train loss: 0.6774 cross-entropy loss: 0.6774
log--[66/240][161/162][client-0] train loss: 0.7185 cross-entropy loss: 0.7185
Epoch 0	Test (client-0):	Loss 1.0070 (1.0070)	Prec@1 77.734 (77.734)
 * Prec@1 71.626
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[67/240][0/162][client-0] train loss: 0.6508 cross-entropy loss: 0.6508
log--[67/240][161/162][client-0] train loss: 0.6906 cross-entropy loss: 0.6906
Epoch 0	Test (client-0):	Loss 0.9684 (0.9684)	Prec@1 79.297 (79.297)
 * Prec@1 72.249
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[68/240][0/162][client-0] train loss: 0.6953 cross-entropy loss: 0.6953
log--[68/240][161/162][client-0] train loss: 0.6638 cross-entropy loss: 0.6638
Epoch 0	Test (client-0):	Loss 1.0105 (1.0105)	Prec@1 78.125 (78.125)
 * Prec@1 71.972
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[69/240][0/162][client-0] train loss: 0.6075 cross-entropy loss: 0.6075
log--[69/240][161/162][client-0] train loss: 0.6333 cross-entropy loss: 0.6333
Epoch 0	Test (client-0):	Loss 1.0042 (1.0042)	Prec@1 80.078 (80.078)
 * Prec@1 72.641
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[70/240][0/162][client-0] train loss: 0.5048 cross-entropy loss: 0.5048
log--[70/240][161/162][client-0] train loss: 0.6123 cross-entropy loss: 0.6123
Epoch 0	Test (client-0):	Loss 0.9093 (0.9093)	Prec@1 79.688 (79.688)
 * Prec@1 73.310
best model saved at: 70
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[71/240][0/162][client-0] train loss: 0.5606 cross-entropy loss: 0.5606
log--[71/240][161/162][client-0] train loss: 0.5974 cross-entropy loss: 0.5974
Epoch 0	Test (client-0):	Loss 1.1062 (1.1062)	Prec@1 77.344 (77.344)
 * Prec@1 72.457
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[72/240][0/162][client-0] train loss: 0.5252 cross-entropy loss: 0.5252
log--[72/240][161/162][client-0] train loss: 0.5716 cross-entropy loss: 0.5716
Epoch 0	Test (client-0):	Loss 1.1132 (1.1132)	Prec@1 80.469 (80.469)
 * Prec@1 71.465
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[73/240][0/162][client-0] train loss: 0.4777 cross-entropy loss: 0.4777
log--[73/240][161/162][client-0] train loss: 0.5528 cross-entropy loss: 0.5528
Epoch 0	Test (client-0):	Loss 1.1611 (1.1611)	Prec@1 76.172 (76.172)
 * Prec@1 71.488
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[74/240][0/162][client-0] train loss: 0.4423 cross-entropy loss: 0.4423
log--[74/240][161/162][client-0] train loss: 0.5418 cross-entropy loss: 0.5418
Epoch 0	Test (client-0):	Loss 1.0019 (1.0019)	Prec@1 80.469 (80.469)
 * Prec@1 72.042
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[75/240][0/162][client-0] train loss: 0.5265 cross-entropy loss: 0.5265
log--[75/240][161/162][client-0] train loss: 0.5253 cross-entropy loss: 0.5253
Epoch 0	Test (client-0):	Loss 1.1244 (1.1244)	Prec@1 80.078 (80.078)
 * Prec@1 72.595
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[76/240][0/162][client-0] train loss: 0.5346 cross-entropy loss: 0.5346
log--[76/240][161/162][client-0] train loss: 0.5047 cross-entropy loss: 0.5047
Epoch 0	Test (client-0):	Loss 1.0789 (1.0789)	Prec@1 80.078 (80.078)
 * Prec@1 70.773
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[77/240][0/162][client-0] train loss: 0.4786 cross-entropy loss: 0.4786
log--[77/240][161/162][client-0] train loss: 0.5064 cross-entropy loss: 0.5064
Epoch 0	Test (client-0):	Loss 1.1908 (1.1908)	Prec@1 77.344 (77.344)
 * Prec@1 70.888
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[78/240][0/162][client-0] train loss: 0.4873 cross-entropy loss: 0.4873
log--[78/240][161/162][client-0] train loss: 0.4811 cross-entropy loss: 0.4811
Epoch 0	Test (client-0):	Loss 1.2907 (1.2907)	Prec@1 76.953 (76.953)
 * Prec@1 71.050
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[79/240][0/162][client-0] train loss: 0.4952 cross-entropy loss: 0.4952
log--[79/240][161/162][client-0] train loss: 0.4733 cross-entropy loss: 0.4733
Epoch 0	Test (client-0):	Loss 1.2651 (1.2651)	Prec@1 73.047 (73.047)
 * Prec@1 71.534
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[80/240][0/162][client-0] train loss: 0.4943 cross-entropy loss: 0.4943
log--[80/240][161/162][client-0] train loss: 0.4541 cross-entropy loss: 0.4541
Epoch 0	Test (client-0):	Loss 1.1230 (1.1230)	Prec@1 76.953 (76.953)
 * Prec@1 71.834
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[81/240][0/162][client-0] train loss: 0.4182 cross-entropy loss: 0.4182
log--[81/240][161/162][client-0] train loss: 0.4454 cross-entropy loss: 0.4454
Epoch 0	Test (client-0):	Loss 1.2281 (1.2281)	Prec@1 76.953 (76.953)
 * Prec@1 71.742
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[82/240][0/162][client-0] train loss: 0.3623 cross-entropy loss: 0.3623
log--[82/240][161/162][client-0] train loss: 0.4226 cross-entropy loss: 0.4226
Epoch 0	Test (client-0):	Loss 1.2128 (1.2128)	Prec@1 78.125 (78.125)
 * Prec@1 69.619
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[83/240][0/162][client-0] train loss: 0.5692 cross-entropy loss: 0.5692
log--[83/240][161/162][client-0] train loss: 0.4170 cross-entropy loss: 0.4170
Epoch 0	Test (client-0):	Loss 1.2134 (1.2134)	Prec@1 76.953 (76.953)
 * Prec@1 70.681
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[84/240][0/162][client-0] train loss: 0.2990 cross-entropy loss: 0.2990
log--[84/240][161/162][client-0] train loss: 0.4100 cross-entropy loss: 0.4100
Epoch 0	Test (client-0):	Loss 1.1393 (1.1393)	Prec@1 77.734 (77.734)
 * Prec@1 72.065
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[85/240][0/162][client-0] train loss: 0.2325 cross-entropy loss: 0.2325
log--[85/240][161/162][client-0] train loss: 0.3964 cross-entropy loss: 0.3964
Epoch 0	Test (client-0):	Loss 1.1080 (1.1080)	Prec@1 78.906 (78.906)
 * Prec@1 72.272
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[86/240][0/162][client-0] train loss: 0.3159 cross-entropy loss: 0.3159
log--[86/240][161/162][client-0] train loss: 0.3897 cross-entropy loss: 0.3897
Epoch 0	Test (client-0):	Loss 1.2358 (1.2358)	Prec@1 75.000 (75.000)
 * Prec@1 70.980
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[87/240][0/162][client-0] train loss: 0.3004 cross-entropy loss: 0.3004
log--[87/240][161/162][client-0] train loss: 0.3749 cross-entropy loss: 0.3749
Epoch 0	Test (client-0):	Loss 1.3002 (1.3002)	Prec@1 76.953 (76.953)
 * Prec@1 71.188
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[88/240][0/162][client-0] train loss: 0.2981 cross-entropy loss: 0.2981
log--[88/240][161/162][client-0] train loss: 0.3695 cross-entropy loss: 0.3695
Epoch 0	Test (client-0):	Loss 1.1477 (1.1477)	Prec@1 78.516 (78.516)
 * Prec@1 71.626
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[89/240][0/162][client-0] train loss: 0.3303 cross-entropy loss: 0.3303
log--[89/240][161/162][client-0] train loss: 0.3568 cross-entropy loss: 0.3568
Epoch 0	Test (client-0):	Loss 1.2508 (1.2508)	Prec@1 77.734 (77.734)
 * Prec@1 70.565
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[90/240][0/162][client-0] train loss: 0.4568 cross-entropy loss: 0.4568
log--[90/240][161/162][client-0] train loss: 0.3539 cross-entropy loss: 0.3539
Epoch 0	Test (client-0):	Loss 1.1902 (1.1902)	Prec@1 78.516 (78.516)
 * Prec@1 70.911
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[91/240][0/162][client-0] train loss: 0.3246 cross-entropy loss: 0.3246
log--[91/240][161/162][client-0] train loss: 0.3447 cross-entropy loss: 0.3447
Epoch 0	Test (client-0):	Loss 1.4693 (1.4693)	Prec@1 76.562 (76.562)
 * Prec@1 71.765
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[92/240][0/162][client-0] train loss: 0.2976 cross-entropy loss: 0.2976
log--[92/240][161/162][client-0] train loss: 0.3411 cross-entropy loss: 0.3411
Epoch 0	Test (client-0):	Loss 1.1321 (1.1321)	Prec@1 80.469 (80.469)
 * Prec@1 71.765
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[93/240][0/162][client-0] train loss: 0.3065 cross-entropy loss: 0.3065
log--[93/240][161/162][client-0] train loss: 0.3278 cross-entropy loss: 0.3278
Epoch 0	Test (client-0):	Loss 1.2596 (1.2596)	Prec@1 76.953 (76.953)
 * Prec@1 70.542
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[94/240][0/162][client-0] train loss: 0.2642 cross-entropy loss: 0.2642
log--[94/240][161/162][client-0] train loss: 0.3053 cross-entropy loss: 0.3053
Epoch 0	Test (client-0):	Loss 1.5621 (1.5621)	Prec@1 74.219 (74.219)
 * Prec@1 68.420
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[95/240][0/162][client-0] train loss: 0.3760 cross-entropy loss: 0.3760
log--[95/240][161/162][client-0] train loss: 0.3131 cross-entropy loss: 0.3131
Epoch 0	Test (client-0):	Loss 1.3405 (1.3405)	Prec@1 78.516 (78.516)
 * Prec@1 71.834
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[96/240][0/162][client-0] train loss: 0.2378 cross-entropy loss: 0.2378
log--[96/240][161/162][client-0] train loss: 0.3093 cross-entropy loss: 0.3093
Epoch 0	Test (client-0):	Loss 1.0115 (1.0115)	Prec@1 82.812 (82.812)
 * Prec@1 73.080
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[97/240][0/162][client-0] train loss: 0.3199 cross-entropy loss: 0.3199
log--[97/240][161/162][client-0] train loss: 0.2984 cross-entropy loss: 0.2984
Epoch 0	Test (client-0):	Loss 1.4206 (1.4206)	Prec@1 75.000 (75.000)
 * Prec@1 71.649
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[98/240][0/162][client-0] train loss: 0.2310 cross-entropy loss: 0.2310
log--[98/240][161/162][client-0] train loss: 0.2912 cross-entropy loss: 0.2912
Epoch 0	Test (client-0):	Loss 1.1987 (1.1987)	Prec@1 78.125 (78.125)
 * Prec@1 71.765
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[99/240][0/162][client-0] train loss: 0.2515 cross-entropy loss: 0.2515
log--[99/240][161/162][client-0] train loss: 0.2949 cross-entropy loss: 0.2949
Epoch 0	Test (client-0):	Loss 1.0795 (1.0795)	Prec@1 80.469 (80.469)
 * Prec@1 71.534
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[100/240][0/162][client-0] train loss: 0.2556 cross-entropy loss: 0.2556
log--[100/240][161/162][client-0] train loss: 0.2857 cross-entropy loss: 0.2857
Epoch 0	Test (client-0):	Loss 1.2667 (1.2667)	Prec@1 76.953 (76.953)
 * Prec@1 70.012
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[101/240][0/162][client-0] train loss: 0.3022 cross-entropy loss: 0.3022
log--[101/240][161/162][client-0] train loss: 0.2775 cross-entropy loss: 0.2775
Epoch 0	Test (client-0):	Loss 1.1046 (1.1046)	Prec@1 77.734 (77.734)
 * Prec@1 71.557
best model saved at: 101
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[102/240][0/162][client-0] train loss: 0.2642 cross-entropy loss: 0.2642
log--[102/240][161/162][client-0] train loss: 0.2787 cross-entropy loss: 0.2787
Epoch 0	Test (client-0):	Loss 1.3064 (1.3064)	Prec@1 79.297 (79.297)
 * Prec@1 70.104
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[103/240][0/162][client-0] train loss: 0.1823 cross-entropy loss: 0.1823
log--[103/240][161/162][client-0] train loss: 0.2704 cross-entropy loss: 0.2704
Epoch 0	Test (client-0):	Loss 1.2319 (1.2319)	Prec@1 77.734 (77.734)
 * Prec@1 71.834
best model saved at: 103
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[104/240][0/162][client-0] train loss: 0.1788 cross-entropy loss: 0.1788
log--[104/240][161/162][client-0] train loss: 0.2743 cross-entropy loss: 0.2743
Epoch 0	Test (client-0):	Loss 1.0964 (1.0964)	Prec@1 79.297 (79.297)
 * Prec@1 71.396
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[105/240][0/162][client-0] train loss: 0.1969 cross-entropy loss: 0.1969
log--[105/240][161/162][client-0] train loss: 0.2565 cross-entropy loss: 0.2565
Epoch 0	Test (client-0):	Loss 1.1465 (1.1465)	Prec@1 78.125 (78.125)
 * Prec@1 72.272
best model saved at: 105
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[106/240][0/162][client-0] train loss: 0.2108 cross-entropy loss: 0.2108
log--[106/240][161/162][client-0] train loss: 0.2557 cross-entropy loss: 0.2557
Epoch 0	Test (client-0):	Loss 1.0764 (1.0764)	Prec@1 79.688 (79.688)
 * Prec@1 72.088
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[107/240][0/162][client-0] train loss: 0.2381 cross-entropy loss: 0.2381
log--[107/240][161/162][client-0] train loss: 0.2538 cross-entropy loss: 0.2538
Epoch 0	Test (client-0):	Loss 1.2286 (1.2286)	Prec@1 78.906 (78.906)
 * Prec@1 71.949
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[108/240][0/162][client-0] train loss: 0.2536 cross-entropy loss: 0.2536
log--[108/240][161/162][client-0] train loss: 0.2458 cross-entropy loss: 0.2458
Epoch 0	Test (client-0):	Loss 1.2068 (1.2068)	Prec@1 79.297 (79.297)
 * Prec@1 72.249
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[109/240][0/162][client-0] train loss: 0.2589 cross-entropy loss: 0.2589
log--[109/240][161/162][client-0] train loss: 0.2488 cross-entropy loss: 0.2488
Epoch 0	Test (client-0):	Loss 1.3194 (1.3194)	Prec@1 78.516 (78.516)
 * Prec@1 71.442
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[110/240][0/162][client-0] train loss: 0.1975 cross-entropy loss: 0.1975
log--[110/240][161/162][client-0] train loss: 0.2367 cross-entropy loss: 0.2367
Epoch 0	Test (client-0):	Loss 1.1789 (1.1789)	Prec@1 79.688 (79.688)
 * Prec@1 70.865
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[111/240][0/162][client-0] train loss: 0.2141 cross-entropy loss: 0.2141
log--[111/240][161/162][client-0] train loss: 0.2241 cross-entropy loss: 0.2241
Epoch 0	Test (client-0):	Loss 1.1859 (1.1859)	Prec@1 77.734 (77.734)
 * Prec@1 71.165
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[112/240][0/162][client-0] train loss: 0.2433 cross-entropy loss: 0.2433
log--[112/240][161/162][client-0] train loss: 0.2388 cross-entropy loss: 0.2388
Epoch 0	Test (client-0):	Loss 1.3067 (1.3067)	Prec@1 77.344 (77.344)
 * Prec@1 71.696
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[113/240][0/162][client-0] train loss: 0.2024 cross-entropy loss: 0.2024
log--[113/240][161/162][client-0] train loss: 0.2298 cross-entropy loss: 0.2298
Epoch 0	Test (client-0):	Loss 1.2792 (1.2792)	Prec@1 78.516 (78.516)
 * Prec@1 72.272
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[114/240][0/162][client-0] train loss: 0.2678 cross-entropy loss: 0.2678
log--[114/240][161/162][client-0] train loss: 0.2231 cross-entropy loss: 0.2231
Epoch 0	Test (client-0):	Loss 1.3345 (1.3345)	Prec@1 75.391 (75.391)
 * Prec@1 71.373
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[115/240][0/162][client-0] train loss: 0.2847 cross-entropy loss: 0.2847
log--[115/240][161/162][client-0] train loss: 0.2234 cross-entropy loss: 0.2234
Epoch 0	Test (client-0):	Loss 1.1266 (1.1266)	Prec@1 78.516 (78.516)
 * Prec@1 73.010
best model saved at: 115
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[116/240][0/162][client-0] train loss: 0.2108 cross-entropy loss: 0.2108
log--[116/240][161/162][client-0] train loss: 0.2072 cross-entropy loss: 0.2072
Epoch 0	Test (client-0):	Loss 1.2407 (1.2407)	Prec@1 78.125 (78.125)
 * Prec@1 72.780
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[117/240][0/162][client-0] train loss: 0.2203 cross-entropy loss: 0.2203
log--[117/240][161/162][client-0] train loss: 0.2092 cross-entropy loss: 0.2092
Epoch 0	Test (client-0):	Loss 1.2909 (1.2909)	Prec@1 78.906 (78.906)
 * Prec@1 70.819
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[118/240][0/162][client-0] train loss: 0.1461 cross-entropy loss: 0.1461
log--[118/240][161/162][client-0] train loss: 0.2174 cross-entropy loss: 0.2174
Epoch 0	Test (client-0):	Loss 1.1725 (1.1725)	Prec@1 79.297 (79.297)
 * Prec@1 71.719
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[119/240][0/162][client-0] train loss: 0.1268 cross-entropy loss: 0.1268
log--[119/240][161/162][client-0] train loss: 0.1975 cross-entropy loss: 0.1975
Epoch 0	Test (client-0):	Loss 1.1848 (1.1848)	Prec@1 80.859 (80.859)
 * Prec@1 72.157
lambd value is: 1.5999999999999996 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[120/240][0/162][client-0] train loss: 0.2279 cross-entropy loss: 0.2279
log--[120/240][161/162][client-0] train loss: 0.1133 cross-entropy loss: 0.1133
Epoch 0	Test (client-0):	Loss 0.9807 (0.9807)	Prec@1 82.422 (82.422)
 * Prec@1 76.378
best model saved at: 120
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[121/240][0/162][client-0] train loss: 0.0629 cross-entropy loss: 0.0629
log--[121/240][161/162][client-0] train loss: 0.0758 cross-entropy loss: 0.0758
Epoch 0	Test (client-0):	Loss 0.9830 (0.9830)	Prec@1 83.984 (83.984)
 * Prec@1 76.217
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[122/240][0/162][client-0] train loss: 0.0527 cross-entropy loss: 0.0527
log--[122/240][161/162][client-0] train loss: 0.0639 cross-entropy loss: 0.0639
Epoch 0	Test (client-0):	Loss 0.9925 (0.9925)	Prec@1 84.375 (84.375)
 * Prec@1 76.724
best model saved at: 122
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[123/240][0/162][client-0] train loss: 0.0515 cross-entropy loss: 0.0515
log--[123/240][161/162][client-0] train loss: 0.0557 cross-entropy loss: 0.0557
Epoch 0	Test (client-0):	Loss 0.9660 (0.9660)	Prec@1 84.375 (84.375)
 * Prec@1 76.955
best model saved at: 123
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[124/240][0/162][client-0] train loss: 0.0231 cross-entropy loss: 0.0231
log--[124/240][161/162][client-0] train loss: 0.0536 cross-entropy loss: 0.0536
Epoch 0	Test (client-0):	Loss 0.9605 (0.9605)	Prec@1 83.594 (83.594)
 * Prec@1 76.886
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[125/240][0/162][client-0] train loss: 0.0512 cross-entropy loss: 0.0512
log--[125/240][161/162][client-0] train loss: 0.0482 cross-entropy loss: 0.0482
Epoch 0	Test (client-0):	Loss 1.0246 (1.0246)	Prec@1 83.594 (83.594)
 * Prec@1 76.794
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[126/240][0/162][client-0] train loss: 0.0205 cross-entropy loss: 0.0205
log--[126/240][161/162][client-0] train loss: 0.0453 cross-entropy loss: 0.0453
Epoch 0	Test (client-0):	Loss 1.0426 (1.0426)	Prec@1 83.203 (83.203)
 * Prec@1 76.909
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[127/240][0/162][client-0] train loss: 0.0520 cross-entropy loss: 0.0520
log--[127/240][161/162][client-0] train loss: 0.0431 cross-entropy loss: 0.0431
Epoch 0	Test (client-0):	Loss 0.9593 (0.9593)	Prec@1 83.984 (83.984)
 * Prec@1 76.655
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[128/240][0/162][client-0] train loss: 0.0754 cross-entropy loss: 0.0754
log--[128/240][161/162][client-0] train loss: 0.0419 cross-entropy loss: 0.0419
Epoch 0	Test (client-0):	Loss 0.9841 (0.9841)	Prec@1 83.594 (83.594)
 * Prec@1 77.116
best model saved at: 128
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[129/240][0/162][client-0] train loss: 0.0549 cross-entropy loss: 0.0549
log--[129/240][161/162][client-0] train loss: 0.0388 cross-entropy loss: 0.0388
Epoch 0	Test (client-0):	Loss 1.0226 (1.0226)	Prec@1 82.422 (82.422)
 * Prec@1 76.863
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[130/240][0/162][client-0] train loss: 0.0288 cross-entropy loss: 0.0288
log--[130/240][161/162][client-0] train loss: 0.0397 cross-entropy loss: 0.0397
Epoch 0	Test (client-0):	Loss 0.9336 (0.9336)	Prec@1 84.375 (84.375)
 * Prec@1 76.932
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[131/240][0/162][client-0] train loss: 0.0646 cross-entropy loss: 0.0646
log--[131/240][161/162][client-0] train loss: 0.0367 cross-entropy loss: 0.0367
Epoch 0	Test (client-0):	Loss 1.0069 (1.0069)	Prec@1 84.766 (84.766)
 * Prec@1 77.116
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[132/240][0/162][client-0] train loss: 0.0231 cross-entropy loss: 0.0231
log--[132/240][161/162][client-0] train loss: 0.0353 cross-entropy loss: 0.0353
Epoch 0	Test (client-0):	Loss 0.9544 (0.9544)	Prec@1 85.547 (85.547)
 * Prec@1 76.909
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[133/240][0/162][client-0] train loss: 0.0303 cross-entropy loss: 0.0303
log--[133/240][161/162][client-0] train loss: 0.0355 cross-entropy loss: 0.0355
Epoch 0	Test (client-0):	Loss 0.9573 (0.9573)	Prec@1 84.375 (84.375)
 * Prec@1 76.817
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[134/240][0/162][client-0] train loss: 0.0199 cross-entropy loss: 0.0199
log--[134/240][161/162][client-0] train loss: 0.0345 cross-entropy loss: 0.0345
Epoch 0	Test (client-0):	Loss 0.9896 (0.9896)	Prec@1 84.375 (84.375)
 * Prec@1 77.093
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[135/240][0/162][client-0] train loss: 0.0243 cross-entropy loss: 0.0243
log--[135/240][161/162][client-0] train loss: 0.0348 cross-entropy loss: 0.0348
Epoch 0	Test (client-0):	Loss 0.9997 (0.9997)	Prec@1 84.375 (84.375)
 * Prec@1 77.163
best model saved at: 135
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[136/240][0/162][client-0] train loss: 0.0451 cross-entropy loss: 0.0451
log--[136/240][161/162][client-0] train loss: 0.0334 cross-entropy loss: 0.0334
Epoch 0	Test (client-0):	Loss 1.0180 (1.0180)	Prec@1 82.812 (82.812)
 * Prec@1 77.163
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[137/240][0/162][client-0] train loss: 0.0268 cross-entropy loss: 0.0268
log--[137/240][161/162][client-0] train loss: 0.0321 cross-entropy loss: 0.0321
Epoch 0	Test (client-0):	Loss 1.0056 (1.0056)	Prec@1 84.766 (84.766)
 * Prec@1 77.439
best model saved at: 137
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[138/240][0/162][client-0] train loss: 0.0444 cross-entropy loss: 0.0444
log--[138/240][161/162][client-0] train loss: 0.0314 cross-entropy loss: 0.0314
Epoch 0	Test (client-0):	Loss 0.9884 (0.9884)	Prec@1 83.984 (83.984)
 * Prec@1 77.416
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[139/240][0/162][client-0] train loss: 0.0272 cross-entropy loss: 0.0272
log--[139/240][161/162][client-0] train loss: 0.0327 cross-entropy loss: 0.0327
Epoch 0	Test (client-0):	Loss 0.9983 (0.9983)	Prec@1 83.594 (83.594)
 * Prec@1 77.463
best model saved at: 139
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[140/240][0/162][client-0] train loss: 0.0378 cross-entropy loss: 0.0378
log--[140/240][161/162][client-0] train loss: 0.0283 cross-entropy loss: 0.0283
Epoch 0	Test (client-0):	Loss 0.9622 (0.9622)	Prec@1 83.594 (83.594)
 * Prec@1 77.024
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[141/240][0/162][client-0] train loss: 0.0268 cross-entropy loss: 0.0268
log--[141/240][161/162][client-0] train loss: 0.0292 cross-entropy loss: 0.0292
Epoch 0	Test (client-0):	Loss 0.9866 (0.9866)	Prec@1 83.594 (83.594)
 * Prec@1 77.232
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[142/240][0/162][client-0] train loss: 0.0068 cross-entropy loss: 0.0068
log--[142/240][161/162][client-0] train loss: 0.0281 cross-entropy loss: 0.0281
Epoch 0	Test (client-0):	Loss 0.9573 (0.9573)	Prec@1 84.375 (84.375)
 * Prec@1 77.278
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[143/240][0/162][client-0] train loss: 0.0131 cross-entropy loss: 0.0131
log--[143/240][161/162][client-0] train loss: 0.0281 cross-entropy loss: 0.0281
Epoch 0	Test (client-0):	Loss 0.9518 (0.9518)	Prec@1 85.547 (85.547)
 * Prec@1 77.463
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[144/240][0/162][client-0] train loss: 0.0336 cross-entropy loss: 0.0336
log--[144/240][161/162][client-0] train loss: 0.0269 cross-entropy loss: 0.0269
Epoch 0	Test (client-0):	Loss 0.9819 (0.9819)	Prec@1 83.984 (83.984)
 * Prec@1 77.255
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[145/240][0/162][client-0] train loss: 0.0246 cross-entropy loss: 0.0246
log--[145/240][161/162][client-0] train loss: 0.0265 cross-entropy loss: 0.0265
Epoch 0	Test (client-0):	Loss 1.0121 (1.0121)	Prec@1 83.203 (83.203)
 * Prec@1 77.347
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[146/240][0/162][client-0] train loss: 0.0356 cross-entropy loss: 0.0356
log--[146/240][161/162][client-0] train loss: 0.0265 cross-entropy loss: 0.0265
Epoch 0	Test (client-0):	Loss 0.9575 (0.9575)	Prec@1 85.156 (85.156)
 * Prec@1 77.578
best model saved at: 146
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[147/240][0/162][client-0] train loss: 0.0247 cross-entropy loss: 0.0247
log--[147/240][161/162][client-0] train loss: 0.0261 cross-entropy loss: 0.0261
Epoch 0	Test (client-0):	Loss 0.9432 (0.9432)	Prec@1 84.766 (84.766)
 * Prec@1 77.624
best model saved at: 147
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[148/240][0/162][client-0] train loss: 0.0184 cross-entropy loss: 0.0184
log--[148/240][161/162][client-0] train loss: 0.0256 cross-entropy loss: 0.0256
Epoch 0	Test (client-0):	Loss 0.9382 (0.9382)	Prec@1 84.375 (84.375)
 * Prec@1 77.209
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[149/240][0/162][client-0] train loss: 0.0125 cross-entropy loss: 0.0125
log--[149/240][161/162][client-0] train loss: 0.0244 cross-entropy loss: 0.0244
Epoch 0	Test (client-0):	Loss 0.9440 (0.9440)	Prec@1 85.547 (85.547)
 * Prec@1 77.463
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[150/240][0/162][client-0] train loss: 0.0125 cross-entropy loss: 0.0125
log--[150/240][161/162][client-0] train loss: 0.0260 cross-entropy loss: 0.0260
Epoch 0	Test (client-0):	Loss 0.9894 (0.9894)	Prec@1 83.984 (83.984)
 * Prec@1 77.116
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[151/240][0/162][client-0] train loss: 0.0145 cross-entropy loss: 0.0145
log--[151/240][161/162][client-0] train loss: 0.0236 cross-entropy loss: 0.0236
Epoch 0	Test (client-0):	Loss 0.9825 (0.9825)	Prec@1 84.766 (84.766)
 * Prec@1 77.532
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[152/240][0/162][client-0] train loss: 0.0262 cross-entropy loss: 0.0262
log--[152/240][161/162][client-0] train loss: 0.0244 cross-entropy loss: 0.0244
Epoch 0	Test (client-0):	Loss 0.9631 (0.9631)	Prec@1 85.547 (85.547)
 * Prec@1 77.532
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[153/240][0/162][client-0] train loss: 0.0248 cross-entropy loss: 0.0248
log--[153/240][161/162][client-0] train loss: 0.0233 cross-entropy loss: 0.0233
Epoch 0	Test (client-0):	Loss 0.9831 (0.9831)	Prec@1 84.766 (84.766)
 * Prec@1 77.140
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[154/240][0/162][client-0] train loss: 0.0083 cross-entropy loss: 0.0083
log--[154/240][161/162][client-0] train loss: 0.0240 cross-entropy loss: 0.0240
Epoch 0	Test (client-0):	Loss 0.9500 (0.9500)	Prec@1 85.547 (85.547)
 * Prec@1 77.301
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[155/240][0/162][client-0] train loss: 0.0219 cross-entropy loss: 0.0219
log--[155/240][161/162][client-0] train loss: 0.0230 cross-entropy loss: 0.0230
Epoch 0	Test (client-0):	Loss 0.9515 (0.9515)	Prec@1 83.984 (83.984)
 * Prec@1 77.186
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[156/240][0/162][client-0] train loss: 0.0133 cross-entropy loss: 0.0133
log--[156/240][161/162][client-0] train loss: 0.0230 cross-entropy loss: 0.0230
Epoch 0	Test (client-0):	Loss 1.0005 (1.0005)	Prec@1 83.984 (83.984)
 * Prec@1 77.647
best model saved at: 156
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[157/240][0/162][client-0] train loss: 0.0089 cross-entropy loss: 0.0089
log--[157/240][161/162][client-0] train loss: 0.0222 cross-entropy loss: 0.0222
Epoch 0	Test (client-0):	Loss 0.9720 (0.9720)	Prec@1 83.984 (83.984)
 * Prec@1 77.924
best model saved at: 157
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[158/240][0/162][client-0] train loss: 0.0233 cross-entropy loss: 0.0233
log--[158/240][161/162][client-0] train loss: 0.0220 cross-entropy loss: 0.0220
Epoch 0	Test (client-0):	Loss 0.9974 (0.9974)	Prec@1 83.984 (83.984)
 * Prec@1 77.439
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[159/240][0/162][client-0] train loss: 0.0250 cross-entropy loss: 0.0250
log--[159/240][161/162][client-0] train loss: 0.0225 cross-entropy loss: 0.0225
Epoch 0	Test (client-0):	Loss 0.9672 (0.9672)	Prec@1 85.547 (85.547)
 * Prec@1 77.509
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[160/240][0/162][client-0] train loss: 0.0166 cross-entropy loss: 0.0166
log--[160/240][161/162][client-0] train loss: 0.0223 cross-entropy loss: 0.0223
Epoch 0	Test (client-0):	Loss 1.0137 (1.0137)	Prec@1 83.594 (83.594)
 * Prec@1 77.509
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[161/240][0/162][client-0] train loss: 0.0224 cross-entropy loss: 0.0224
log--[161/240][161/162][client-0] train loss: 0.0219 cross-entropy loss: 0.0219
Epoch 0	Test (client-0):	Loss 0.9564 (0.9564)	Prec@1 82.031 (82.031)
 * Prec@1 77.370
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[162/240][0/162][client-0] train loss: 0.0404 cross-entropy loss: 0.0404
log--[162/240][161/162][client-0] train loss: 0.0227 cross-entropy loss: 0.0227
Epoch 0	Test (client-0):	Loss 0.9408 (0.9408)	Prec@1 86.328 (86.328)
 * Prec@1 77.232
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[163/240][0/162][client-0] train loss: 0.0217 cross-entropy loss: 0.0217
log--[163/240][161/162][client-0] train loss: 0.0213 cross-entropy loss: 0.0213
Epoch 0	Test (client-0):	Loss 0.9450 (0.9450)	Prec@1 84.375 (84.375)
 * Prec@1 77.024
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[164/240][0/162][client-0] train loss: 0.0195 cross-entropy loss: 0.0195
log--[164/240][161/162][client-0] train loss: 0.0215 cross-entropy loss: 0.0215
Epoch 0	Test (client-0):	Loss 0.9954 (0.9954)	Prec@1 84.766 (84.766)
 * Prec@1 77.578
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[165/240][0/162][client-0] train loss: 0.0110 cross-entropy loss: 0.0110
log--[165/240][161/162][client-0] train loss: 0.0205 cross-entropy loss: 0.0205
Epoch 0	Test (client-0):	Loss 0.9831 (0.9831)	Prec@1 83.984 (83.984)
 * Prec@1 77.762
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[166/240][0/162][client-0] train loss: 0.0225 cross-entropy loss: 0.0225
log--[166/240][161/162][client-0] train loss: 0.0208 cross-entropy loss: 0.0208
Epoch 0	Test (client-0):	Loss 0.9705 (0.9705)	Prec@1 83.594 (83.594)
 * Prec@1 77.393
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[167/240][0/162][client-0] train loss: 0.0105 cross-entropy loss: 0.0105
log--[167/240][161/162][client-0] train loss: 0.0210 cross-entropy loss: 0.0210
Epoch 0	Test (client-0):	Loss 1.0033 (1.0033)	Prec@1 83.984 (83.984)
 * Prec@1 77.278
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[168/240][0/162][client-0] train loss: 0.0257 cross-entropy loss: 0.0257
log--[168/240][161/162][client-0] train loss: 0.0213 cross-entropy loss: 0.0213
Epoch 0	Test (client-0):	Loss 0.9521 (0.9521)	Prec@1 83.984 (83.984)
 * Prec@1 77.047
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[169/240][0/162][client-0] train loss: 0.0075 cross-entropy loss: 0.0075
log--[169/240][161/162][client-0] train loss: 0.0207 cross-entropy loss: 0.0207
Epoch 0	Test (client-0):	Loss 0.9678 (0.9678)	Prec@1 83.203 (83.203)
 * Prec@1 77.393
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[170/240][0/162][client-0] train loss: 0.0234 cross-entropy loss: 0.0234
log--[170/240][161/162][client-0] train loss: 0.0195 cross-entropy loss: 0.0195
Epoch 0	Test (client-0):	Loss 0.9734 (0.9734)	Prec@1 83.203 (83.203)
 * Prec@1 77.070
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[171/240][0/162][client-0] train loss: 0.0164 cross-entropy loss: 0.0164
log--[171/240][161/162][client-0] train loss: 0.0215 cross-entropy loss: 0.0215
Epoch 0	Test (client-0):	Loss 0.9350 (0.9350)	Prec@1 83.594 (83.594)
 * Prec@1 77.278
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[172/240][0/162][client-0] train loss: 0.0141 cross-entropy loss: 0.0141
log--[172/240][161/162][client-0] train loss: 0.0206 cross-entropy loss: 0.0206
Epoch 0	Test (client-0):	Loss 0.9497 (0.9497)	Prec@1 85.156 (85.156)
 * Prec@1 76.955
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[173/240][0/162][client-0] train loss: 0.0115 cross-entropy loss: 0.0115
log--[173/240][161/162][client-0] train loss: 0.0195 cross-entropy loss: 0.0195
Epoch 0	Test (client-0):	Loss 0.9005 (0.9005)	Prec@1 85.156 (85.156)
 * Prec@1 77.716
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[174/240][0/162][client-0] train loss: 0.0230 cross-entropy loss: 0.0230
log--[174/240][161/162][client-0] train loss: 0.0189 cross-entropy loss: 0.0189
Epoch 0	Test (client-0):	Loss 0.9658 (0.9658)	Prec@1 83.984 (83.984)
 * Prec@1 77.624
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[175/240][0/162][client-0] train loss: 0.0070 cross-entropy loss: 0.0070
log--[175/240][161/162][client-0] train loss: 0.0203 cross-entropy loss: 0.0203
Epoch 0	Test (client-0):	Loss 0.9483 (0.9483)	Prec@1 83.594 (83.594)
 * Prec@1 77.670
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[176/240][0/162][client-0] train loss: 0.0114 cross-entropy loss: 0.0114
log--[176/240][161/162][client-0] train loss: 0.0200 cross-entropy loss: 0.0200
Epoch 0	Test (client-0):	Loss 0.9764 (0.9764)	Prec@1 82.812 (82.812)
 * Prec@1 77.555
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[177/240][0/162][client-0] train loss: 0.0212 cross-entropy loss: 0.0212
log--[177/240][161/162][client-0] train loss: 0.0198 cross-entropy loss: 0.0198
Epoch 0	Test (client-0):	Loss 0.9442 (0.9442)	Prec@1 83.203 (83.203)
 * Prec@1 77.739
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[178/240][0/162][client-0] train loss: 0.0206 cross-entropy loss: 0.0206
log--[178/240][161/162][client-0] train loss: 0.0189 cross-entropy loss: 0.0189
Epoch 0	Test (client-0):	Loss 0.9556 (0.9556)	Prec@1 81.641 (81.641)
 * Prec@1 77.439
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[179/240][0/162][client-0] train loss: 0.0331 cross-entropy loss: 0.0331
log--[179/240][161/162][client-0] train loss: 0.0185 cross-entropy loss: 0.0185
Epoch 0	Test (client-0):	Loss 1.0127 (1.0127)	Prec@1 82.031 (82.031)
 * Prec@1 77.532
lambd value is: 7.999999999999998 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[180/240][0/162][client-0] train loss: 0.0173 cross-entropy loss: 0.0173
log--[180/240][161/162][client-0] train loss: 0.0182 cross-entropy loss: 0.0182
Epoch 0	Test (client-0):	Loss 0.9755 (0.9755)	Prec@1 83.203 (83.203)
 * Prec@1 77.693
lambd value is: 16.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[181/240][0/162][client-0] train loss: 0.0101 cross-entropy loss: 0.0101
log--[181/240][161/162][client-0] train loss: 0.0176 cross-entropy loss: 0.0176
Epoch 0	Test (client-0):	Loss 0.9518 (0.9518)	Prec@1 83.203 (83.203)
 * Prec@1 77.486
lambd value is: 16.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[182/240][0/162][client-0] train loss: 0.0246 cross-entropy loss: 0.0246
log--[182/240][161/162][client-0] train loss: 0.0179 cross-entropy loss: 0.0179
Epoch 0	Test (client-0):	Loss 0.9559 (0.9559)	Prec@1 84.375 (84.375)
 * Prec@1 77.785
lambd value is: 16.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[183/240][0/162][client-0] train loss: 0.0087 cross-entropy loss: 0.0087
log--[183/240][161/162][client-0] train loss: 0.0169 cross-entropy loss: 0.0169
Epoch 0	Test (client-0):	Loss 0.9467 (0.9467)	Prec@1 84.766 (84.766)
 * Prec@1 77.762
lambd value is: 16.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[184/240][0/162][client-0] train loss: 0.0177 cross-entropy loss: 0.0177
log--[184/240][161/162][client-0] train loss: 0.0174 cross-entropy loss: 0.0174
Epoch 0	Test (client-0):	Loss 0.9701 (0.9701)	Prec@1 84.375 (84.375)
 * Prec@1 77.832
lambd value is: 16.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[185/240][0/162][client-0] train loss: 0.0161 cross-entropy loss: 0.0161
log--[185/240][161/162][client-0] train loss: 0.0162 cross-entropy loss: 0.0162
Epoch 0	Test (client-0):	Loss 0.9757 (0.9757)	Prec@1 84.375 (84.375)
 * Prec@1 77.947
best model saved at: 185
lambd value is: 16.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[186/240][0/162][client-0] train loss: 0.0080 cross-entropy loss: 0.0080
log--[186/240][161/162][client-0] train loss: 0.0166 cross-entropy loss: 0.0166
Epoch 0	Test (client-0):	Loss 0.9903 (0.9903)	Prec@1 83.984 (83.984)
 * Prec@1 77.901
lambd value is: 16.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[187/240][0/162][client-0] train loss: 0.0247 cross-entropy loss: 0.0247
log--[187/240][161/162][client-0] train loss: 0.0175 cross-entropy loss: 0.0175
Epoch 0	Test (client-0):	Loss 0.9689 (0.9689)	Prec@1 83.203 (83.203)
 * Prec@1 77.785
lambd value is: 16.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[188/240][0/162][client-0] train loss: 0.0119 cross-entropy loss: 0.0119
log--[188/240][161/162][client-0] train loss: 0.0168 cross-entropy loss: 0.0168
Epoch 0	Test (client-0):	Loss 0.9790 (0.9790)	Prec@1 83.594 (83.594)
 * Prec@1 77.832
lambd value is: 16.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[189/240][0/162][client-0] train loss: 0.0126 cross-entropy loss: 0.0126
log--[189/240][161/162][client-0] train loss: 0.0165 cross-entropy loss: 0.0165
Epoch 0	Test (client-0):	Loss 0.9713 (0.9713)	Prec@1 83.594 (83.594)
 * Prec@1 77.809
lambd value is: 16.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[190/240][0/162][client-0] train loss: 0.0118 cross-entropy loss: 0.0118
log--[190/240][161/162][client-0] train loss: 0.0166 cross-entropy loss: 0.0166
Epoch 0	Test (client-0):	Loss 0.9902 (0.9902)	Prec@1 84.375 (84.375)
 * Prec@1 77.901
lambd value is: 16.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[191/240][0/162][client-0] train loss: 0.0064 cross-entropy loss: 0.0064
log--[191/240][161/162][client-0] train loss: 0.0167 cross-entropy loss: 0.0167
Epoch 0	Test (client-0):	Loss 0.9738 (0.9738)	Prec@1 83.594 (83.594)
 * Prec@1 78.085
best model saved at: 191
lambd value is: 16.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[192/240][0/162][client-0] train loss: 0.0165 cross-entropy loss: 0.0165
log--[192/240][161/162][client-0] train loss: 0.0163 cross-entropy loss: 0.0163
Epoch 0	Test (client-0):	Loss 0.9580 (0.9580)	Prec@1 83.203 (83.203)
 * Prec@1 77.762
lambd value is: 16.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[193/240][0/162][client-0] train loss: 0.0216 cross-entropy loss: 0.0216
log--[193/240][161/162][client-0] train loss: 0.0159 cross-entropy loss: 0.0159
Epoch 0	Test (client-0):	Loss 0.9568 (0.9568)	Prec@1 85.156 (85.156)
 * Prec@1 77.878
lambd value is: 16.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[194/240][0/162][client-0] train loss: 0.0073 cross-entropy loss: 0.0073
log--[194/240][161/162][client-0] train loss: 0.0165 cross-entropy loss: 0.0165
Epoch 0	Test (client-0):	Loss 0.9320 (0.9320)	Prec@1 84.375 (84.375)
 * Prec@1 77.924
lambd value is: 16.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[195/240][0/162][client-0] train loss: 0.0135 cross-entropy loss: 0.0135
log--[195/240][161/162][client-0] train loss: 0.0157 cross-entropy loss: 0.0157
Epoch 0	Test (client-0):	Loss 0.9604 (0.9604)	Prec@1 83.984 (83.984)
 * Prec@1 78.062
lambd value is: 16.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[196/240][0/162][client-0] train loss: 0.0185 cross-entropy loss: 0.0185
log--[196/240][161/162][client-0] train loss: 0.0161 cross-entropy loss: 0.0161
Epoch 0	Test (client-0):	Loss 0.9634 (0.9634)	Prec@1 83.984 (83.984)
 * Prec@1 77.855
lambd value is: 16.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[197/240][0/162][client-0] train loss: 0.0210 cross-entropy loss: 0.0210
log--[197/240][161/162][client-0] train loss: 0.0151 cross-entropy loss: 0.0151
Epoch 0	Test (client-0):	Loss 0.9713 (0.9713)	Prec@1 83.984 (83.984)
 * Prec@1 77.878
lambd value is: 16.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[198/240][0/162][client-0] train loss: 0.0583 cross-entropy loss: 0.0583
log--[198/240][161/162][client-0] train loss: 0.0159 cross-entropy loss: 0.0159
Epoch 0	Test (client-0):	Loss 0.9653 (0.9653)	Prec@1 83.594 (83.594)
 * Prec@1 78.016
lambd value is: 16.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[199/240][0/162][client-0] train loss: 0.0193 cross-entropy loss: 0.0193
log--[199/240][161/162][client-0] train loss: 0.0156 cross-entropy loss: 0.0156
Epoch 0	Test (client-0):	Loss 0.9747 (0.9747)	Prec@1 83.203 (83.203)
 * Prec@1 77.739
lambd value is: 16.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[200/240][0/162][client-0] train loss: 0.0141 cross-entropy loss: 0.0141
log--[200/240][161/162][client-0] train loss: 0.0160 cross-entropy loss: 0.0160
Epoch 0	Test (client-0):	Loss 0.9627 (0.9627)	Prec@1 82.422 (82.422)
 * Prec@1 77.739
lambd value is: 16.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[201/240][0/162][client-0] train loss: 0.0291 cross-entropy loss: 0.0291
log--[201/240][161/162][client-0] train loss: 0.0154 cross-entropy loss: 0.0154
Epoch 0	Test (client-0):	Loss 0.9676 (0.9676)	Prec@1 83.203 (83.203)
 * Prec@1 77.716
best model saved at: 201
lambd value is: 16.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[202/240][0/162][client-0] train loss: 0.0070 cross-entropy loss: 0.0070
log--[202/240][161/162][client-0] train loss: 0.0158 cross-entropy loss: 0.0158
Epoch 0	Test (client-0):	Loss 0.9407 (0.9407)	Prec@1 83.984 (83.984)
 * Prec@1 77.855
best model saved at: 202
lambd value is: 16.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[203/240][0/162][client-0] train loss: 0.0126 cross-entropy loss: 0.0126
log--[203/240][161/162][client-0] train loss: 0.0157 cross-entropy loss: 0.0157
Epoch 0	Test (client-0):	Loss 0.9636 (0.9636)	Prec@1 83.203 (83.203)
 * Prec@1 77.832
lambd value is: 16.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[204/240][0/162][client-0] train loss: 0.0100 cross-entropy loss: 0.0100
log--[204/240][161/162][client-0] train loss: 0.0151 cross-entropy loss: 0.0151
Epoch 0	Test (client-0):	Loss 0.9470 (0.9470)	Prec@1 83.984 (83.984)
 * Prec@1 77.832
lambd value is: 16.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[205/240][0/162][client-0] train loss: 0.0091 cross-entropy loss: 0.0091
log--[205/240][161/162][client-0] train loss: 0.0154 cross-entropy loss: 0.0154
Epoch 0	Test (client-0):	Loss 0.9410 (0.9410)	Prec@1 83.594 (83.594)
 * Prec@1 77.855
best model saved at: 205
lambd value is: 16.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[206/240][0/162][client-0] train loss: 0.0168 cross-entropy loss: 0.0168
log--[206/240][161/162][client-0] train loss: 0.0156 cross-entropy loss: 0.0156
Epoch 0	Test (client-0):	Loss 0.9524 (0.9524)	Prec@1 83.203 (83.203)
 * Prec@1 78.062
best model saved at: 206
lambd value is: 16.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[207/240][0/162][client-0] train loss: 0.0161 cross-entropy loss: 0.0161
log--[207/240][161/162][client-0] train loss: 0.0162 cross-entropy loss: 0.0162
Epoch 0	Test (client-0):	Loss 0.9399 (0.9399)	Prec@1 82.812 (82.812)
 * Prec@1 77.832
lambd value is: 16.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[208/240][0/162][client-0] train loss: 0.0142 cross-entropy loss: 0.0142
log--[208/240][161/162][client-0] train loss: 0.0155 cross-entropy loss: 0.0155
Epoch 0	Test (client-0):	Loss 0.9563 (0.9563)	Prec@1 82.812 (82.812)
 * Prec@1 77.809
lambd value is: 16.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[209/240][0/162][client-0] train loss: 0.0278 cross-entropy loss: 0.0278
log--[209/240][161/162][client-0] train loss: 0.0155 cross-entropy loss: 0.0155
Epoch 0	Test (client-0):	Loss 0.9533 (0.9533)	Prec@1 83.984 (83.984)
 * Prec@1 77.855
lambd value is: 16.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[210/240][0/162][client-0] train loss: 0.0081 cross-entropy loss: 0.0081
log--[210/240][161/162][client-0] train loss: 0.0151 cross-entropy loss: 0.0151
Epoch 0	Test (client-0):	Loss 0.9731 (0.9731)	Prec@1 82.812 (82.812)
 * Prec@1 77.486
lambd value is: 16.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[211/240][0/162][client-0] train loss: 0.0075 cross-entropy loss: 0.0075
log--[211/240][161/162][client-0] train loss: 0.0160 cross-entropy loss: 0.0160
Epoch 0	Test (client-0):	Loss 0.9765 (0.9765)	Prec@1 82.812 (82.812)
 * Prec@1 77.670
lambd value is: 16.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[212/240][0/162][client-0] train loss: 0.0069 cross-entropy loss: 0.0069
log--[212/240][161/162][client-0] train loss: 0.0152 cross-entropy loss: 0.0152
Epoch 0	Test (client-0):	Loss 0.9470 (0.9470)	Prec@1 83.984 (83.984)
 * Prec@1 77.716
lambd value is: 16.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[213/240][0/162][client-0] train loss: 0.0187 cross-entropy loss: 0.0187
log--[213/240][161/162][client-0] train loss: 0.0147 cross-entropy loss: 0.0147
Epoch 0	Test (client-0):	Loss 0.9444 (0.9444)	Prec@1 83.594 (83.594)
 * Prec@1 77.832
lambd value is: 16.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[214/240][0/162][client-0] train loss: 0.0114 cross-entropy loss: 0.0114
log--[214/240][161/162][client-0] train loss: 0.0151 cross-entropy loss: 0.0151
Epoch 0	Test (client-0):	Loss 0.9720 (0.9720)	Prec@1 82.812 (82.812)
 * Prec@1 77.716
lambd value is: 16.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[215/240][0/162][client-0] train loss: 0.0078 cross-entropy loss: 0.0078
log--[215/240][161/162][client-0] train loss: 0.0153 cross-entropy loss: 0.0153
Epoch 0	Test (client-0):	Loss 0.9728 (0.9728)	Prec@1 83.203 (83.203)
 * Prec@1 78.131
best model saved at: 215
lambd value is: 16.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[216/240][0/162][client-0] train loss: 0.0394 cross-entropy loss: 0.0394
log--[216/240][161/162][client-0] train loss: 0.0153 cross-entropy loss: 0.0153
Epoch 0	Test (client-0):	Loss 0.9651 (0.9651)	Prec@1 83.984 (83.984)
 * Prec@1 77.993
lambd value is: 16.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[217/240][0/162][client-0] train loss: 0.0109 cross-entropy loss: 0.0109
log--[217/240][161/162][client-0] train loss: 0.0157 cross-entropy loss: 0.0157
Epoch 0	Test (client-0):	Loss 0.9450 (0.9450)	Prec@1 83.203 (83.203)
 * Prec@1 77.855
lambd value is: 16.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[218/240][0/162][client-0] train loss: 0.0084 cross-entropy loss: 0.0084
log--[218/240][161/162][client-0] train loss: 0.0149 cross-entropy loss: 0.0149
Epoch 0	Test (client-0):	Loss 0.9577 (0.9577)	Prec@1 82.031 (82.031)
 * Prec@1 77.762
lambd value is: 16.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[219/240][0/162][client-0] train loss: 0.0125 cross-entropy loss: 0.0125
log--[219/240][161/162][client-0] train loss: 0.0151 cross-entropy loss: 0.0151
Epoch 0	Test (client-0):	Loss 0.9617 (0.9617)	Prec@1 83.594 (83.594)
 * Prec@1 77.716
lambd value is: 16.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[220/240][0/162][client-0] train loss: 0.0204 cross-entropy loss: 0.0204
log--[220/240][161/162][client-0] train loss: 0.0145 cross-entropy loss: 0.0145
Epoch 0	Test (client-0):	Loss 0.9540 (0.9540)	Prec@1 83.594 (83.594)
 * Prec@1 77.947
lambd value is: 16.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[221/240][0/162][client-0] train loss: 0.0135 cross-entropy loss: 0.0135
log--[221/240][161/162][client-0] train loss: 0.0145 cross-entropy loss: 0.0145
Epoch 0	Test (client-0):	Loss 0.9589 (0.9589)	Prec@1 83.203 (83.203)
 * Prec@1 77.901
lambd value is: 16.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[222/240][0/162][client-0] train loss: 0.0261 cross-entropy loss: 0.0261
log--[222/240][161/162][client-0] train loss: 0.0149 cross-entropy loss: 0.0149
Epoch 0	Test (client-0):	Loss 0.9682 (0.9682)	Prec@1 83.594 (83.594)
 * Prec@1 77.739
lambd value is: 16.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[223/240][0/162][client-0] train loss: 0.0289 cross-entropy loss: 0.0289
log--[223/240][161/162][client-0] train loss: 0.0156 cross-entropy loss: 0.0156
Epoch 0	Test (client-0):	Loss 0.9511 (0.9511)	Prec@1 84.375 (84.375)
 * Prec@1 77.693
lambd value is: 16.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[224/240][0/162][client-0] train loss: 0.0060 cross-entropy loss: 0.0060
log--[224/240][161/162][client-0] train loss: 0.0157 cross-entropy loss: 0.0157
Epoch 0	Test (client-0):	Loss 0.9423 (0.9423)	Prec@1 84.375 (84.375)
 * Prec@1 78.016
lambd value is: 16.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[225/240][0/162][client-0] train loss: 0.0138 cross-entropy loss: 0.0138
log--[225/240][161/162][client-0] train loss: 0.0151 cross-entropy loss: 0.0151
Epoch 0	Test (client-0):	Loss 0.9458 (0.9458)	Prec@1 82.812 (82.812)
 * Prec@1 77.832
lambd value is: 16.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[226/240][0/162][client-0] train loss: 0.0111 cross-entropy loss: 0.0111
log--[226/240][161/162][client-0] train loss: 0.0156 cross-entropy loss: 0.0156
Epoch 0	Test (client-0):	Loss 0.9656 (0.9656)	Prec@1 83.594 (83.594)
 * Prec@1 77.509
lambd value is: 16.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[227/240][0/162][client-0] train loss: 0.0193 cross-entropy loss: 0.0193
log--[227/240][161/162][client-0] train loss: 0.0147 cross-entropy loss: 0.0147
Epoch 0	Test (client-0):	Loss 0.9394 (0.9394)	Prec@1 83.594 (83.594)
 * Prec@1 77.486
lambd value is: 16.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[228/240][0/162][client-0] train loss: 0.0218 cross-entropy loss: 0.0218
log--[228/240][161/162][client-0] train loss: 0.0151 cross-entropy loss: 0.0151
Epoch 0	Test (client-0):	Loss 0.9758 (0.9758)	Prec@1 83.594 (83.594)
 * Prec@1 77.762
lambd value is: 16.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[229/240][0/162][client-0] train loss: 0.0108 cross-entropy loss: 0.0108
log--[229/240][161/162][client-0] train loss: 0.0153 cross-entropy loss: 0.0153
Epoch 0	Test (client-0):	Loss 0.9689 (0.9689)	Prec@1 83.984 (83.984)
 * Prec@1 77.670
lambd value is: 16.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[230/240][0/162][client-0] train loss: 0.0289 cross-entropy loss: 0.0289
log--[230/240][161/162][client-0] train loss: 0.0152 cross-entropy loss: 0.0152
Epoch 0	Test (client-0):	Loss 0.9432 (0.9432)	Prec@1 85.156 (85.156)
 * Prec@1 77.970
lambd value is: 16.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[231/240][0/162][client-0] train loss: 0.0102 cross-entropy loss: 0.0102
log--[231/240][161/162][client-0] train loss: 0.0151 cross-entropy loss: 0.0151
Epoch 0	Test (client-0):	Loss 0.9564 (0.9564)	Prec@1 82.812 (82.812)
 * Prec@1 77.785
lambd value is: 16.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[232/240][0/162][client-0] train loss: 0.0077 cross-entropy loss: 0.0077
log--[232/240][161/162][client-0] train loss: 0.0155 cross-entropy loss: 0.0155
Epoch 0	Test (client-0):	Loss 0.9536 (0.9536)	Prec@1 83.984 (83.984)
 * Prec@1 77.716
lambd value is: 16.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[233/240][0/162][client-0] train loss: 0.0065 cross-entropy loss: 0.0065
log--[233/240][161/162][client-0] train loss: 0.0151 cross-entropy loss: 0.0151
Epoch 0	Test (client-0):	Loss 0.9712 (0.9712)	Prec@1 83.594 (83.594)
 * Prec@1 77.993
lambd value is: 16.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[234/240][0/162][client-0] train loss: 0.0134 cross-entropy loss: 0.0134
log--[234/240][161/162][client-0] train loss: 0.0147 cross-entropy loss: 0.0147
Epoch 0	Test (client-0):	Loss 0.9734 (0.9734)	Prec@1 83.984 (83.984)
 * Prec@1 77.785
lambd value is: 16.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[235/240][0/162][client-0] train loss: 0.0088 cross-entropy loss: 0.0088
log--[235/240][161/162][client-0] train loss: 0.0141 cross-entropy loss: 0.0141
Epoch 0	Test (client-0):	Loss 0.9618 (0.9618)	Prec@1 83.203 (83.203)
 * Prec@1 77.762
lambd value is: 16.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[236/240][0/162][client-0] train loss: 0.0161 cross-entropy loss: 0.0161
log--[236/240][161/162][client-0] train loss: 0.0148 cross-entropy loss: 0.0148
Epoch 0	Test (client-0):	Loss 0.9684 (0.9684)	Prec@1 82.812 (82.812)
 * Prec@1 77.647
lambd value is: 16.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[237/240][0/162][client-0] train loss: 0.0321 cross-entropy loss: 0.0321
log--[237/240][161/162][client-0] train loss: 0.0153 cross-entropy loss: 0.0153
Epoch 0	Test (client-0):	Loss 0.9377 (0.9377)	Prec@1 84.766 (84.766)
 * Prec@1 77.832
lambd value is: 16.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[238/240][0/162][client-0] train loss: 0.0180 cross-entropy loss: 0.0180
log--[238/240][161/162][client-0] train loss: 0.0145 cross-entropy loss: 0.0145
Epoch 0	Test (client-0):	Loss 0.9697 (0.9697)	Prec@1 83.594 (83.594)
 * Prec@1 78.062
lambd value is: 16.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[239/240][0/162][client-0] train loss: 0.0296 cross-entropy loss: 0.0296
log--[239/240][161/162][client-0] train loss: 0.0144 cross-entropy loss: 0.0144
Epoch 0	Test (client-0):	Loss 0.9488 (0.9488)	Prec@1 84.375 (84.375)
 * Prec@1 77.693
lambd value is: 16.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[240/240][0/162][client-0] train loss: 0.0093 cross-entropy loss: 0.0093
log--[240/240][161/162][client-0] train loss: 0.0156 cross-entropy loss: 0.0156
Epoch 0	Test (client-0):	Loss 0.9518 (0.9518)	Prec@1 83.984 (83.984)
 * Prec@1 78.085
lambd value is: 16.0 learning rate is: 8.000000000000002e-05
Best Average Validation Accuracy is 78.13148791039279
2025-12-24 02:11:29.969361: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-24 02:11:30.012807: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-24 02:11:30.776357: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
Split Learning Scheme: Overall Cutting_layer 4/13
Total number of batches per epoch for each client is  162
64
original channel size of smashed-data is 128
added bottleneck, new channel size of smashed-data is 8
local:
Sequential(
  (0): ResizeLayer()
  (1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): ReLU(inplace=True)
  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): ReLU(inplace=True)
  (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (9): Conv2d(128, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (10): LCALayer()
  (11): Sigmoid()
)
cloud:
Sequential(
  (0): Conv2d(8, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): ReLU(inplace=True)
  (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (6): ReLU(inplace=True)
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (10): ReLU(inplace=True)
  (11): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (12): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (13): ReLU(inplace=True)
  (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (17): ReLU(inplace=True)
  (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (20): ReLU(inplace=True)
  (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
classifier:
Sequential(
  (0): Dropout(p=0.5, inplace=False)
  (1): Linear(in_features=512, out_features=512, bias=True)
  (2): ReLU(inplace=True)
  (3): Dropout(p=0.5, inplace=False)
  (4): Linear(in_features=512, out_features=512, bias=True)
  (5): ReLU(inplace=True)
  (6): Linear(in_features=512, out_features=530, bias=True)
)
the test model is: best
load client 0's local
./saves/facescrub/SCA_new_slotatt_opt_lg1_thre0.125/pretrain_False_lambd_16_noise_0.025_epoch_240_bottleneck_noRELU_C8S1_log_1_ATstrength_0.3_lr_0.05_varthres_0.125/checkpoint_f_best.tar
load cloud
load classifier
Model's smashed-data size is torch.Size([1, 8, 16, 16])
Sequential(
  85.26 k, 1.992% Params, 86.9 MMac, 1.001% MACs, 
  (0): ResizeLayer(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
  (1): Conv2d(1.79 k, 0.042% Params, 7.34 MMac, 0.085% MACs, 3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (2): BatchNorm2d(128, 0.003% Params, 524.29 KMac, 0.006% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): ReLU(0, 0.000% Params, 262.14 KMac, 0.003% MACs, inplace=True)
  (4): MaxPool2d(0, 0.000% Params, 262.14 KMac, 0.003% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (5): Conv2d(73.86 k, 1.726% Params, 75.63 MMac, 0.871% MACs, 64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (6): BatchNorm2d(256, 0.006% Params, 262.14 KMac, 0.003% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): ReLU(0, 0.000% Params, 131.07 KMac, 0.002% MACs, inplace=True)
  (8): MaxPool2d(0, 0.000% Params, 131.07 KMac, 0.002% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (9): Conv2d(9.22 k, 0.216% Params, 2.36 MMac, 0.027% MACs, 128, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (10): LCALayer(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
  (11): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
)
FLOPs: 8.68 GMac, Parameters: 4.28 M
VGG(
  10.04 M, 70.537% Params, 619.53 MMac, 6.723% MACs, 
  (local): Sequential(
    85.26 k, 0.599% Params, 86.9 MMac, 0.943% MACs, 
    (0): ResizeLayer(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
    (1): Conv2d(1.79 k, 0.013% Params, 7.34 MMac, 0.080% MACs, 3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (2): BatchNorm2d(128, 0.001% Params, 524.29 KMac, 0.006% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): ReLU(0, 0.000% Params, 262.14 KMac, 0.003% MACs, inplace=True)
    (4): MaxPool2d(0, 0.000% Params, 262.14 KMac, 0.003% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(73.86 k, 0.519% Params, 75.63 MMac, 0.821% MACs, 64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): BatchNorm2d(256, 0.002% Params, 262.14 KMac, 0.003% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (7): ReLU(0, 0.000% Params, 131.07 KMac, 0.001% MACs, inplace=True)
    (8): MaxPool2d(0, 0.000% Params, 131.07 KMac, 0.001% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (9): Conv2d(9.22 k, 0.065% Params, 2.36 MMac, 0.026% MACs, 128, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (10): LCALayer(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
    (11): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
  )
  (cloud): Sequential(
    9.16 M, 64.339% Params, 531.83 MMac, 5.772% MACs, 
    (0): Conv2d(9.34 k, 0.066% Params, 2.39 MMac, 0.026% MACs, 8, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): Conv2d(295.17 k, 2.073% Params, 75.56 MMac, 0.820% MACs, 128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (2): BatchNorm2d(512, 0.004% Params, 131.07 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): ReLU(0, 0.000% Params, 65.54 KMac, 0.001% MACs, inplace=True)
    (4): Conv2d(590.08 k, 4.145% Params, 151.06 MMac, 1.639% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): BatchNorm2d(512, 0.004% Params, 131.07 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU(0, 0.000% Params, 65.54 KMac, 0.001% MACs, inplace=True)
    (7): MaxPool2d(0, 0.000% Params, 65.54 KMac, 0.001% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Conv2d(1.18 M, 8.290% Params, 75.53 MMac, 0.820% MACs, 256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): BatchNorm2d(1.02 k, 0.007% Params, 65.54 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): ReLU(0, 0.000% Params, 32.77 KMac, 0.000% MACs, inplace=True)
    (11): Conv2d(2.36 M, 16.576% Params, 151.03 MMac, 1.639% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (12): BatchNorm2d(1.02 k, 0.007% Params, 65.54 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (13): ReLU(0, 0.000% Params, 32.77 KMac, 0.000% MACs, inplace=True)
    (14): MaxPool2d(0, 0.000% Params, 32.77 KMac, 0.000% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (15): Conv2d(2.36 M, 16.576% Params, 37.76 MMac, 0.410% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (16): BatchNorm2d(1.02 k, 0.007% Params, 16.38 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (17): ReLU(0, 0.000% Params, 8.19 KMac, 0.000% MACs, inplace=True)
    (18): Conv2d(2.36 M, 16.576% Params, 37.76 MMac, 0.410% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (19): BatchNorm2d(1.02 k, 0.007% Params, 16.38 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): ReLU(0, 0.000% Params, 8.19 KMac, 0.000% MACs, inplace=True)
    (21): MaxPool2d(0, 0.000% Params, 8.19 KMac, 0.000% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (classifier): Sequential(
    797.2 k, 5.600% Params, 798.23 KMac, 0.009% MACs, 
    (0): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.5, inplace=False)
    (1): Linear(262.66 k, 1.845% Params, 262.66 KMac, 0.003% MACs, in_features=512, out_features=512, bias=True)
    (2): ReLU(0, 0.000% Params, 512.0 Mac, 0.000% MACs, inplace=True)
    (3): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.5, inplace=False)
    (4): Linear(262.66 k, 1.845% Params, 262.66 KMac, 0.003% MACs, in_features=512, out_features=512, bias=True)
    (5): ReLU(0, 0.000% Params, 512.0 Mac, 0.000% MACs, inplace=True)
    (6): Linear(271.89 k, 1.910% Params, 271.89 KMac, 0.003% MACs, in_features=512, out_features=530, bias=True)
  )
)
FLOPs: 9.21 GMac, Parameters: 14.24 M
Model 1 Params: 4279560, Model 2 Params: 14236058
: 528.97 ms
Epoch 0	Test (client-0):	Loss 0.9646 (0.9646)	Prec@1 83.984 (83.984)
 * Prec@1 77.716
Best Average Validation Accuracy is 77.71626286314158
Generating IR ...... (may take a while)
run the attack for training time
2.0 torch.Size([1, 8, 16, 16])
Epoch 1, Current LR: 0.0009990232305719944
the pred acc is: 3.6216839672647407
epoch [1/50], train_loss 0.0335 (0.0336), val_loss 0.0246 (0.0262)
Epoch 2, Current LR: 0.0009960967771506664
the pred acc is: 3.5755478588355043
epoch [2/50], train_loss 0.0317 (0.0261), val_loss 0.0232 (0.0245)
Epoch 3, Current LR: 0.0009912321891107007
the pred acc is: 4.17531718525782
epoch [3/50], train_loss 0.0301 (0.0248), val_loss 0.0225 (0.0242)
Epoch 4, Current LR: 0.0009844486647586721
the pred acc is: 4.682814301215241
epoch [4/50], train_loss 0.0294 (0.0240), val_loss 0.0224 (0.0238)
Epoch 5, Current LR: 0.0009757729755661009
the pred acc is: 5.351787771183177
epoch [5/50], train_loss 0.0288 (0.0236), val_loss 0.0220 (0.0235)
Epoch 6, Current LR: 0.0009652393605146842
the pred acc is: 5.005767005317626
epoch [6/50], train_loss 0.0281 (0.0230), val_loss 0.0219 (0.0233)
Epoch 7, Current LR: 0.0009528893909706795
the pred acc is: 5.605536328275036
epoch [7/50], train_loss 0.0276 (0.0227), val_loss 0.0216 (0.0231)
Epoch 8, Current LR: 0.0009387718066217122
the pred acc is: 5.582468275215387
epoch [8/50], train_loss 0.0274 (0.0225), val_loss 0.0212 (0.0228)
Epoch 9, Current LR: 0.0009229423231234972
the pred acc is: 5.9284890302462285
epoch [9/50], train_loss 0.0272 (0.0222), val_loss 0.0214 (0.0227)
Epoch 10, Current LR: 0.0009054634122155987
the pred acc is: 5.121107261376123
epoch [10/50], train_loss 0.0272 (0.0220), val_loss 0.0214 (0.0226)
Epoch 11, Current LR: 0.0008864040551740153
the pred acc is: 5.513264128741096
epoch [11/50], train_loss 0.0266 (0.0218), val_loss 0.0211 (0.0225)
Epoch 12, Current LR: 0.0008658394705735984
the pred acc is: 5.536332165191193
epoch [12/50], train_loss 0.0267 (0.0218), val_loss 0.0213 (0.0223)
Epoch 13, Current LR: 0.0008438508174347006
the pred acc is: 5.974625124375851
epoch [13/50], train_loss 0.0261 (0.0216), val_loss 0.0210 (0.0224)
Epoch 14, Current LR: 0.000820524874925601
the pred acc is: 6.297577846861491
epoch [14/50], train_loss 0.0259 (0.0214), val_loss 0.0206 (0.0220)
Epoch 15, Current LR: 0.000795953699884774
the pred acc is: 5.6747404719994
epoch [15/50], train_loss 0.0254 (0.0208), val_loss 0.0202 (0.0218)
Epoch 16, Current LR: 0.000770234263514603
the pred acc is: 6.136101501293249
epoch [16/50], train_loss 0.0254 (0.0207), val_loss 0.0205 (0.0218)
Epoch 17, Current LR: 0.0007434680686803488
the pred acc is: 5.905420993796132
epoch [17/50], train_loss 0.0254 (0.0207), val_loss 0.0205 (0.0219)
Epoch 18, Current LR: 0.0007157607493247108
the pred acc is: 5.836216839237059
epoch [18/50], train_loss 0.0250 (0.0204), val_loss 0.0206 (0.0221)
Epoch 19, Current LR: 0.0006872216535789154
the pred acc is: 6.620530555047516
epoch [19/50], train_loss 0.0247 (0.0201), val_loss 0.0205 (0.0222)
Epoch 20, Current LR: 0.0006579634122155987
the pred acc is: 6.712802772345977
epoch [20/50], train_loss 0.0246 (0.0198), val_loss 0.0204 (0.0218)
Epoch 21, Current LR: 0.0006281014941466028
the pred acc is: 6.597462506607742
epoch [21/50], train_loss 0.0248 (0.0199), val_loss 0.0208 (0.0221)
Epoch 22, Current LR: 0.0005977537507199335
the pred acc is: 7.174163777660471
epoch [22/50], train_loss 0.0241 (0.0197), val_loss 0.0210 (0.0222)
Epoch 23, Current LR: 0.0005670399506143305
the pred acc is: 7.450980386657011
epoch [23/50], train_loss 0.0238 (0.0196), val_loss 0.0215 (0.0223)
Epoch 24, Current LR: 0.0005360813071670099
the pred acc is: 7.704728949523715
epoch [24/50], train_loss 0.0235 (0.0196), val_loss 0.0208 (0.0218)
Epoch 25, Current LR: 0.0005049999999999998
the pred acc is: 7.958477488851052
epoch [25/50], train_loss 0.0231 (0.0193), val_loss 0.0205 (0.0216)
Epoch 26, Current LR: 0.0004739186928329897
the pred acc is: 7.889273326922178
epoch [26/50], train_loss 0.0223 (0.0188), val_loss 0.0198 (0.0212)
Epoch 27, Current LR: 0.0004429600493856692
the pred acc is: 7.773933097152974
epoch [27/50], train_loss 0.0220 (0.0183), val_loss 0.0198 (0.0211)
Epoch 28, Current LR: 0.0004122462492800661
the pred acc is: 8.304498264396342
epoch [28/50], train_loss 0.0217 (0.0180), val_loss 0.0200 (0.0212)
Epoch 29, Current LR: 0.0003818985058533967
the pred acc is: 7.773933092533098
epoch [29/50], train_loss 0.0210 (0.0178), val_loss 0.0200 (0.0212)
Epoch 30, Current LR: 0.00035203658778440103
the pred acc is: 8.096885783669583
epoch [30/50], train_loss 0.0210 (0.0176), val_loss 0.0200 (0.0211)
Epoch 31, Current LR: 0.00032277834642108444
the pred acc is: 7.543252589655857
epoch [31/50], train_loss 0.0210 (0.0174), val_loss 0.0199 (0.0212)
Epoch 32, Current LR: 0.0002942392506752889
the pred acc is: 8.050749691849898
epoch [32/50], train_loss 0.0206 (0.0172), val_loss 0.0200 (0.0212)
Epoch 33, Current LR: 0.00026653193131965077
the pred acc is: 8.604382924142593
epoch [33/50], train_loss 0.0214 (0.0170), val_loss 0.0203 (0.0213)
Epoch 34, Current LR: 0.00023976573648539642
the pred acc is: 8.581314861403204
epoch [34/50], train_loss 0.0209 (0.0169), val_loss 0.0199 (0.0209)
Epoch 35, Current LR: 0.00021404630011522574
the pred acc is: 9.642445207879618
epoch [35/50], train_loss 0.0204 (0.0167), val_loss 0.0203 (0.0210)
Epoch 36, Current LR: 0.00018947512507439847
the pred acc is: 10.011534010195264
epoch [36/50], train_loss 0.0196 (0.0161), val_loss 0.0203 (0.0209)
Epoch 37, Current LR: 0.000166149182565299
the pred acc is: 9.919261792896803
epoch [37/50], train_loss 0.0192 (0.0157), val_loss 0.0202 (0.0209)
Epoch 38, Current LR: 0.00014416052942640132
the pred acc is: 10.449826960140172
epoch [38/50], train_loss 0.0188 (0.0155), val_loss 0.0200 (0.0209)
Epoch 39, Current LR: 0.00012359594482598432
the pred acc is: 9.642445226799108
epoch [39/50], train_loss 0.0186 (0.0153), val_loss 0.0196 (0.0206)
Epoch 40, Current LR: 0.00010453658778440102
the pred acc is: 10.35755480004017
epoch [40/50], train_loss 0.0183 (0.0151), val_loss 0.0195 (0.0205)
Epoch 41, Current LR: 8.70576768765026e-05
the pred acc is: 10.265282582741708
epoch [41/50], train_loss 0.0180 (0.0148), val_loss 0.0193 (0.0203)
Epoch 42, Current LR: 7.12281933782875e-05
the pred acc is: 9.711649348139076
epoch [42/50], train_loss 0.0178 (0.0146), val_loss 0.0195 (0.0203)
Epoch 43, Current LR: 5.71106090293204e-05
the pred acc is: 9.504036893701608
epoch [43/50], train_loss 0.0177 (0.0144), val_loss 0.0198 (0.0203)
Epoch 44, Current LR: 4.4760639485315563e-05
the pred acc is: 9.942329862566005
epoch [44/50], train_loss 0.0177 (0.0142), val_loss 0.0193 (0.0202)
Epoch 45, Current LR: 3.422702443389899e-05
the pred acc is: 9.66551324894959
epoch [45/50], train_loss 0.0174 (0.0140), val_loss 0.0192 (0.0201)
Epoch 46, Current LR: 2.5551335241327665e-05
the pred acc is: 9.642445198199878
epoch [46/50], train_loss 0.0172 (0.0139), val_loss 0.0191 (0.0200)
Epoch 47, Current LR: 1.876781088929908e-05
the pred acc is: 10.219146452643054
epoch [47/50], train_loss 0.0171 (0.0138), val_loss 0.0190 (0.0199)
Epoch 48, Current LR: 1.3903222849333505e-05
the pred acc is: 10.080738148144786
epoch [48/50], train_loss 0.0170 (0.0136), val_loss 0.0191 (0.0197)
Epoch 49, Current LR: 1.0976769428005579e-05
the pred acc is: 10.057670111694687
epoch [49/50], train_loss 0.0170 (0.0135), val_loss 0.0191 (0.0197)
Epoch 50, Current LR: 1e-05
No valid Checkpoint Found!
new_saves/cifar10/None_infocons_sgm_lg1_thre0.125/pretrain_False_lambd_0_noise_0.01_epoch_240_bottleneck_noRELU_C8S1_log_1_ATstrength_0.3_lr_0.05_varthres_0.125/checkpoint_f_best.tar
the pred acc is: 9.709110397644116
epoch [50/50], train_loss 0.0170 (0.0135), val_loss 0.0188 (0.0198)
Best Validation Loss is 0.017260370776057243
MSE Loss on ALL Image is 0.0195 (Real Attack Results on the Target Client)
SSIM Loss on ALL Image is 0.5943 (Real Attack Results on the Target Client)
PSNR Loss on ALL Image is 17.1060 (Real Attack Results on the Target Client)
run the attack for inference time
2.0 torch.Size([1, 8, 16, 16])
Epoch 1, Current LR: 0.0009990232305719944
the pred acc is: 0.23094688221709006
epoch [1/50], train_loss 0.0493 (0.0629), val_loss 0.0476 (0.0484)
Epoch 2, Current LR: 0.0009960967771506664
the pred acc is: 1.6166281920382388
epoch [2/50], train_loss 0.0348 (0.0407), val_loss 0.0336 (0.0346)
Epoch 3, Current LR: 0.0009912321891107007
the pred acc is: 2.7713626196422974
epoch [3/50], train_loss 0.0267 (0.0318), val_loss 0.0294 (0.0305)
Epoch 4, Current LR: 0.0009844486647586721
the pred acc is: 2.309468838689509
epoch [4/50], train_loss 0.0213 (0.0279), val_loss 0.0284 (0.0293)
Epoch 5, Current LR: 0.0009757729755661009
the pred acc is: 1.8475750472749353
epoch [5/50], train_loss 0.0181 (0.0252), val_loss 0.0285 (0.0292)
Epoch 6, Current LR: 0.0009652393605146842
the pred acc is: 3.0023094583603855
epoch [6/50], train_loss 0.0169 (0.0229), val_loss 0.0288 (0.0299)
Epoch 7, Current LR: 0.0009528893909706795
the pred acc is: 2.771362603123689
epoch [7/50], train_loss 0.0184 (0.0228), val_loss 0.0304 (0.0303)
Epoch 8, Current LR: 0.0009387718066217122
the pred acc is: 3.2332563675578694
epoch [8/50], train_loss 0.0179 (0.0223), val_loss 0.0273 (0.0283)
Epoch 9, Current LR: 0.0009229423231234972
the pred acc is: 4.157043912944838
epoch [9/50], train_loss 0.0164 (0.0209), val_loss 0.0289 (0.0294)
Epoch 10, Current LR: 0.0009054634122155987
the pred acc is: 5.311778324030288
epoch [10/50], train_loss 0.0153 (0.0201), val_loss 0.0284 (0.0295)
Epoch 11, Current LR: 0.0008864040551740153
the pred acc is: 3.2332563675578694
epoch [11/50], train_loss 0.0166 (0.0201), val_loss 0.0274 (0.0283)
Epoch 12, Current LR: 0.0008658394705735984
the pred acc is: 4.38799077864332
epoch [12/50], train_loss 0.0152 (0.0191), val_loss 0.0275 (0.0283)
Epoch 13, Current LR: 0.0008438508174347006
the pred acc is: 3.6951501319920492
epoch [13/50], train_loss 0.0142 (0.0181), val_loss 0.0272 (0.0285)
Epoch 14, Current LR: 0.000820524874925601
the pred acc is: 3.4642032497749593
epoch [14/50], train_loss 0.0139 (0.0176), val_loss 0.0294 (0.0300)
Epoch 15, Current LR: 0.000795953699884774
the pred acc is: 3.464203222794566
epoch [15/50], train_loss 0.0145 (0.0176), val_loss 0.0272 (0.0287)
Epoch 16, Current LR: 0.000770234263514603
the pred acc is: 5.311778270069501
epoch [16/50], train_loss 0.0132 (0.0173), val_loss 0.0279 (0.0300)
Epoch 17, Current LR: 0.0007434680686803488
the pred acc is: 5.542725206247378
epoch [17/50], train_loss 0.0129 (0.0173), val_loss 0.0293 (0.0313)
Epoch 18, Current LR: 0.0007157607493247108
the pred acc is: 3.2332563675578694
epoch [18/50], train_loss 0.0145 (0.0180), val_loss 0.0288 (0.0300)
Epoch 19, Current LR: 0.0006872216535789154
the pred acc is: 4.387990795161928
epoch [19/50], train_loss 0.0136 (0.0179), val_loss 0.0282 (0.0290)
Epoch 20, Current LR: 0.0006579634122155987
the pred acc is: 5.08083142529459
epoch [20/50], train_loss 0.0135 (0.0169), val_loss 0.0276 (0.0292)
Epoch 21, Current LR: 0.0006281014941466028
the pred acc is: 5.3117782805312865
epoch [21/50], train_loss 0.0119 (0.0160), val_loss 0.0275 (0.0288)
Epoch 22, Current LR: 0.0005977537507199335
the pred acc is: 6.235565869417257
epoch [22/50], train_loss 0.0122 (0.0152), val_loss 0.0281 (0.0292)
Epoch 23, Current LR: 0.0005670399506143305
the pred acc is: 4.849884489116713
epoch [23/50], train_loss 0.0111 (0.0147), val_loss 0.0268 (0.0281)
Epoch 24, Current LR: 0.0005360813071670099
the pred acc is: 5.311778253550893
epoch [24/50], train_loss 0.0118 (0.0147), val_loss 0.0270 (0.0284)
Epoch 25, Current LR: 0.0005049999999999998
the pred acc is: 5.5427251627483765
epoch [25/50], train_loss 0.0107 (0.0145), val_loss 0.0279 (0.0294)
Epoch 26, Current LR: 0.0004739186928329897
the pred acc is: 6.69745960081422
epoch [26/50], train_loss 0.0104 (0.0139), val_loss 0.0270 (0.0288)
Epoch 27, Current LR: 0.0004429600493856692
the pred acc is: 6.235565782419253
epoch [27/50], train_loss 0.0102 (0.0134), val_loss 0.0279 (0.0291)
Epoch 28, Current LR: 0.0004122462492800661
the pred acc is: 5.542725222765986
epoch [28/50], train_loss 0.0100 (0.0131), val_loss 0.0296 (0.0301)
Epoch 29, Current LR: 0.0003818985058533967
the pred acc is: 5.080831387852411
epoch [29/50], train_loss 0.0095 (0.0130), val_loss 0.0286 (0.0296)
Epoch 30, Current LR: 0.00035203658778440103
the pred acc is: 4.618937633880016
epoch [30/50], train_loss 0.0092 (0.0126), val_loss 0.0289 (0.0299)
Epoch 31, Current LR: 0.00032277834642108444
the pred acc is: 6.004618943701165
epoch [31/50], train_loss 0.0089 (0.0122), val_loss 0.0282 (0.0299)
Epoch 32, Current LR: 0.0002942392506752889
the pred acc is: 4.849884489116713
epoch [32/50], train_loss 0.0085 (0.0119), val_loss 0.0275 (0.0293)
Epoch 33, Current LR: 0.00026653193131965077
the pred acc is: 5.773672104983077
epoch [33/50], train_loss 0.0083 (0.0117), val_loss 0.0274 (0.0289)
Epoch 34, Current LR: 0.00023976573648539642
the pred acc is: 5.080831414832804
epoch [34/50], train_loss 0.0083 (0.0114), val_loss 0.0279 (0.0292)
Epoch 35, Current LR: 0.00021404630011522574
the pred acc is: 6.466512751634347
epoch [35/50], train_loss 0.0081 (0.0113), val_loss 0.0285 (0.0297)
Epoch 36, Current LR: 0.00018947512507439847
the pred acc is: 8.083140927153977
epoch [36/50], train_loss 0.0079 (0.0111), val_loss 0.0282 (0.0296)
Epoch 37, Current LR: 0.000166149182565299
the pred acc is: 5.773672061484075
epoch [37/50], train_loss 0.0078 (0.0110), val_loss 0.0287 (0.0296)
Epoch 38, Current LR: 0.00014416052942640132
the pred acc is: 5.773672061484075
epoch [38/50], train_loss 0.0075 (0.0110), val_loss 0.0292 (0.0303)
Epoch 39, Current LR: 0.00012359594482598432
the pred acc is: 9.468822236975125
epoch [39/50], train_loss 0.0078 (0.0110), val_loss 0.0281 (0.0298)
Epoch 40, Current LR: 0.00010453658778440102
the pred acc is: 8.31408775541028
epoch [40/50], train_loss 0.0078 (0.0110), val_loss 0.0280 (0.0298)
Epoch 41, Current LR: 8.70576768765026e-05
the pred acc is: 7.159353273845435
epoch [41/50], train_loss 0.0075 (0.0109), val_loss 0.0282 (0.0301)
Epoch 42, Current LR: 7.12281933782875e-05
the pred acc is: 7.621247119220795
epoch [42/50], train_loss 0.0072 (0.0106), val_loss 0.0280 (0.0297)
Epoch 43, Current LR: 5.71106090293204e-05
the pred acc is: 6.928406445589132
epoch [43/50], train_loss 0.0070 (0.0101), val_loss 0.0281 (0.0296)
Epoch 44, Current LR: 4.4760639485315563e-05
the pred acc is: 6.928406472569525
epoch [44/50], train_loss 0.0066 (0.0098), val_loss 0.0279 (0.0296)
Epoch 45, Current LR: 3.422702443389899e-05
the pred acc is: 8.54503463762737
epoch [45/50], train_loss 0.0065 (0.0096), val_loss 0.0278 (0.0296)
Epoch 46, Current LR: 2.5551335241327665e-05
the pred acc is: 7.621247162719797
epoch [46/50], train_loss 0.0065 (0.0094), val_loss 0.0279 (0.0296)
Epoch 47, Current LR: 1.876781088929908e-05
the pred acc is: 7.390300193504704
epoch [47/50], train_loss 0.0064 (0.0093), val_loss 0.0277 (0.0294)
Epoch 48, Current LR: 1.3903222849333505e-05
the pred acc is: 7.8521939579388835
epoch [48/50], train_loss 0.0063 (0.0093), val_loss 0.0279 (0.0296)
Epoch 49, Current LR: 1.0976769428005579e-05
the pred acc is: 8.314087684930884
epoch [49/50], train_loss 0.0063 (0.0092), val_loss 0.0280 (0.0296)
Epoch 50, Current LR: 1e-05
No valid Checkpoint Found!
new_saves/cifar10/None_infocons_sgm_lg1_thre0.125/pretrain_False_lambd_0_noise_0.01_epoch_240_bottleneck_noRELU_C8S1_log_1_ATstrength_0.3_lr_0.05_varthres_0.125/checkpoint_f_best.tar
the pred acc is: 8.129169231448724
epoch [50/50], train_loss 0.0063 (0.0092), val_loss 0.0278 (0.0295)
Best Validation Loss is 0.026809116825461388
MSE Loss on ALL Image is 0.0273 (Real Attack Results on the Target Client)
SSIM Loss on ALL Image is 0.5237 (Real Attack Results on the Target Client)
PSNR Loss on ALL Image is 15.6418 (Real Attack Results on the Target Client)
== res_normN8C64 Training-based MIA performance Score with optimizer Adam, lr 0.001, loss type MSE on best epoch saved model ==
Reverse Intermediate activation at layer -1 (-1 is the smashed-data)
The tested model is: best
MIA performance Score training time is (MSE, SSIM, PSNR) averaging 1 times
0.01951119118813756, 0.5943197133906847, 17.105967987394937
MIA performance Score inference time is (MSE, SSIM, PSNR): 0.02733, 0.52367, 15.64
2025-12-24 02:19:27.393308: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-24 02:19:27.435058: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-24 02:19:28.195314: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
Split Learning Scheme: Overall Cutting_layer 4/13
Total number of batches per epoch for each client is  162
64
original channel size of smashed-data is 128
added bottleneck, new channel size of smashed-data is 8
local:
Sequential(
  (0): ResizeLayer()
  (1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): ReLU(inplace=True)
  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): ReLU(inplace=True)
  (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (9): Conv2d(128, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (10): LCALayer()
  (11): Sigmoid()
)
cloud:
Sequential(
  (0): Conv2d(8, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): ReLU(inplace=True)
  (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (6): ReLU(inplace=True)
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (10): ReLU(inplace=True)
  (11): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (12): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (13): ReLU(inplace=True)
  (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (17): ReLU(inplace=True)
  (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (20): ReLU(inplace=True)
  (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
classifier:
Sequential(
  (0): Dropout(p=0.5, inplace=False)
  (1): Linear(in_features=512, out_features=512, bias=True)
  (2): ReLU(inplace=True)
  (3): Dropout(p=0.5, inplace=False)
  (4): Linear(in_features=512, out_features=512, bias=True)
  (5): ReLU(inplace=True)
  (6): Linear(in_features=512, out_features=530, bias=True)
)
Namespace(arch='vgg11_bn_sgm', cutlayer=4, batch_size=256, filename='pretrain_False_lambd_24_noise_0.025_epoch_240_bottleneck_noRELU_C8S1_log_1_ATstrength_0.3_lr_0.05_varthres_0.125', folder='saves/facescrub/SCA_new_slotatt_opt_lg1_thre0.125', num_client=1, num_epochs=240, learning_rate=0.05, lambd=24.0, dataset_portion=1.0, client_sample_ratio=1.0, noniid=1.0, local_lr=-1.0, dataset='facescrub', scheme='V2_epoch', regularization='Gaussian_kl', regularization_strength=0.025, var_threshold=0.125, AT_regularization='SCA_new', AT_regularization_strength=0.3, log_entropy=1.0, ssim_threshold=0.5, gan_AE_type='res_normN4C64', gan_loss_type='SSIM', bottleneck_option='noRELU_C8S1', optimize_computation=1, decoder_sync=False, load_from_checkpoint=False, load_from_checkpoint_server=False, transfer_source_task='cifar100', finetune_freeze_bn=False, save_more_checkpoints=False, initialize_different=False, random_seed=125)
Model's smashed-data size is torch.Size([1, 8, 16, 16])
Real Train Phase: done by all clients, for total 240 epochs
GAN training interval N (once every N step) is set to 1!
Train in V2_epoch style
log--[1/240][0/162][client-0] train loss: 6.2841 cross-entropy loss: 6.2841
log--[1/240][161/162][client-0] train loss: 6.2907 cross-entropy loss: 6.2907
Epoch 0	Test (client-0):	Loss 6.2786 (6.2786)	Prec@1 0.000 (0.000)
 * Prec@1 0.208
best model saved at: 1
lambd value is: 0.48 learning rate is: 0.05
/home/unnc/miniconda3/lib/python3.13/site-packages/torch/optim/lr_scheduler.py:209: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Train in V2_epoch style
log--[2/240][0/162][client-0] train loss: 6.3034 cross-entropy loss: 6.3034
log--[2/240][161/162][client-0] train loss: 6.1862 cross-entropy loss: 6.1862
Epoch 0	Test (client-0):	Loss 6.1808 (6.1808)	Prec@1 0.000 (0.000)
 * Prec@1 0.231
best model saved at: 2
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[3/240][0/162][client-0] train loss: 5.9163 cross-entropy loss: 5.9163
log--[3/240][161/162][client-0] train loss: 5.5253 cross-entropy loss: 5.5253
Epoch 0	Test (client-0):	Loss 5.2939 (5.2939)	Prec@1 0.781 (0.781)
 * Prec@1 1.176
best model saved at: 3
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[4/240][0/162][client-0] train loss: 5.4329 cross-entropy loss: 5.4329
log--[4/240][161/162][client-0] train loss: 5.2375 cross-entropy loss: 5.2375
Epoch 0	Test (client-0):	Loss 4.9313 (4.9313)	Prec@1 0.000 (0.000)
 * Prec@1 2.284
best model saved at: 4
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[5/240][0/162][client-0] train loss: 5.0935 cross-entropy loss: 5.0935
log--[5/240][161/162][client-0] train loss: 5.0126 cross-entropy loss: 5.0126
Epoch 0	Test (client-0):	Loss 4.7461 (4.7461)	Prec@1 1.953 (1.953)
 * Prec@1 3.276
best model saved at: 5
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[6/240][0/162][client-0] train loss: 4.8395 cross-entropy loss: 4.8395
log--[6/240][161/162][client-0] train loss: 4.7752 cross-entropy loss: 4.7752
Epoch 0	Test (client-0):	Loss 4.3471 (4.3471)	Prec@1 7.812 (7.812)
 * Prec@1 5.052
best model saved at: 6
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[7/240][0/162][client-0] train loss: 4.7265 cross-entropy loss: 4.7265
log--[7/240][161/162][client-0] train loss: 4.5501 cross-entropy loss: 4.5501
Epoch 0	Test (client-0):	Loss 4.6151 (4.6151)	Prec@1 2.344 (2.344)
 * Prec@1 5.882
best model saved at: 7
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[8/240][0/162][client-0] train loss: 4.3992 cross-entropy loss: 4.3992
log--[8/240][161/162][client-0] train loss: 4.2742 cross-entropy loss: 4.2742
Epoch 0	Test (client-0):	Loss 3.9628 (3.9628)	Prec@1 13.672 (13.672)
 * Prec@1 9.366
best model saved at: 8
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[9/240][0/162][client-0] train loss: 4.2090 cross-entropy loss: 4.2090
log--[9/240][161/162][client-0] train loss: 4.0837 cross-entropy loss: 4.0837
Epoch 0	Test (client-0):	Loss 3.9509 (3.9509)	Prec@1 14.062 (14.062)
 * Prec@1 9.527
best model saved at: 9
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[10/240][0/162][client-0] train loss: 3.8022 cross-entropy loss: 3.8022
log--[10/240][161/162][client-0] train loss: 3.9587 cross-entropy loss: 3.9587
Epoch 0	Test (client-0):	Loss 3.5704 (3.5704)	Prec@1 18.359 (18.359)
 * Prec@1 14.072
best model saved at: 10
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[11/240][0/162][client-0] train loss: 3.7327 cross-entropy loss: 3.7327
log--[11/240][161/162][client-0] train loss: 3.8188 cross-entropy loss: 3.8188
Epoch 0	Test (client-0):	Loss 3.4538 (3.4538)	Prec@1 19.531 (19.531)
 * Prec@1 17.047
best model saved at: 11
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[12/240][0/162][client-0] train loss: 3.7199 cross-entropy loss: 3.7199
log--[12/240][161/162][client-0] train loss: 3.7034 cross-entropy loss: 3.7034
Epoch 0	Test (client-0):	Loss 3.6798 (3.6798)	Prec@1 16.016 (16.016)
 * Prec@1 17.578
best model saved at: 12
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[13/240][0/162][client-0] train loss: 3.5838 cross-entropy loss: 3.5838
log--[13/240][161/162][client-0] train loss: 3.5920 cross-entropy loss: 3.5920
Epoch 0	Test (client-0):	Loss 3.1801 (3.1801)	Prec@1 23.828 (23.828)
 * Prec@1 21.315
best model saved at: 13
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[14/240][0/162][client-0] train loss: 3.4331 cross-entropy loss: 3.4331
log--[14/240][161/162][client-0] train loss: 3.4936 cross-entropy loss: 3.4936
Epoch 0	Test (client-0):	Loss 3.6881 (3.6881)	Prec@1 17.188 (17.188)
 * Prec@1 15.963
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[15/240][0/162][client-0] train loss: 3.2827 cross-entropy loss: 3.2827
log--[15/240][161/162][client-0] train loss: 3.3771 cross-entropy loss: 3.3771
Epoch 0	Test (client-0):	Loss 3.3044 (3.3044)	Prec@1 23.828 (23.828)
 * Prec@1 20.300
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[16/240][0/162][client-0] train loss: 3.2560 cross-entropy loss: 3.2560
log--[16/240][161/162][client-0] train loss: 3.2653 cross-entropy loss: 3.2653
Epoch 0	Test (client-0):	Loss 3.2372 (3.2372)	Prec@1 25.781 (25.781)
 * Prec@1 23.552
best model saved at: 16
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[17/240][0/162][client-0] train loss: 3.2642 cross-entropy loss: 3.2642
log--[17/240][161/162][client-0] train loss: 3.1576 cross-entropy loss: 3.1576
Epoch 0	Test (client-0):	Loss 3.0666 (3.0666)	Prec@1 24.219 (24.219)
 * Prec@1 24.106
best model saved at: 17
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[18/240][0/162][client-0] train loss: 2.9926 cross-entropy loss: 2.9926
log--[18/240][161/162][client-0] train loss: 3.0693 cross-entropy loss: 3.0693
Epoch 0	Test (client-0):	Loss 2.8030 (2.8030)	Prec@1 30.469 (30.469)
 * Prec@1 28.074
best model saved at: 18
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[19/240][0/162][client-0] train loss: 3.0070 cross-entropy loss: 3.0070
log--[19/240][161/162][client-0] train loss: 2.9941 cross-entropy loss: 2.9941
Epoch 0	Test (client-0):	Loss 2.8111 (2.8111)	Prec@1 33.203 (33.203)
 * Prec@1 29.596
best model saved at: 19
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[20/240][0/162][client-0] train loss: 2.8284 cross-entropy loss: 2.8284
log--[20/240][161/162][client-0] train loss: 2.9172 cross-entropy loss: 2.9172
Epoch 0	Test (client-0):	Loss 2.6463 (2.6463)	Prec@1 32.812 (32.812)
 * Prec@1 31.326
best model saved at: 20
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[21/240][0/162][client-0] train loss: 2.5911 cross-entropy loss: 2.5911
log--[21/240][161/162][client-0] train loss: 2.8454 cross-entropy loss: 2.8454
Epoch 0	Test (client-0):	Loss 2.6042 (2.6042)	Prec@1 33.594 (33.594)
 * Prec@1 30.842
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[22/240][0/162][client-0] train loss: 2.7855 cross-entropy loss: 2.7855
log--[22/240][161/162][client-0] train loss: 2.7668 cross-entropy loss: 2.7668
Epoch 0	Test (client-0):	Loss 2.5686 (2.5686)	Prec@1 39.844 (39.844)
 * Prec@1 33.287
best model saved at: 22
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[23/240][0/162][client-0] train loss: 2.8575 cross-entropy loss: 2.8575
log--[23/240][161/162][client-0] train loss: 2.6891 cross-entropy loss: 2.6891
Epoch 0	Test (client-0):	Loss 2.8306 (2.8306)	Prec@1 35.938 (35.938)
 * Prec@1 32.042
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[24/240][0/162][client-0] train loss: 2.6908 cross-entropy loss: 2.6908
log--[24/240][161/162][client-0] train loss: 2.6134 cross-entropy loss: 2.6134
Epoch 0	Test (client-0):	Loss 2.3595 (2.3595)	Prec@1 43.750 (43.750)
 * Prec@1 36.678
best model saved at: 24
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[25/240][0/162][client-0] train loss: 2.5228 cross-entropy loss: 2.5228
log--[25/240][161/162][client-0] train loss: 2.5515 cross-entropy loss: 2.5515
Epoch 0	Test (client-0):	Loss 2.3925 (2.3925)	Prec@1 41.406 (41.406)
 * Prec@1 39.723
best model saved at: 25
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[26/240][0/162][client-0] train loss: 2.4058 cross-entropy loss: 2.4058
log--[26/240][161/162][client-0] train loss: 2.5042 cross-entropy loss: 2.5042
Epoch 0	Test (client-0):	Loss 2.2447 (2.2447)	Prec@1 49.219 (49.219)
 * Prec@1 36.055
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[27/240][0/162][client-0] train loss: 2.4463 cross-entropy loss: 2.4463
log--[27/240][161/162][client-0] train loss: 2.4319 cross-entropy loss: 2.4319
Epoch 0	Test (client-0):	Loss 2.2622 (2.2622)	Prec@1 47.266 (47.266)
 * Prec@1 38.339
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[28/240][0/162][client-0] train loss: 2.3659 cross-entropy loss: 2.3659
log--[28/240][161/162][client-0] train loss: 2.3859 cross-entropy loss: 2.3859
Epoch 0	Test (client-0):	Loss 2.3203 (2.3203)	Prec@1 42.969 (42.969)
 * Prec@1 43.899
best model saved at: 28
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[29/240][0/162][client-0] train loss: 2.2424 cross-entropy loss: 2.2424
log--[29/240][161/162][client-0] train loss: 2.3272 cross-entropy loss: 2.3272
Epoch 0	Test (client-0):	Loss 2.4615 (2.4615)	Prec@1 45.312 (45.312)
 * Prec@1 38.893
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[30/240][0/162][client-0] train loss: 2.1685 cross-entropy loss: 2.1685
log--[30/240][161/162][client-0] train loss: 2.2686 cross-entropy loss: 2.2686
Epoch 0	Test (client-0):	Loss 2.8108 (2.8108)	Prec@1 38.281 (38.281)
 * Prec@1 36.009
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[31/240][0/162][client-0] train loss: 2.2972 cross-entropy loss: 2.2972
log--[31/240][161/162][client-0] train loss: 2.2356 cross-entropy loss: 2.2356
Epoch 0	Test (client-0):	Loss 2.2358 (2.2358)	Prec@1 42.969 (42.969)
 * Prec@1 44.360
best model saved at: 31
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[32/240][0/162][client-0] train loss: 2.2956 cross-entropy loss: 2.2956
log--[32/240][161/162][client-0] train loss: 2.2202 cross-entropy loss: 2.2202
Epoch 0	Test (client-0):	Loss 2.4411 (2.4411)	Prec@1 42.578 (42.578)
 * Prec@1 42.491
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[33/240][0/162][client-0] train loss: 2.1104 cross-entropy loss: 2.1104
log--[33/240][161/162][client-0] train loss: 2.1717 cross-entropy loss: 2.1717
Epoch 0	Test (client-0):	Loss 2.1678 (2.1678)	Prec@1 51.172 (51.172)
 * Prec@1 43.645
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[34/240][0/162][client-0] train loss: 2.2889 cross-entropy loss: 2.2889
log--[34/240][161/162][client-0] train loss: 2.1398 cross-entropy loss: 2.1398
Epoch 0	Test (client-0):	Loss 2.1613 (2.1613)	Prec@1 48.828 (48.828)
 * Prec@1 46.667
best model saved at: 34
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[35/240][0/162][client-0] train loss: 2.0399 cross-entropy loss: 2.0399
log--[35/240][161/162][client-0] train loss: 2.1126 cross-entropy loss: 2.1126
Epoch 0	Test (client-0):	Loss 1.9564 (1.9564)	Prec@1 52.734 (52.734)
 * Prec@1 45.329
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[36/240][0/162][client-0] train loss: 2.0878 cross-entropy loss: 2.0878
log--[36/240][161/162][client-0] train loss: 2.0738 cross-entropy loss: 2.0738
Epoch 0	Test (client-0):	Loss 2.1303 (2.1303)	Prec@1 46.875 (46.875)
 * Prec@1 44.291
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[37/240][0/162][client-0] train loss: 1.9455 cross-entropy loss: 1.9455
log--[37/240][161/162][client-0] train loss: 2.0500 cross-entropy loss: 2.0500
Epoch 0	Test (client-0):	Loss 2.2902 (2.2902)	Prec@1 48.047 (48.047)
 * Prec@1 43.022
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[38/240][0/162][client-0] train loss: 1.8470 cross-entropy loss: 1.8470
log--[38/240][161/162][client-0] train loss: 2.0214 cross-entropy loss: 2.0214
Epoch 0	Test (client-0):	Loss 2.2643 (2.2643)	Prec@1 46.875 (46.875)
 * Prec@1 43.529
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[39/240][0/162][client-0] train loss: 1.7803 cross-entropy loss: 1.7803
log--[39/240][161/162][client-0] train loss: 1.9890 cross-entropy loss: 1.9890
Epoch 0	Test (client-0):	Loss 1.9791 (1.9791)	Prec@1 51.562 (51.562)
 * Prec@1 49.343
best model saved at: 39
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[40/240][0/162][client-0] train loss: 2.0696 cross-entropy loss: 2.0696
log--[40/240][161/162][client-0] train loss: 1.9608 cross-entropy loss: 1.9608
Epoch 0	Test (client-0):	Loss 2.0723 (2.0723)	Prec@1 53.516 (53.516)
 * Prec@1 44.152
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[41/240][0/162][client-0] train loss: 1.8590 cross-entropy loss: 1.8590
log--[41/240][161/162][client-0] train loss: 1.9431 cross-entropy loss: 1.9431
Epoch 0	Test (client-0):	Loss 1.8921 (1.8921)	Prec@1 53.516 (53.516)
 * Prec@1 45.721
best model saved at: 41
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[42/240][0/162][client-0] train loss: 1.8340 cross-entropy loss: 1.8340
log--[42/240][161/162][client-0] train loss: 1.9078 cross-entropy loss: 1.9078
Epoch 0	Test (client-0):	Loss 1.6571 (1.6571)	Prec@1 59.375 (59.375)
 * Prec@1 51.857
best model saved at: 42
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[43/240][0/162][client-0] train loss: 1.9094 cross-entropy loss: 1.9094
log--[43/240][161/162][client-0] train loss: 1.8733 cross-entropy loss: 1.8733
Epoch 0	Test (client-0):	Loss 2.0627 (2.0627)	Prec@1 50.391 (50.391)
 * Prec@1 47.197
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[44/240][0/162][client-0] train loss: 1.8004 cross-entropy loss: 1.8004
log--[44/240][161/162][client-0] train loss: 1.8563 cross-entropy loss: 1.8563
Epoch 0	Test (client-0):	Loss 2.2705 (2.2705)	Prec@1 50.391 (50.391)
 * Prec@1 45.283
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[45/240][0/162][client-0] train loss: 1.8738 cross-entropy loss: 1.8738
log--[45/240][161/162][client-0] train loss: 1.8485 cross-entropy loss: 1.8485
Epoch 0	Test (client-0):	Loss 1.8803 (1.8803)	Prec@1 51.953 (51.953)
 * Prec@1 53.679
best model saved at: 45
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[46/240][0/162][client-0] train loss: 1.7742 cross-entropy loss: 1.7742
log--[46/240][161/162][client-0] train loss: 1.7912 cross-entropy loss: 1.7912
Epoch 0	Test (client-0):	Loss 2.0356 (2.0356)	Prec@1 53.516 (53.516)
 * Prec@1 53.287
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[47/240][0/162][client-0] train loss: 1.7290 cross-entropy loss: 1.7290
log--[47/240][161/162][client-0] train loss: 1.7770 cross-entropy loss: 1.7770
Epoch 0	Test (client-0):	Loss 1.7985 (1.7985)	Prec@1 57.031 (57.031)
 * Prec@1 55.248
best model saved at: 47
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[48/240][0/162][client-0] train loss: 1.7725 cross-entropy loss: 1.7725
log--[48/240][161/162][client-0] train loss: 1.7481 cross-entropy loss: 1.7481
Epoch 0	Test (client-0):	Loss 1.7731 (1.7731)	Prec@1 57.812 (57.812)
 * Prec@1 54.256
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[49/240][0/162][client-0] train loss: 1.5345 cross-entropy loss: 1.5345
log--[49/240][161/162][client-0] train loss: 1.7477 cross-entropy loss: 1.7477
Epoch 0	Test (client-0):	Loss 1.9535 (1.9535)	Prec@1 52.734 (52.734)
 * Prec@1 48.674
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[50/240][0/162][client-0] train loss: 1.8190 cross-entropy loss: 1.8190
log--[50/240][161/162][client-0] train loss: 1.7089 cross-entropy loss: 1.7089
Epoch 0	Test (client-0):	Loss 1.6538 (1.6538)	Prec@1 62.500 (62.500)
 * Prec@1 57.116
best model saved at: 50
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[51/240][0/162][client-0] train loss: 1.5394 cross-entropy loss: 1.5394
log--[51/240][161/162][client-0] train loss: 1.6905 cross-entropy loss: 1.6905
Epoch 0	Test (client-0):	Loss 2.1368 (2.1368)	Prec@1 53.906 (53.906)
 * Prec@1 52.572
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[52/240][0/162][client-0] train loss: 1.6026 cross-entropy loss: 1.6026
log--[52/240][161/162][client-0] train loss: 1.6715 cross-entropy loss: 1.6715
Epoch 0	Test (client-0):	Loss 1.4887 (1.4887)	Prec@1 62.891 (62.891)
 * Prec@1 55.456
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[53/240][0/162][client-0] train loss: 1.5420 cross-entropy loss: 1.5420
log--[53/240][161/162][client-0] train loss: 1.6563 cross-entropy loss: 1.6563
Epoch 0	Test (client-0):	Loss 1.6415 (1.6415)	Prec@1 58.984 (58.984)
 * Prec@1 50.681
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[54/240][0/162][client-0] train loss: 1.5827 cross-entropy loss: 1.5827
log--[54/240][161/162][client-0] train loss: 1.6178 cross-entropy loss: 1.6178
Epoch 0	Test (client-0):	Loss 1.8136 (1.8136)	Prec@1 57.031 (57.031)
 * Prec@1 56.286
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[55/240][0/162][client-0] train loss: 1.4345 cross-entropy loss: 1.4345
log--[55/240][161/162][client-0] train loss: 1.6051 cross-entropy loss: 1.6051
Epoch 0	Test (client-0):	Loss 1.7924 (1.7924)	Prec@1 60.156 (60.156)
 * Prec@1 55.225
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[56/240][0/162][client-0] train loss: 1.7352 cross-entropy loss: 1.7352
log--[56/240][161/162][client-0] train loss: 1.6097 cross-entropy loss: 1.6097
Epoch 0	Test (client-0):	Loss 2.0229 (2.0229)	Prec@1 53.125 (53.125)
 * Prec@1 54.394
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[57/240][0/162][client-0] train loss: 1.3670 cross-entropy loss: 1.3670
log--[57/240][161/162][client-0] train loss: 1.5564 cross-entropy loss: 1.5564
Epoch 0	Test (client-0):	Loss 1.3298 (1.3298)	Prec@1 67.188 (67.188)
 * Prec@1 60.208
best model saved at: 57
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[58/240][0/162][client-0] train loss: 1.3438 cross-entropy loss: 1.3438
log--[58/240][161/162][client-0] train loss: 1.5500 cross-entropy loss: 1.5500
Epoch 0	Test (client-0):	Loss 2.0173 (2.0173)	Prec@1 52.344 (52.344)
 * Prec@1 52.457
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[59/240][0/162][client-0] train loss: 1.4963 cross-entropy loss: 1.4963
log--[59/240][161/162][client-0] train loss: 1.5423 cross-entropy loss: 1.5423
Epoch 0	Test (client-0):	Loss 1.4886 (1.4886)	Prec@1 64.062 (64.062)
 * Prec@1 54.348
lambd value is: 0.48 learning rate is: 0.05
Train in V2_epoch style
log--[60/240][0/162][client-0] train loss: 1.6232 cross-entropy loss: 1.6232
log--[60/240][161/162][client-0] train loss: 1.0525 cross-entropy loss: 1.0525
Epoch 0	Test (client-0):	Loss 0.9978 (0.9978)	Prec@1 77.344 (77.344)
 * Prec@1 72.226
best model saved at: 60
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[61/240][0/162][client-0] train loss: 1.1030 cross-entropy loss: 1.1030
log--[61/240][161/162][client-0] train loss: 0.8677 cross-entropy loss: 0.8677
Epoch 0	Test (client-0):	Loss 0.9593 (0.9593)	Prec@1 77.344 (77.344)
 * Prec@1 73.518
best model saved at: 61
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[62/240][0/162][client-0] train loss: 0.8579 cross-entropy loss: 0.8579
log--[62/240][161/162][client-0] train loss: 0.7999 cross-entropy loss: 0.7999
Epoch 0	Test (client-0):	Loss 0.9673 (0.9673)	Prec@1 78.906 (78.906)
 * Prec@1 74.302
best model saved at: 62
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[63/240][0/162][client-0] train loss: 0.7588 cross-entropy loss: 0.7588
log--[63/240][161/162][client-0] train loss: 0.7394 cross-entropy loss: 0.7394
Epoch 0	Test (client-0):	Loss 0.9029 (0.9029)	Prec@1 81.641 (81.641)
 * Prec@1 73.979
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[64/240][0/162][client-0] train loss: 0.8287 cross-entropy loss: 0.8287
log--[64/240][161/162][client-0] train loss: 0.6951 cross-entropy loss: 0.6951
Epoch 0	Test (client-0):	Loss 0.9260 (0.9260)	Prec@1 80.078 (80.078)
 * Prec@1 73.749
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[65/240][0/162][client-0] train loss: 0.6287 cross-entropy loss: 0.6287
log--[65/240][161/162][client-0] train loss: 0.6765 cross-entropy loss: 0.6765
Epoch 0	Test (client-0):	Loss 0.9985 (0.9985)	Prec@1 78.125 (78.125)
 * Prec@1 73.679
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[66/240][0/162][client-0] train loss: 0.6492 cross-entropy loss: 0.6492
log--[66/240][161/162][client-0] train loss: 0.6266 cross-entropy loss: 0.6266
Epoch 0	Test (client-0):	Loss 0.9742 (0.9742)	Prec@1 78.906 (78.906)
 * Prec@1 73.587
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[67/240][0/162][client-0] train loss: 0.4793 cross-entropy loss: 0.4793
log--[67/240][161/162][client-0] train loss: 0.6062 cross-entropy loss: 0.6062
Epoch 0	Test (client-0):	Loss 1.0431 (1.0431)	Prec@1 80.078 (80.078)
 * Prec@1 73.472
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[68/240][0/162][client-0] train loss: 0.4758 cross-entropy loss: 0.4758
log--[68/240][161/162][client-0] train loss: 0.5798 cross-entropy loss: 0.5798
Epoch 0	Test (client-0):	Loss 1.0344 (1.0344)	Prec@1 76.562 (76.562)
 * Prec@1 73.518
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[69/240][0/162][client-0] train loss: 0.6040 cross-entropy loss: 0.6040
log--[69/240][161/162][client-0] train loss: 0.5494 cross-entropy loss: 0.5494
Epoch 0	Test (client-0):	Loss 1.1231 (1.1231)	Prec@1 78.906 (78.906)
 * Prec@1 73.103
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[70/240][0/162][client-0] train loss: 0.5178 cross-entropy loss: 0.5178
log--[70/240][161/162][client-0] train loss: 0.5285 cross-entropy loss: 0.5285
Epoch 0	Test (client-0):	Loss 1.0435 (1.0435)	Prec@1 78.906 (78.906)
 * Prec@1 73.841
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[71/240][0/162][client-0] train loss: 0.4838 cross-entropy loss: 0.4838
log--[71/240][161/162][client-0] train loss: 0.5153 cross-entropy loss: 0.5153
Epoch 0	Test (client-0):	Loss 0.9942 (0.9942)	Prec@1 80.859 (80.859)
 * Prec@1 73.749
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[72/240][0/162][client-0] train loss: 0.4360 cross-entropy loss: 0.4360
log--[72/240][161/162][client-0] train loss: 0.4849 cross-entropy loss: 0.4849
Epoch 0	Test (client-0):	Loss 1.1296 (1.1296)	Prec@1 75.781 (75.781)
 * Prec@1 73.379
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[73/240][0/162][client-0] train loss: 0.4522 cross-entropy loss: 0.4522
log--[73/240][161/162][client-0] train loss: 0.4752 cross-entropy loss: 0.4752
Epoch 0	Test (client-0):	Loss 1.2191 (1.2191)	Prec@1 75.391 (75.391)
 * Prec@1 72.341
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[74/240][0/162][client-0] train loss: 0.3861 cross-entropy loss: 0.3861
log--[74/240][161/162][client-0] train loss: 0.4596 cross-entropy loss: 0.4596
Epoch 0	Test (client-0):	Loss 1.1784 (1.1784)	Prec@1 76.172 (76.172)
 * Prec@1 73.564
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[75/240][0/162][client-0] train loss: 0.4578 cross-entropy loss: 0.4578
log--[75/240][161/162][client-0] train loss: 0.4443 cross-entropy loss: 0.4443
Epoch 0	Test (client-0):	Loss 1.2098 (1.2098)	Prec@1 76.562 (76.562)
 * Prec@1 72.411
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[76/240][0/162][client-0] train loss: 0.4151 cross-entropy loss: 0.4151
log--[76/240][161/162][client-0] train loss: 0.4273 cross-entropy loss: 0.4273
Epoch 0	Test (client-0):	Loss 1.2287 (1.2287)	Prec@1 77.344 (77.344)
 * Prec@1 72.618
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[77/240][0/162][client-0] train loss: 0.4128 cross-entropy loss: 0.4128
log--[77/240][161/162][client-0] train loss: 0.4293 cross-entropy loss: 0.4293
Epoch 0	Test (client-0):	Loss 1.2115 (1.2115)	Prec@1 75.391 (75.391)
 * Prec@1 72.595
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[78/240][0/162][client-0] train loss: 0.4184 cross-entropy loss: 0.4184
log--[78/240][161/162][client-0] train loss: 0.4087 cross-entropy loss: 0.4087
Epoch 0	Test (client-0):	Loss 1.3941 (1.3941)	Prec@1 71.484 (71.484)
 * Prec@1 71.788
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[79/240][0/162][client-0] train loss: 0.4451 cross-entropy loss: 0.4451
log--[79/240][161/162][client-0] train loss: 0.3870 cross-entropy loss: 0.3870
Epoch 0	Test (client-0):	Loss 1.1794 (1.1794)	Prec@1 78.516 (78.516)
 * Prec@1 73.241
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[80/240][0/162][client-0] train loss: 0.3833 cross-entropy loss: 0.3833
log--[80/240][161/162][client-0] train loss: 0.3748 cross-entropy loss: 0.3748
Epoch 0	Test (client-0):	Loss 1.3263 (1.3263)	Prec@1 74.219 (74.219)
 * Prec@1 73.495
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[81/240][0/162][client-0] train loss: 0.3283 cross-entropy loss: 0.3283
log--[81/240][161/162][client-0] train loss: 0.3812 cross-entropy loss: 0.3812
Epoch 0	Test (client-0):	Loss 1.2273 (1.2273)	Prec@1 78.906 (78.906)
 * Prec@1 73.633
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[82/240][0/162][client-0] train loss: 0.2554 cross-entropy loss: 0.2554
log--[82/240][161/162][client-0] train loss: 0.3564 cross-entropy loss: 0.3564
Epoch 0	Test (client-0):	Loss 1.2701 (1.2701)	Prec@1 75.391 (75.391)
 * Prec@1 71.303
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[83/240][0/162][client-0] train loss: 0.3481 cross-entropy loss: 0.3481
log--[83/240][161/162][client-0] train loss: 0.3496 cross-entropy loss: 0.3496
Epoch 0	Test (client-0):	Loss 1.3117 (1.3117)	Prec@1 75.000 (75.000)
 * Prec@1 71.511
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[84/240][0/162][client-0] train loss: 0.3239 cross-entropy loss: 0.3239
log--[84/240][161/162][client-0] train loss: 0.3451 cross-entropy loss: 0.3451
Epoch 0	Test (client-0):	Loss 1.2908 (1.2908)	Prec@1 73.438 (73.438)
 * Prec@1 72.526
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[85/240][0/162][client-0] train loss: 0.2746 cross-entropy loss: 0.2746
log--[85/240][161/162][client-0] train loss: 0.3389 cross-entropy loss: 0.3389
Epoch 0	Test (client-0):	Loss 1.2072 (1.2072)	Prec@1 75.781 (75.781)
 * Prec@1 72.318
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[86/240][0/162][client-0] train loss: 0.2302 cross-entropy loss: 0.2302
log--[86/240][161/162][client-0] train loss: 0.3330 cross-entropy loss: 0.3330
Epoch 0	Test (client-0):	Loss 1.2194 (1.2194)	Prec@1 76.953 (76.953)
 * Prec@1 73.426
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[87/240][0/162][client-0] train loss: 0.2673 cross-entropy loss: 0.2673
log--[87/240][161/162][client-0] train loss: 0.3166 cross-entropy loss: 0.3166
Epoch 0	Test (client-0):	Loss 1.1377 (1.1377)	Prec@1 75.781 (75.781)
 * Prec@1 72.872
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[88/240][0/162][client-0] train loss: 0.4035 cross-entropy loss: 0.4035
log--[88/240][161/162][client-0] train loss: 0.3163 cross-entropy loss: 0.3163
Epoch 0	Test (client-0):	Loss 1.4319 (1.4319)	Prec@1 75.781 (75.781)
 * Prec@1 72.065
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[89/240][0/162][client-0] train loss: 0.2687 cross-entropy loss: 0.2687
log--[89/240][161/162][client-0] train loss: 0.3139 cross-entropy loss: 0.3139
Epoch 0	Test (client-0):	Loss 1.2877 (1.2877)	Prec@1 77.734 (77.734)
 * Prec@1 73.103
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[90/240][0/162][client-0] train loss: 0.3540 cross-entropy loss: 0.3540
log--[90/240][161/162][client-0] train loss: 0.2996 cross-entropy loss: 0.2996
Epoch 0	Test (client-0):	Loss 1.3171 (1.3171)	Prec@1 73.828 (73.828)
 * Prec@1 71.696
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[91/240][0/162][client-0] train loss: 0.3815 cross-entropy loss: 0.3815
log--[91/240][161/162][client-0] train loss: 0.2916 cross-entropy loss: 0.2916
Epoch 0	Test (client-0):	Loss 1.3690 (1.3690)	Prec@1 73.047 (73.047)
 * Prec@1 71.649
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[92/240][0/162][client-0] train loss: 0.3007 cross-entropy loss: 0.3007
log--[92/240][161/162][client-0] train loss: 0.2899 cross-entropy loss: 0.2899
Epoch 0	Test (client-0):	Loss 1.2284 (1.2284)	Prec@1 77.734 (77.734)
 * Prec@1 71.834
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[93/240][0/162][client-0] train loss: 0.2437 cross-entropy loss: 0.2437
log--[93/240][161/162][client-0] train loss: 0.2651 cross-entropy loss: 0.2651
Epoch 0	Test (client-0):	Loss 1.2590 (1.2590)	Prec@1 76.562 (76.562)
 * Prec@1 71.511
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[94/240][0/162][client-0] train loss: 0.2249 cross-entropy loss: 0.2249
log--[94/240][161/162][client-0] train loss: 0.2749 cross-entropy loss: 0.2749
Epoch 0	Test (client-0):	Loss 1.2202 (1.2202)	Prec@1 75.781 (75.781)
 * Prec@1 72.272
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[95/240][0/162][client-0] train loss: 0.2119 cross-entropy loss: 0.2119
log--[95/240][161/162][client-0] train loss: 0.2746 cross-entropy loss: 0.2746
Epoch 0	Test (client-0):	Loss 1.4109 (1.4109)	Prec@1 73.828 (73.828)
 * Prec@1 71.719
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[96/240][0/162][client-0] train loss: 0.2715 cross-entropy loss: 0.2715
log--[96/240][161/162][client-0] train loss: 0.2577 cross-entropy loss: 0.2577
Epoch 0	Test (client-0):	Loss 1.3126 (1.3126)	Prec@1 74.219 (74.219)
 * Prec@1 72.042
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[97/240][0/162][client-0] train loss: 0.3012 cross-entropy loss: 0.3012
log--[97/240][161/162][client-0] train loss: 0.2617 cross-entropy loss: 0.2617
Epoch 0	Test (client-0):	Loss 1.4746 (1.4746)	Prec@1 73.438 (73.438)
 * Prec@1 71.234
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[98/240][0/162][client-0] train loss: 0.2882 cross-entropy loss: 0.2882
log--[98/240][161/162][client-0] train loss: 0.2651 cross-entropy loss: 0.2651
Epoch 0	Test (client-0):	Loss 1.3838 (1.3838)	Prec@1 74.219 (74.219)
 * Prec@1 72.595
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[99/240][0/162][client-0] train loss: 0.2477 cross-entropy loss: 0.2477
log--[99/240][161/162][client-0] train loss: 0.2536 cross-entropy loss: 0.2536
Epoch 0	Test (client-0):	Loss 1.3169 (1.3169)	Prec@1 75.781 (75.781)
 * Prec@1 71.719
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[100/240][0/162][client-0] train loss: 0.2763 cross-entropy loss: 0.2763
log--[100/240][161/162][client-0] train loss: 0.2446 cross-entropy loss: 0.2446
Epoch 0	Test (client-0):	Loss 1.2261 (1.2261)	Prec@1 76.953 (76.953)
 * Prec@1 71.857
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[101/240][0/162][client-0] train loss: 0.2397 cross-entropy loss: 0.2397
log--[101/240][161/162][client-0] train loss: 0.2409 cross-entropy loss: 0.2409
Epoch 0	Test (client-0):	Loss 1.1558 (1.1558)	Prec@1 78.516 (78.516)
 * Prec@1 73.656
best model saved at: 101
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[102/240][0/162][client-0] train loss: 0.2431 cross-entropy loss: 0.2431
log--[102/240][161/162][client-0] train loss: 0.2416 cross-entropy loss: 0.2416
Epoch 0	Test (client-0):	Loss 1.2022 (1.2022)	Prec@1 75.391 (75.391)
 * Prec@1 71.788
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[103/240][0/162][client-0] train loss: 0.2218 cross-entropy loss: 0.2218
log--[103/240][161/162][client-0] train loss: 0.2316 cross-entropy loss: 0.2316
Epoch 0	Test (client-0):	Loss 1.1770 (1.1770)	Prec@1 76.953 (76.953)
 * Prec@1 72.987
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[104/240][0/162][client-0] train loss: 0.1281 cross-entropy loss: 0.1281
log--[104/240][161/162][client-0] train loss: 0.2229 cross-entropy loss: 0.2229
Epoch 0	Test (client-0):	Loss 1.3103 (1.3103)	Prec@1 76.562 (76.562)
 * Prec@1 72.457
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[105/240][0/162][client-0] train loss: 0.2003 cross-entropy loss: 0.2003
log--[105/240][161/162][client-0] train loss: 0.2297 cross-entropy loss: 0.2297
Epoch 0	Test (client-0):	Loss 1.1853 (1.1853)	Prec@1 77.344 (77.344)
 * Prec@1 72.203
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[106/240][0/162][client-0] train loss: 0.1637 cross-entropy loss: 0.1637
log--[106/240][161/162][client-0] train loss: 0.2181 cross-entropy loss: 0.2181
Epoch 0	Test (client-0):	Loss 1.3167 (1.3167)	Prec@1 77.344 (77.344)
 * Prec@1 73.241
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[107/240][0/162][client-0] train loss: 0.2473 cross-entropy loss: 0.2473
log--[107/240][161/162][client-0] train loss: 0.2294 cross-entropy loss: 0.2294
Epoch 0	Test (client-0):	Loss 1.3521 (1.3521)	Prec@1 75.781 (75.781)
 * Prec@1 70.035
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[108/240][0/162][client-0] train loss: 0.2367 cross-entropy loss: 0.2367
log--[108/240][161/162][client-0] train loss: 0.2217 cross-entropy loss: 0.2217
Epoch 0	Test (client-0):	Loss 1.1369 (1.1369)	Prec@1 79.688 (79.688)
 * Prec@1 72.341
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[109/240][0/162][client-0] train loss: 0.2267 cross-entropy loss: 0.2267
log--[109/240][161/162][client-0] train loss: 0.2135 cross-entropy loss: 0.2135
Epoch 0	Test (client-0):	Loss 1.3554 (1.3554)	Prec@1 74.609 (74.609)
 * Prec@1 72.318
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[110/240][0/162][client-0] train loss: 0.1690 cross-entropy loss: 0.1690
log--[110/240][161/162][client-0] train loss: 0.1987 cross-entropy loss: 0.1987
Epoch 0	Test (client-0):	Loss 1.2544 (1.2544)	Prec@1 76.172 (76.172)
 * Prec@1 71.488
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[111/240][0/162][client-0] train loss: 0.1617 cross-entropy loss: 0.1617
log--[111/240][161/162][client-0] train loss: 0.2091 cross-entropy loss: 0.2091
Epoch 0	Test (client-0):	Loss 1.2125 (1.2125)	Prec@1 77.734 (77.734)
 * Prec@1 72.042
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[112/240][0/162][client-0] train loss: 0.2358 cross-entropy loss: 0.2358
log--[112/240][161/162][client-0] train loss: 0.2048 cross-entropy loss: 0.2048
Epoch 0	Test (client-0):	Loss 1.4111 (1.4111)	Prec@1 75.391 (75.391)
 * Prec@1 70.657
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[113/240][0/162][client-0] train loss: 0.1944 cross-entropy loss: 0.1944
log--[113/240][161/162][client-0] train loss: 0.2048 cross-entropy loss: 0.2048
Epoch 0	Test (client-0):	Loss 1.0689 (1.0689)	Prec@1 80.469 (80.469)
 * Prec@1 73.725
best model saved at: 113
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[114/240][0/162][client-0] train loss: 0.2186 cross-entropy loss: 0.2186
log--[114/240][161/162][client-0] train loss: 0.2023 cross-entropy loss: 0.2023
Epoch 0	Test (client-0):	Loss 1.1601 (1.1601)	Prec@1 79.688 (79.688)
 * Prec@1 72.803
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[115/240][0/162][client-0] train loss: 0.1159 cross-entropy loss: 0.1159
log--[115/240][161/162][client-0] train loss: 0.1934 cross-entropy loss: 0.1934
Epoch 0	Test (client-0):	Loss 1.1396 (1.1396)	Prec@1 78.906 (78.906)
 * Prec@1 72.411
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[116/240][0/162][client-0] train loss: 0.2189 cross-entropy loss: 0.2189
log--[116/240][161/162][client-0] train loss: 0.1988 cross-entropy loss: 0.1988
Epoch 0	Test (client-0):	Loss 1.2443 (1.2443)	Prec@1 77.734 (77.734)
 * Prec@1 71.811
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[117/240][0/162][client-0] train loss: 0.1317 cross-entropy loss: 0.1317
log--[117/240][161/162][client-0] train loss: 0.1911 cross-entropy loss: 0.1911
Epoch 0	Test (client-0):	Loss 1.2581 (1.2581)	Prec@1 77.344 (77.344)
 * Prec@1 71.903
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[118/240][0/162][client-0] train loss: 0.1627 cross-entropy loss: 0.1627
log--[118/240][161/162][client-0] train loss: 0.1995 cross-entropy loss: 0.1995
Epoch 0	Test (client-0):	Loss 1.3165 (1.3165)	Prec@1 76.953 (76.953)
 * Prec@1 72.480
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[119/240][0/162][client-0] train loss: 0.1594 cross-entropy loss: 0.1594
log--[119/240][161/162][client-0] train loss: 0.1796 cross-entropy loss: 0.1796
Epoch 0	Test (client-0):	Loss 1.2399 (1.2399)	Prec@1 78.516 (78.516)
 * Prec@1 71.788
lambd value is: 2.3999999999999995 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[120/240][0/162][client-0] train loss: 0.1624 cross-entropy loss: 0.1624
log--[120/240][161/162][client-0] train loss: 0.1017 cross-entropy loss: 0.1017
Epoch 0	Test (client-0):	Loss 0.9675 (0.9675)	Prec@1 81.250 (81.250)
 * Prec@1 76.817
best model saved at: 120
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[121/240][0/162][client-0] train loss: 0.0796 cross-entropy loss: 0.0796
log--[121/240][161/162][client-0] train loss: 0.0707 cross-entropy loss: 0.0707
Epoch 0	Test (client-0):	Loss 0.9953 (0.9953)	Prec@1 80.469 (80.469)
 * Prec@1 76.840
best model saved at: 121
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[122/240][0/162][client-0] train loss: 0.0542 cross-entropy loss: 0.0542
log--[122/240][161/162][client-0] train loss: 0.0598 cross-entropy loss: 0.0598
Epoch 0	Test (client-0):	Loss 1.0229 (1.0229)	Prec@1 79.297 (79.297)
 * Prec@1 77.047
best model saved at: 122
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[123/240][0/162][client-0] train loss: 0.0726 cross-entropy loss: 0.0726
log--[123/240][161/162][client-0] train loss: 0.0501 cross-entropy loss: 0.0501
Epoch 0	Test (client-0):	Loss 0.9761 (0.9761)	Prec@1 80.859 (80.859)
 * Prec@1 77.301
best model saved at: 123
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[124/240][0/162][client-0] train loss: 0.0432 cross-entropy loss: 0.0432
log--[124/240][161/162][client-0] train loss: 0.0475 cross-entropy loss: 0.0475
Epoch 0	Test (client-0):	Loss 0.9770 (0.9770)	Prec@1 81.641 (81.641)
 * Prec@1 77.486
best model saved at: 124
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[125/240][0/162][client-0] train loss: 0.0417 cross-entropy loss: 0.0417
log--[125/240][161/162][client-0] train loss: 0.0442 cross-entropy loss: 0.0442
Epoch 0	Test (client-0):	Loss 0.9998 (0.9998)	Prec@1 80.469 (80.469)
 * Prec@1 77.832
best model saved at: 125
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[126/240][0/162][client-0] train loss: 0.0294 cross-entropy loss: 0.0294
log--[126/240][161/162][client-0] train loss: 0.0427 cross-entropy loss: 0.0427
Epoch 0	Test (client-0):	Loss 0.9966 (0.9966)	Prec@1 80.469 (80.469)
 * Prec@1 77.347
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[127/240][0/162][client-0] train loss: 0.0509 cross-entropy loss: 0.0509
log--[127/240][161/162][client-0] train loss: 0.0392 cross-entropy loss: 0.0392
Epoch 0	Test (client-0):	Loss 1.0174 (1.0174)	Prec@1 79.297 (79.297)
 * Prec@1 77.670
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[128/240][0/162][client-0] train loss: 0.0588 cross-entropy loss: 0.0588
log--[128/240][161/162][client-0] train loss: 0.0390 cross-entropy loss: 0.0390
Epoch 0	Test (client-0):	Loss 1.0288 (1.0288)	Prec@1 79.297 (79.297)
 * Prec@1 77.347
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[129/240][0/162][client-0] train loss: 0.0269 cross-entropy loss: 0.0269
log--[129/240][161/162][client-0] train loss: 0.0343 cross-entropy loss: 0.0343
Epoch 0	Test (client-0):	Loss 0.9796 (0.9796)	Prec@1 80.078 (80.078)
 * Prec@1 77.762
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[130/240][0/162][client-0] train loss: 0.0639 cross-entropy loss: 0.0639
log--[130/240][161/162][client-0] train loss: 0.0346 cross-entropy loss: 0.0346
Epoch 0	Test (client-0):	Loss 0.9794 (0.9794)	Prec@1 80.469 (80.469)
 * Prec@1 77.647
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[131/240][0/162][client-0] train loss: 0.0791 cross-entropy loss: 0.0791
log--[131/240][161/162][client-0] train loss: 0.0342 cross-entropy loss: 0.0342
Epoch 0	Test (client-0):	Loss 1.0264 (1.0264)	Prec@1 81.641 (81.641)
 * Prec@1 77.486
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[132/240][0/162][client-0] train loss: 0.0189 cross-entropy loss: 0.0189
log--[132/240][161/162][client-0] train loss: 0.0348 cross-entropy loss: 0.0348
Epoch 0	Test (client-0):	Loss 0.9842 (0.9842)	Prec@1 82.422 (82.422)
 * Prec@1 77.578
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[133/240][0/162][client-0] train loss: 0.0287 cross-entropy loss: 0.0287
log--[133/240][161/162][client-0] train loss: 0.0338 cross-entropy loss: 0.0338
Epoch 0	Test (client-0):	Loss 0.9841 (0.9841)	Prec@1 81.641 (81.641)
 * Prec@1 77.693
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[134/240][0/162][client-0] train loss: 0.0398 cross-entropy loss: 0.0398
log--[134/240][161/162][client-0] train loss: 0.0346 cross-entropy loss: 0.0346
Epoch 0	Test (client-0):	Loss 0.9819 (0.9819)	Prec@1 80.078 (80.078)
 * Prec@1 77.047
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[135/240][0/162][client-0] train loss: 0.0314 cross-entropy loss: 0.0314
log--[135/240][161/162][client-0] train loss: 0.0303 cross-entropy loss: 0.0303
Epoch 0	Test (client-0):	Loss 0.9725 (0.9725)	Prec@1 79.688 (79.688)
 * Prec@1 77.555
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[136/240][0/162][client-0] train loss: 0.0524 cross-entropy loss: 0.0524
log--[136/240][161/162][client-0] train loss: 0.0311 cross-entropy loss: 0.0311
Epoch 0	Test (client-0):	Loss 0.9710 (0.9710)	Prec@1 80.078 (80.078)
 * Prec@1 77.693
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[137/240][0/162][client-0] train loss: 0.0110 cross-entropy loss: 0.0110
log--[137/240][161/162][client-0] train loss: 0.0295 cross-entropy loss: 0.0295
Epoch 0	Test (client-0):	Loss 0.9869 (0.9869)	Prec@1 80.469 (80.469)
 * Prec@1 77.278
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[138/240][0/162][client-0] train loss: 0.0504 cross-entropy loss: 0.0504
log--[138/240][161/162][client-0] train loss: 0.0271 cross-entropy loss: 0.0271
Epoch 0	Test (client-0):	Loss 0.9847 (0.9847)	Prec@1 81.641 (81.641)
 * Prec@1 77.739
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[139/240][0/162][client-0] train loss: 0.0172 cross-entropy loss: 0.0172
log--[139/240][161/162][client-0] train loss: 0.0285 cross-entropy loss: 0.0285
Epoch 0	Test (client-0):	Loss 1.0172 (1.0172)	Prec@1 79.297 (79.297)
 * Prec@1 77.278
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[140/240][0/162][client-0] train loss: 0.0239 cross-entropy loss: 0.0239
log--[140/240][161/162][client-0] train loss: 0.0276 cross-entropy loss: 0.0276
Epoch 0	Test (client-0):	Loss 1.0056 (1.0056)	Prec@1 81.641 (81.641)
 * Prec@1 77.578
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[141/240][0/162][client-0] train loss: 0.0399 cross-entropy loss: 0.0399
log--[141/240][161/162][client-0] train loss: 0.0269 cross-entropy loss: 0.0269
Epoch 0	Test (client-0):	Loss 0.9743 (0.9743)	Prec@1 80.859 (80.859)
 * Prec@1 77.878
best model saved at: 141
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[142/240][0/162][client-0] train loss: 0.0102 cross-entropy loss: 0.0102
log--[142/240][161/162][client-0] train loss: 0.0265 cross-entropy loss: 0.0265
Epoch 0	Test (client-0):	Loss 0.9976 (0.9976)	Prec@1 80.859 (80.859)
 * Prec@1 77.301
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[143/240][0/162][client-0] train loss: 0.0218 cross-entropy loss: 0.0218
log--[143/240][161/162][client-0] train loss: 0.0267 cross-entropy loss: 0.0267
Epoch 0	Test (client-0):	Loss 0.9540 (0.9540)	Prec@1 80.859 (80.859)
 * Prec@1 77.716
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[144/240][0/162][client-0] train loss: 0.0218 cross-entropy loss: 0.0218
log--[144/240][161/162][client-0] train loss: 0.0253 cross-entropy loss: 0.0253
Epoch 0	Test (client-0):	Loss 0.9753 (0.9753)	Prec@1 81.250 (81.250)
 * Prec@1 77.555
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[145/240][0/162][client-0] train loss: 0.0144 cross-entropy loss: 0.0144
log--[145/240][161/162][client-0] train loss: 0.0255 cross-entropy loss: 0.0255
Epoch 0	Test (client-0):	Loss 0.9951 (0.9951)	Prec@1 81.250 (81.250)
 * Prec@1 77.670
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[146/240][0/162][client-0] train loss: 0.0062 cross-entropy loss: 0.0062
log--[146/240][161/162][client-0] train loss: 0.0256 cross-entropy loss: 0.0256
Epoch 0	Test (client-0):	Loss 1.0492 (1.0492)	Prec@1 79.688 (79.688)
 * Prec@1 77.785
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[147/240][0/162][client-0] train loss: 0.0184 cross-entropy loss: 0.0184
log--[147/240][161/162][client-0] train loss: 0.0253 cross-entropy loss: 0.0253
Epoch 0	Test (client-0):	Loss 1.0274 (1.0274)	Prec@1 80.078 (80.078)
 * Prec@1 77.624
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[148/240][0/162][client-0] train loss: 0.0157 cross-entropy loss: 0.0157
log--[148/240][161/162][client-0] train loss: 0.0252 cross-entropy loss: 0.0252
Epoch 0	Test (client-0):	Loss 0.9829 (0.9829)	Prec@1 80.469 (80.469)
 * Prec@1 77.601
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[149/240][0/162][client-0] train loss: 0.0211 cross-entropy loss: 0.0211
log--[149/240][161/162][client-0] train loss: 0.0232 cross-entropy loss: 0.0232
Epoch 0	Test (client-0):	Loss 1.0065 (1.0065)	Prec@1 81.250 (81.250)
 * Prec@1 77.555
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[150/240][0/162][client-0] train loss: 0.0149 cross-entropy loss: 0.0149
log--[150/240][161/162][client-0] train loss: 0.0229 cross-entropy loss: 0.0229
Epoch 0	Test (client-0):	Loss 1.0119 (1.0119)	Prec@1 81.250 (81.250)
 * Prec@1 77.785
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[151/240][0/162][client-0] train loss: 0.0099 cross-entropy loss: 0.0099
log--[151/240][161/162][client-0] train loss: 0.0232 cross-entropy loss: 0.0232
Epoch 0	Test (client-0):	Loss 1.0022 (1.0022)	Prec@1 81.641 (81.641)
 * Prec@1 77.624
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[152/240][0/162][client-0] train loss: 0.0108 cross-entropy loss: 0.0108
log--[152/240][161/162][client-0] train loss: 0.0221 cross-entropy loss: 0.0221
Epoch 0	Test (client-0):	Loss 1.0108 (1.0108)	Prec@1 80.469 (80.469)
 * Prec@1 77.601
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[153/240][0/162][client-0] train loss: 0.0236 cross-entropy loss: 0.0236
log--[153/240][161/162][client-0] train loss: 0.0223 cross-entropy loss: 0.0223
Epoch 0	Test (client-0):	Loss 1.0244 (1.0244)	Prec@1 80.078 (80.078)
 * Prec@1 77.463
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[154/240][0/162][client-0] train loss: 0.0083 cross-entropy loss: 0.0083
log--[154/240][161/162][client-0] train loss: 0.0214 cross-entropy loss: 0.0214
Epoch 0	Test (client-0):	Loss 1.0119 (1.0119)	Prec@1 81.250 (81.250)
 * Prec@1 78.016
best model saved at: 154
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[155/240][0/162][client-0] train loss: 0.0294 cross-entropy loss: 0.0294
log--[155/240][161/162][client-0] train loss: 0.0225 cross-entropy loss: 0.0225
Epoch 0	Test (client-0):	Loss 1.0054 (1.0054)	Prec@1 80.078 (80.078)
 * Prec@1 77.301
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[156/240][0/162][client-0] train loss: 0.0225 cross-entropy loss: 0.0225
log--[156/240][161/162][client-0] train loss: 0.0219 cross-entropy loss: 0.0219
Epoch 0	Test (client-0):	Loss 1.0358 (1.0358)	Prec@1 81.250 (81.250)
 * Prec@1 77.832
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[157/240][0/162][client-0] train loss: 0.0129 cross-entropy loss: 0.0129
log--[157/240][161/162][client-0] train loss: 0.0205 cross-entropy loss: 0.0205
Epoch 0	Test (client-0):	Loss 1.0725 (1.0725)	Prec@1 80.469 (80.469)
 * Prec@1 77.647
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[158/240][0/162][client-0] train loss: 0.0145 cross-entropy loss: 0.0145
log--[158/240][161/162][client-0] train loss: 0.0221 cross-entropy loss: 0.0221
Epoch 0	Test (client-0):	Loss 1.0249 (1.0249)	Prec@1 82.422 (82.422)
 * Prec@1 77.762
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[159/240][0/162][client-0] train loss: 0.0119 cross-entropy loss: 0.0119
log--[159/240][161/162][client-0] train loss: 0.0201 cross-entropy loss: 0.0201
Epoch 0	Test (client-0):	Loss 1.0243 (1.0243)	Prec@1 82.031 (82.031)
 * Prec@1 77.901
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[160/240][0/162][client-0] train loss: 0.0121 cross-entropy loss: 0.0121
log--[160/240][161/162][client-0] train loss: 0.0217 cross-entropy loss: 0.0217
Epoch 0	Test (client-0):	Loss 1.0441 (1.0441)	Prec@1 80.078 (80.078)
 * Prec@1 77.832
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[161/240][0/162][client-0] train loss: 0.0103 cross-entropy loss: 0.0103
log--[161/240][161/162][client-0] train loss: 0.0202 cross-entropy loss: 0.0202
Epoch 0	Test (client-0):	Loss 1.0082 (1.0082)	Prec@1 80.078 (80.078)
 * Prec@1 77.809
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[162/240][0/162][client-0] train loss: 0.0215 cross-entropy loss: 0.0215
log--[162/240][161/162][client-0] train loss: 0.0205 cross-entropy loss: 0.0205
Epoch 0	Test (client-0):	Loss 1.0523 (1.0523)	Prec@1 80.078 (80.078)
 * Prec@1 77.878
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[163/240][0/162][client-0] train loss: 0.0138 cross-entropy loss: 0.0138
log--[163/240][161/162][client-0] train loss: 0.0212 cross-entropy loss: 0.0212
Epoch 0	Test (client-0):	Loss 1.0500 (1.0500)	Prec@1 79.688 (79.688)
 * Prec@1 77.486
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[164/240][0/162][client-0] train loss: 0.0209 cross-entropy loss: 0.0209
log--[164/240][161/162][client-0] train loss: 0.0203 cross-entropy loss: 0.0203
Epoch 0	Test (client-0):	Loss 1.0419 (1.0419)	Prec@1 79.297 (79.297)
 * Prec@1 77.739
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[165/240][0/162][client-0] train loss: 0.0241 cross-entropy loss: 0.0241
log--[165/240][161/162][client-0] train loss: 0.0203 cross-entropy loss: 0.0203
Epoch 0	Test (client-0):	Loss 1.0130 (1.0130)	Prec@1 80.078 (80.078)
 * Prec@1 77.647
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[166/240][0/162][client-0] train loss: 0.0291 cross-entropy loss: 0.0291
log--[166/240][161/162][client-0] train loss: 0.0205 cross-entropy loss: 0.0205
Epoch 0	Test (client-0):	Loss 0.9855 (0.9855)	Prec@1 81.641 (81.641)
 * Prec@1 77.924
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[167/240][0/162][client-0] train loss: 0.0183 cross-entropy loss: 0.0183
log--[167/240][161/162][client-0] train loss: 0.0201 cross-entropy loss: 0.0201
Epoch 0	Test (client-0):	Loss 0.9817 (0.9817)	Prec@1 81.250 (81.250)
 * Prec@1 77.693
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[168/240][0/162][client-0] train loss: 0.0144 cross-entropy loss: 0.0144
log--[168/240][161/162][client-0] train loss: 0.0204 cross-entropy loss: 0.0204
Epoch 0	Test (client-0):	Loss 0.9982 (0.9982)	Prec@1 81.250 (81.250)
 * Prec@1 77.509
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[169/240][0/162][client-0] train loss: 0.0140 cross-entropy loss: 0.0140
log--[169/240][161/162][client-0] train loss: 0.0192 cross-entropy loss: 0.0192
Epoch 0	Test (client-0):	Loss 1.0013 (1.0013)	Prec@1 81.641 (81.641)
 * Prec@1 77.901
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[170/240][0/162][client-0] train loss: 0.0129 cross-entropy loss: 0.0129
log--[170/240][161/162][client-0] train loss: 0.0191 cross-entropy loss: 0.0191
Epoch 0	Test (client-0):	Loss 0.9803 (0.9803)	Prec@1 82.812 (82.812)
 * Prec@1 78.085
best model saved at: 170
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[171/240][0/162][client-0] train loss: 0.0115 cross-entropy loss: 0.0115
log--[171/240][161/162][client-0] train loss: 0.0198 cross-entropy loss: 0.0198
Epoch 0	Test (client-0):	Loss 0.9995 (0.9995)	Prec@1 80.859 (80.859)
 * Prec@1 77.601
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[172/240][0/162][client-0] train loss: 0.0107 cross-entropy loss: 0.0107
log--[172/240][161/162][client-0] train loss: 0.0197 cross-entropy loss: 0.0197
Epoch 0	Test (client-0):	Loss 1.0098 (1.0098)	Prec@1 81.250 (81.250)
 * Prec@1 77.785
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[173/240][0/162][client-0] train loss: 0.0150 cross-entropy loss: 0.0150
log--[173/240][161/162][client-0] train loss: 0.0194 cross-entropy loss: 0.0194
Epoch 0	Test (client-0):	Loss 0.9991 (0.9991)	Prec@1 80.859 (80.859)
 * Prec@1 77.739
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[174/240][0/162][client-0] train loss: 0.0243 cross-entropy loss: 0.0243
log--[174/240][161/162][client-0] train loss: 0.0196 cross-entropy loss: 0.0196
Epoch 0	Test (client-0):	Loss 1.0039 (1.0039)	Prec@1 79.297 (79.297)
 * Prec@1 77.693
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[175/240][0/162][client-0] train loss: 0.0128 cross-entropy loss: 0.0128
log--[175/240][161/162][client-0] train loss: 0.0197 cross-entropy loss: 0.0197
Epoch 0	Test (client-0):	Loss 0.9874 (0.9874)	Prec@1 78.906 (78.906)
 * Prec@1 77.762
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[176/240][0/162][client-0] train loss: 0.0140 cross-entropy loss: 0.0140
log--[176/240][161/162][client-0] train loss: 0.0194 cross-entropy loss: 0.0194
Epoch 0	Test (client-0):	Loss 1.0205 (1.0205)	Prec@1 80.078 (80.078)
 * Prec@1 77.739
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[177/240][0/162][client-0] train loss: 0.0095 cross-entropy loss: 0.0095
log--[177/240][161/162][client-0] train loss: 0.0177 cross-entropy loss: 0.0177
Epoch 0	Test (client-0):	Loss 0.9487 (0.9487)	Prec@1 83.203 (83.203)
 * Prec@1 78.016
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[178/240][0/162][client-0] train loss: 0.0071 cross-entropy loss: 0.0071
log--[178/240][161/162][client-0] train loss: 0.0185 cross-entropy loss: 0.0185
Epoch 0	Test (client-0):	Loss 0.9791 (0.9791)	Prec@1 82.031 (82.031)
 * Prec@1 78.039
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[179/240][0/162][client-0] train loss: 0.0196 cross-entropy loss: 0.0196
log--[179/240][161/162][client-0] train loss: 0.0174 cross-entropy loss: 0.0174
Epoch 0	Test (client-0):	Loss 0.9991 (0.9991)	Prec@1 82.031 (82.031)
 * Prec@1 77.878
lambd value is: 11.999999999999996 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[180/240][0/162][client-0] train loss: 0.0051 cross-entropy loss: 0.0051
log--[180/240][161/162][client-0] train loss: 0.0176 cross-entropy loss: 0.0176
Epoch 0	Test (client-0):	Loss 0.9687 (0.9687)	Prec@1 82.422 (82.422)
 * Prec@1 77.739
lambd value is: 24.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[181/240][0/162][client-0] train loss: 0.0149 cross-entropy loss: 0.0149
log--[181/240][161/162][client-0] train loss: 0.0171 cross-entropy loss: 0.0171
Epoch 0	Test (client-0):	Loss 0.9846 (0.9846)	Prec@1 82.812 (82.812)
 * Prec@1 78.062
lambd value is: 24.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[182/240][0/162][client-0] train loss: 0.0159 cross-entropy loss: 0.0159
log--[182/240][161/162][client-0] train loss: 0.0171 cross-entropy loss: 0.0171
Epoch 0	Test (client-0):	Loss 0.9661 (0.9661)	Prec@1 82.812 (82.812)
 * Prec@1 78.085
lambd value is: 24.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[183/240][0/162][client-0] train loss: 0.0133 cross-entropy loss: 0.0133
log--[183/240][161/162][client-0] train loss: 0.0161 cross-entropy loss: 0.0161
Epoch 0	Test (client-0):	Loss 1.0044 (1.0044)	Prec@1 81.641 (81.641)
 * Prec@1 78.016
lambd value is: 24.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[184/240][0/162][client-0] train loss: 0.0140 cross-entropy loss: 0.0140
log--[184/240][161/162][client-0] train loss: 0.0166 cross-entropy loss: 0.0166
Epoch 0	Test (client-0):	Loss 0.9896 (0.9896)	Prec@1 82.031 (82.031)
 * Prec@1 77.647
lambd value is: 24.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[185/240][0/162][client-0] train loss: 0.0065 cross-entropy loss: 0.0065
log--[185/240][161/162][client-0] train loss: 0.0168 cross-entropy loss: 0.0168
Epoch 0	Test (client-0):	Loss 0.9901 (0.9901)	Prec@1 82.812 (82.812)
 * Prec@1 77.809
lambd value is: 24.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[186/240][0/162][client-0] train loss: 0.0077 cross-entropy loss: 0.0077
log--[186/240][161/162][client-0] train loss: 0.0168 cross-entropy loss: 0.0168
Epoch 0	Test (client-0):	Loss 0.9845 (0.9845)	Prec@1 81.250 (81.250)
 * Prec@1 78.062
lambd value is: 24.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[187/240][0/162][client-0] train loss: 0.0244 cross-entropy loss: 0.0244
log--[187/240][161/162][client-0] train loss: 0.0165 cross-entropy loss: 0.0165
Epoch 0	Test (client-0):	Loss 0.9883 (0.9883)	Prec@1 82.031 (82.031)
 * Prec@1 77.855
lambd value is: 24.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[188/240][0/162][client-0] train loss: 0.0133 cross-entropy loss: 0.0133
log--[188/240][161/162][client-0] train loss: 0.0157 cross-entropy loss: 0.0157
Epoch 0	Test (client-0):	Loss 0.9853 (0.9853)	Prec@1 82.031 (82.031)
 * Prec@1 78.155
best model saved at: 188
lambd value is: 24.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[189/240][0/162][client-0] train loss: 0.0070 cross-entropy loss: 0.0070
log--[189/240][161/162][client-0] train loss: 0.0166 cross-entropy loss: 0.0166
Epoch 0	Test (client-0):	Loss 0.9961 (0.9961)	Prec@1 81.641 (81.641)
 * Prec@1 78.085
lambd value is: 24.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[190/240][0/162][client-0] train loss: 0.0175 cross-entropy loss: 0.0175
log--[190/240][161/162][client-0] train loss: 0.0170 cross-entropy loss: 0.0170
Epoch 0	Test (client-0):	Loss 1.0120 (1.0120)	Prec@1 80.469 (80.469)
 * Prec@1 77.855
lambd value is: 24.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[191/240][0/162][client-0] train loss: 0.0153 cross-entropy loss: 0.0153
log--[191/240][161/162][client-0] train loss: 0.0162 cross-entropy loss: 0.0162
Epoch 0	Test (client-0):	Loss 1.0002 (1.0002)	Prec@1 81.641 (81.641)
 * Prec@1 77.809
lambd value is: 24.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[192/240][0/162][client-0] train loss: 0.0167 cross-entropy loss: 0.0167
log--[192/240][161/162][client-0] train loss: 0.0162 cross-entropy loss: 0.0162
Epoch 0	Test (client-0):	Loss 0.9731 (0.9731)	Prec@1 81.641 (81.641)
 * Prec@1 78.016
lambd value is: 24.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[193/240][0/162][client-0] train loss: 0.0325 cross-entropy loss: 0.0325
log--[193/240][161/162][client-0] train loss: 0.0150 cross-entropy loss: 0.0150
Epoch 0	Test (client-0):	Loss 0.9734 (0.9734)	Prec@1 81.250 (81.250)
 * Prec@1 78.062
lambd value is: 24.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[194/240][0/162][client-0] train loss: 0.0074 cross-entropy loss: 0.0074
log--[194/240][161/162][client-0] train loss: 0.0163 cross-entropy loss: 0.0163
Epoch 0	Test (client-0):	Loss 0.9799 (0.9799)	Prec@1 82.031 (82.031)
 * Prec@1 77.947
lambd value is: 24.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[195/240][0/162][client-0] train loss: 0.0163 cross-entropy loss: 0.0163
log--[195/240][161/162][client-0] train loss: 0.0156 cross-entropy loss: 0.0156
Epoch 0	Test (client-0):	Loss 0.9914 (0.9914)	Prec@1 80.078 (80.078)
 * Prec@1 77.739
lambd value is: 24.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[196/240][0/162][client-0] train loss: 0.0071 cross-entropy loss: 0.0071
log--[196/240][161/162][client-0] train loss: 0.0162 cross-entropy loss: 0.0162
Epoch 0	Test (client-0):	Loss 0.9760 (0.9760)	Prec@1 80.469 (80.469)
 * Prec@1 77.993
lambd value is: 24.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[197/240][0/162][client-0] train loss: 0.0147 cross-entropy loss: 0.0147
log--[197/240][161/162][client-0] train loss: 0.0158 cross-entropy loss: 0.0158
Epoch 0	Test (client-0):	Loss 1.0093 (1.0093)	Prec@1 80.078 (80.078)
 * Prec@1 77.785
lambd value is: 24.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[198/240][0/162][client-0] train loss: 0.0402 cross-entropy loss: 0.0402
log--[198/240][161/162][client-0] train loss: 0.0156 cross-entropy loss: 0.0156
Epoch 0	Test (client-0):	Loss 0.9924 (0.9924)	Prec@1 82.031 (82.031)
 * Prec@1 77.947
lambd value is: 24.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[199/240][0/162][client-0] train loss: 0.0091 cross-entropy loss: 0.0091
log--[199/240][161/162][client-0] train loss: 0.0155 cross-entropy loss: 0.0155
Epoch 0	Test (client-0):	Loss 0.9810 (0.9810)	Prec@1 81.250 (81.250)
 * Prec@1 77.855
lambd value is: 24.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[200/240][0/162][client-0] train loss: 0.0149 cross-entropy loss: 0.0149
log--[200/240][161/162][client-0] train loss: 0.0164 cross-entropy loss: 0.0164
Epoch 0	Test (client-0):	Loss 0.9598 (0.9598)	Prec@1 81.641 (81.641)
 * Prec@1 77.947
lambd value is: 24.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[201/240][0/162][client-0] train loss: 0.0282 cross-entropy loss: 0.0282
log--[201/240][161/162][client-0] train loss: 0.0154 cross-entropy loss: 0.0154
Epoch 0	Test (client-0):	Loss 0.9833 (0.9833)	Prec@1 81.641 (81.641)
 * Prec@1 78.224
best model saved at: 201
lambd value is: 24.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[202/240][0/162][client-0] train loss: 0.0081 cross-entropy loss: 0.0081
log--[202/240][161/162][client-0] train loss: 0.0158 cross-entropy loss: 0.0158
Epoch 0	Test (client-0):	Loss 0.9962 (0.9962)	Prec@1 80.859 (80.859)
 * Prec@1 77.924
lambd value is: 24.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[203/240][0/162][client-0] train loss: 0.0184 cross-entropy loss: 0.0184
log--[203/240][161/162][client-0] train loss: 0.0157 cross-entropy loss: 0.0157
Epoch 0	Test (client-0):	Loss 0.9894 (0.9894)	Prec@1 80.859 (80.859)
 * Prec@1 77.809
lambd value is: 24.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[204/240][0/162][client-0] train loss: 0.0172 cross-entropy loss: 0.0172
log--[204/240][161/162][client-0] train loss: 0.0157 cross-entropy loss: 0.0157
Epoch 0	Test (client-0):	Loss 0.9615 (0.9615)	Prec@1 80.859 (80.859)
 * Prec@1 77.947
lambd value is: 24.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[205/240][0/162][client-0] train loss: 0.0052 cross-entropy loss: 0.0052
log--[205/240][161/162][client-0] train loss: 0.0153 cross-entropy loss: 0.0153
Epoch 0	Test (client-0):	Loss 0.9860 (0.9860)	Prec@1 81.641 (81.641)
 * Prec@1 78.016
lambd value is: 24.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[206/240][0/162][client-0] train loss: 0.0208 cross-entropy loss: 0.0208
log--[206/240][161/162][client-0] train loss: 0.0150 cross-entropy loss: 0.0150
Epoch 0	Test (client-0):	Loss 0.9758 (0.9758)	Prec@1 81.641 (81.641)
 * Prec@1 77.924
lambd value is: 24.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[207/240][0/162][client-0] train loss: 0.0052 cross-entropy loss: 0.0052
log--[207/240][161/162][client-0] train loss: 0.0162 cross-entropy loss: 0.0162
Epoch 0	Test (client-0):	Loss 1.0099 (1.0099)	Prec@1 80.078 (80.078)
 * Prec@1 78.108
lambd value is: 24.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[208/240][0/162][client-0] train loss: 0.0353 cross-entropy loss: 0.0353
log--[208/240][161/162][client-0] train loss: 0.0153 cross-entropy loss: 0.0153
Epoch 0	Test (client-0):	Loss 0.9969 (0.9969)	Prec@1 80.078 (80.078)
 * Prec@1 78.039
lambd value is: 24.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[209/240][0/162][client-0] train loss: 0.0286 cross-entropy loss: 0.0286
log--[209/240][161/162][client-0] train loss: 0.0152 cross-entropy loss: 0.0152
Epoch 0	Test (client-0):	Loss 0.9991 (0.9991)	Prec@1 81.250 (81.250)
 * Prec@1 77.901
lambd value is: 24.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[210/240][0/162][client-0] train loss: 0.0151 cross-entropy loss: 0.0151
log--[210/240][161/162][client-0] train loss: 0.0157 cross-entropy loss: 0.0157
Epoch 0	Test (client-0):	Loss 0.9994 (0.9994)	Prec@1 81.250 (81.250)
 * Prec@1 78.062
lambd value is: 24.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[211/240][0/162][client-0] train loss: 0.0155 cross-entropy loss: 0.0155
log--[211/240][161/162][client-0] train loss: 0.0158 cross-entropy loss: 0.0158
Epoch 0	Test (client-0):	Loss 0.9868 (0.9868)	Prec@1 80.078 (80.078)
 * Prec@1 77.970
lambd value is: 24.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[212/240][0/162][client-0] train loss: 0.0114 cross-entropy loss: 0.0114
log--[212/240][161/162][client-0] train loss: 0.0155 cross-entropy loss: 0.0155
Epoch 0	Test (client-0):	Loss 0.9962 (0.9962)	Prec@1 81.641 (81.641)
 * Prec@1 78.062
lambd value is: 24.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[213/240][0/162][client-0] train loss: 0.0140 cross-entropy loss: 0.0140
log--[213/240][161/162][client-0] train loss: 0.0139 cross-entropy loss: 0.0139
Epoch 0	Test (client-0):	Loss 0.9783 (0.9783)	Prec@1 80.859 (80.859)
 * Prec@1 77.924
lambd value is: 24.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[214/240][0/162][client-0] train loss: 0.0099 cross-entropy loss: 0.0099
log--[214/240][161/162][client-0] train loss: 0.0150 cross-entropy loss: 0.0150
Epoch 0	Test (client-0):	Loss 0.9690 (0.9690)	Prec@1 81.250 (81.250)
 * Prec@1 77.901
lambd value is: 24.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[215/240][0/162][client-0] train loss: 0.0085 cross-entropy loss: 0.0085
log--[215/240][161/162][client-0] train loss: 0.0145 cross-entropy loss: 0.0145
Epoch 0	Test (client-0):	Loss 0.9647 (0.9647)	Prec@1 82.812 (82.812)
 * Prec@1 78.178
lambd value is: 24.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[216/240][0/162][client-0] train loss: 0.0320 cross-entropy loss: 0.0320
log--[216/240][161/162][client-0] train loss: 0.0143 cross-entropy loss: 0.0143
Epoch 0	Test (client-0):	Loss 0.9714 (0.9714)	Prec@1 81.641 (81.641)
 * Prec@1 78.224
lambd value is: 24.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[217/240][0/162][client-0] train loss: 0.0151 cross-entropy loss: 0.0151
log--[217/240][161/162][client-0] train loss: 0.0150 cross-entropy loss: 0.0150
Epoch 0	Test (client-0):	Loss 0.9720 (0.9720)	Prec@1 81.250 (81.250)
 * Prec@1 77.878
lambd value is: 24.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[218/240][0/162][client-0] train loss: 0.0163 cross-entropy loss: 0.0163
log--[218/240][161/162][client-0] train loss: 0.0151 cross-entropy loss: 0.0151
Epoch 0	Test (client-0):	Loss 0.9566 (0.9566)	Prec@1 82.031 (82.031)
 * Prec@1 77.924
lambd value is: 24.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[219/240][0/162][client-0] train loss: 0.0101 cross-entropy loss: 0.0101
log--[219/240][161/162][client-0] train loss: 0.0146 cross-entropy loss: 0.0146
Epoch 0	Test (client-0):	Loss 0.9974 (0.9974)	Prec@1 80.469 (80.469)
 * Prec@1 77.970
lambd value is: 24.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[220/240][0/162][client-0] train loss: 0.0226 cross-entropy loss: 0.0226
log--[220/240][161/162][client-0] train loss: 0.0145 cross-entropy loss: 0.0145
Epoch 0	Test (client-0):	Loss 0.9946 (0.9946)	Prec@1 80.859 (80.859)
 * Prec@1 78.293
best model saved at: 220
lambd value is: 24.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[221/240][0/162][client-0] train loss: 0.0206 cross-entropy loss: 0.0206
log--[221/240][161/162][client-0] train loss: 0.0149 cross-entropy loss: 0.0149
Epoch 0	Test (client-0):	Loss 0.9856 (0.9856)	Prec@1 81.250 (81.250)
 * Prec@1 77.878
lambd value is: 24.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[222/240][0/162][client-0] train loss: 0.0340 cross-entropy loss: 0.0340
log--[222/240][161/162][client-0] train loss: 0.0148 cross-entropy loss: 0.0148
Epoch 0	Test (client-0):	Loss 1.0080 (1.0080)	Prec@1 81.641 (81.641)
 * Prec@1 77.878
lambd value is: 24.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[223/240][0/162][client-0] train loss: 0.0217 cross-entropy loss: 0.0217
log--[223/240][161/162][client-0] train loss: 0.0149 cross-entropy loss: 0.0149
Epoch 0	Test (client-0):	Loss 0.9947 (0.9947)	Prec@1 80.859 (80.859)
 * Prec@1 77.993
lambd value is: 24.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[224/240][0/162][client-0] train loss: 0.0070 cross-entropy loss: 0.0070
log--[224/240][161/162][client-0] train loss: 0.0153 cross-entropy loss: 0.0153
Epoch 0	Test (client-0):	Loss 0.9747 (0.9747)	Prec@1 81.250 (81.250)
 * Prec@1 78.270
lambd value is: 24.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[225/240][0/162][client-0] train loss: 0.0113 cross-entropy loss: 0.0113
log--[225/240][161/162][client-0] train loss: 0.0149 cross-entropy loss: 0.0149
Epoch 0	Test (client-0):	Loss 0.9988 (0.9988)	Prec@1 81.250 (81.250)
 * Prec@1 78.108
lambd value is: 24.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[226/240][0/162][client-0] train loss: 0.0116 cross-entropy loss: 0.0116
log--[226/240][161/162][client-0] train loss: 0.0149 cross-entropy loss: 0.0149
Epoch 0	Test (client-0):	Loss 0.9991 (0.9991)	Prec@1 81.250 (81.250)
 * Prec@1 78.224
lambd value is: 24.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[227/240][0/162][client-0] train loss: 0.0121 cross-entropy loss: 0.0121
log--[227/240][161/162][client-0] train loss: 0.0147 cross-entropy loss: 0.0147
Epoch 0	Test (client-0):	Loss 0.9821 (0.9821)	Prec@1 81.641 (81.641)
 * Prec@1 78.016
lambd value is: 24.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[228/240][0/162][client-0] train loss: 0.0227 cross-entropy loss: 0.0227
log--[228/240][161/162][client-0] train loss: 0.0146 cross-entropy loss: 0.0146
Epoch 0	Test (client-0):	Loss 0.9764 (0.9764)	Prec@1 81.250 (81.250)
 * Prec@1 77.993
lambd value is: 24.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[229/240][0/162][client-0] train loss: 0.0189 cross-entropy loss: 0.0189
log--[229/240][161/162][client-0] train loss: 0.0142 cross-entropy loss: 0.0142
Epoch 0	Test (client-0):	Loss 0.9935 (0.9935)	Prec@1 80.469 (80.469)
 * Prec@1 77.901
lambd value is: 24.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[230/240][0/162][client-0] train loss: 0.0039 cross-entropy loss: 0.0039
log--[230/240][161/162][client-0] train loss: 0.0151 cross-entropy loss: 0.0151
Epoch 0	Test (client-0):	Loss 0.9866 (0.9866)	Prec@1 81.641 (81.641)
 * Prec@1 78.131
lambd value is: 24.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[231/240][0/162][client-0] train loss: 0.0118 cross-entropy loss: 0.0118
log--[231/240][161/162][client-0] train loss: 0.0145 cross-entropy loss: 0.0145
Epoch 0	Test (client-0):	Loss 0.9656 (0.9656)	Prec@1 81.250 (81.250)
 * Prec@1 78.155
lambd value is: 24.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[232/240][0/162][client-0] train loss: 0.0077 cross-entropy loss: 0.0077
log--[232/240][161/162][client-0] train loss: 0.0143 cross-entropy loss: 0.0143
Epoch 0	Test (client-0):	Loss 0.9835 (0.9835)	Prec@1 80.859 (80.859)
 * Prec@1 77.832
lambd value is: 24.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[233/240][0/162][client-0] train loss: 0.0098 cross-entropy loss: 0.0098
log--[233/240][161/162][client-0] train loss: 0.0143 cross-entropy loss: 0.0143
Epoch 0	Test (client-0):	Loss 1.0052 (1.0052)	Prec@1 80.469 (80.469)
 * Prec@1 78.108
lambd value is: 24.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[234/240][0/162][client-0] train loss: 0.0115 cross-entropy loss: 0.0115
log--[234/240][161/162][client-0] train loss: 0.0143 cross-entropy loss: 0.0143
Epoch 0	Test (client-0):	Loss 0.9653 (0.9653)	Prec@1 82.031 (82.031)
 * Prec@1 77.901
lambd value is: 24.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[235/240][0/162][client-0] train loss: 0.0094 cross-entropy loss: 0.0094
log--[235/240][161/162][client-0] train loss: 0.0158 cross-entropy loss: 0.0158
Epoch 0	Test (client-0):	Loss 1.0092 (1.0092)	Prec@1 80.469 (80.469)
 * Prec@1 77.878
lambd value is: 24.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[236/240][0/162][client-0] train loss: 0.0068 cross-entropy loss: 0.0068
log--[236/240][161/162][client-0] train loss: 0.0145 cross-entropy loss: 0.0145
Epoch 0	Test (client-0):	Loss 0.9787 (0.9787)	Prec@1 80.078 (80.078)
 * Prec@1 78.016
lambd value is: 24.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[237/240][0/162][client-0] train loss: 0.0134 cross-entropy loss: 0.0134
log--[237/240][161/162][client-0] train loss: 0.0148 cross-entropy loss: 0.0148
Epoch 0	Test (client-0):	Loss 0.9865 (0.9865)	Prec@1 82.422 (82.422)
 * Prec@1 78.155
lambd value is: 24.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[238/240][0/162][client-0] train loss: 0.0116 cross-entropy loss: 0.0116
log--[238/240][161/162][client-0] train loss: 0.0151 cross-entropy loss: 0.0151
Epoch 0	Test (client-0):	Loss 1.0013 (1.0013)	Prec@1 81.641 (81.641)
 * Prec@1 77.785
lambd value is: 24.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[239/240][0/162][client-0] train loss: 0.0126 cross-entropy loss: 0.0126
log--[239/240][161/162][client-0] train loss: 0.0147 cross-entropy loss: 0.0147
Epoch 0	Test (client-0):	Loss 0.9844 (0.9844)	Prec@1 80.469 (80.469)
 * Prec@1 78.339
best model saved at: 239
lambd value is: 24.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[240/240][0/162][client-0] train loss: 0.0247 cross-entropy loss: 0.0247
log--[240/240][161/162][client-0] train loss: 0.0149 cross-entropy loss: 0.0149
Epoch 0	Test (client-0):	Loss 0.9721 (0.9721)	Prec@1 81.250 (81.250)
 * Prec@1 77.993
lambd value is: 24.0 learning rate is: 8.000000000000002e-05
Best Average Validation Accuracy is 78.33910034778071
2025-12-24 02:49:46.613862: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-24 02:49:46.656208: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-24 02:49:47.408162: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
Split Learning Scheme: Overall Cutting_layer 4/13
Total number of batches per epoch for each client is  162
64
original channel size of smashed-data is 128
added bottleneck, new channel size of smashed-data is 8
local:
Sequential(
  (0): ResizeLayer()
  (1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): ReLU(inplace=True)
  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): ReLU(inplace=True)
  (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (9): Conv2d(128, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (10): LCALayer()
  (11): Sigmoid()
)
cloud:
Sequential(
  (0): Conv2d(8, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): ReLU(inplace=True)
  (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (6): ReLU(inplace=True)
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (10): ReLU(inplace=True)
  (11): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (12): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (13): ReLU(inplace=True)
  (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (17): ReLU(inplace=True)
  (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (20): ReLU(inplace=True)
  (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
classifier:
Sequential(
  (0): Dropout(p=0.5, inplace=False)
  (1): Linear(in_features=512, out_features=512, bias=True)
  (2): ReLU(inplace=True)
  (3): Dropout(p=0.5, inplace=False)
  (4): Linear(in_features=512, out_features=512, bias=True)
  (5): ReLU(inplace=True)
  (6): Linear(in_features=512, out_features=530, bias=True)
)
the test model is: best
load client 0's local
./saves/facescrub/SCA_new_slotatt_opt_lg1_thre0.125/pretrain_False_lambd_24_noise_0.025_epoch_240_bottleneck_noRELU_C8S1_log_1_ATstrength_0.3_lr_0.05_varthres_0.125/checkpoint_f_best.tar
load cloud
load classifier
Model's smashed-data size is torch.Size([1, 8, 16, 16])
Sequential(
  85.26 k, 1.992% Params, 86.9 MMac, 1.001% MACs, 
  (0): ResizeLayer(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
  (1): Conv2d(1.79 k, 0.042% Params, 7.34 MMac, 0.085% MACs, 3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (2): BatchNorm2d(128, 0.003% Params, 524.29 KMac, 0.006% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): ReLU(0, 0.000% Params, 262.14 KMac, 0.003% MACs, inplace=True)
  (4): MaxPool2d(0, 0.000% Params, 262.14 KMac, 0.003% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (5): Conv2d(73.86 k, 1.726% Params, 75.63 MMac, 0.871% MACs, 64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (6): BatchNorm2d(256, 0.006% Params, 262.14 KMac, 0.003% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (7): ReLU(0, 0.000% Params, 131.07 KMac, 0.002% MACs, inplace=True)
  (8): MaxPool2d(0, 0.000% Params, 131.07 KMac, 0.002% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (9): Conv2d(9.22 k, 0.216% Params, 2.36 MMac, 0.027% MACs, 128, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (10): LCALayer(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
  (11): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
)
FLOPs: 8.68 GMac, Parameters: 4.28 M
VGG(
  10.04 M, 70.537% Params, 619.53 MMac, 6.723% MACs, 
  (local): Sequential(
    85.26 k, 0.599% Params, 86.9 MMac, 0.943% MACs, 
    (0): ResizeLayer(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
    (1): Conv2d(1.79 k, 0.013% Params, 7.34 MMac, 0.080% MACs, 3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (2): BatchNorm2d(128, 0.001% Params, 524.29 KMac, 0.006% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): ReLU(0, 0.000% Params, 262.14 KMac, 0.003% MACs, inplace=True)
    (4): MaxPool2d(0, 0.000% Params, 262.14 KMac, 0.003% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(73.86 k, 0.519% Params, 75.63 MMac, 0.821% MACs, 64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): BatchNorm2d(256, 0.002% Params, 262.14 KMac, 0.003% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (7): ReLU(0, 0.000% Params, 131.07 KMac, 0.001% MACs, inplace=True)
    (8): MaxPool2d(0, 0.000% Params, 131.07 KMac, 0.001% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (9): Conv2d(9.22 k, 0.065% Params, 2.36 MMac, 0.026% MACs, 128, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (10): LCALayer(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
    (11): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
  )
  (cloud): Sequential(
    9.16 M, 64.339% Params, 531.83 MMac, 5.772% MACs, 
    (0): Conv2d(9.34 k, 0.066% Params, 2.39 MMac, 0.026% MACs, 8, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): Conv2d(295.17 k, 2.073% Params, 75.56 MMac, 0.820% MACs, 128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (2): BatchNorm2d(512, 0.004% Params, 131.07 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): ReLU(0, 0.000% Params, 65.54 KMac, 0.001% MACs, inplace=True)
    (4): Conv2d(590.08 k, 4.145% Params, 151.06 MMac, 1.639% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): BatchNorm2d(512, 0.004% Params, 131.07 KMac, 0.001% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU(0, 0.000% Params, 65.54 KMac, 0.001% MACs, inplace=True)
    (7): MaxPool2d(0, 0.000% Params, 65.54 KMac, 0.001% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Conv2d(1.18 M, 8.290% Params, 75.53 MMac, 0.820% MACs, 256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): BatchNorm2d(1.02 k, 0.007% Params, 65.54 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): ReLU(0, 0.000% Params, 32.77 KMac, 0.000% MACs, inplace=True)
    (11): Conv2d(2.36 M, 16.576% Params, 151.03 MMac, 1.639% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (12): BatchNorm2d(1.02 k, 0.007% Params, 65.54 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (13): ReLU(0, 0.000% Params, 32.77 KMac, 0.000% MACs, inplace=True)
    (14): MaxPool2d(0, 0.000% Params, 32.77 KMac, 0.000% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (15): Conv2d(2.36 M, 16.576% Params, 37.76 MMac, 0.410% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (16): BatchNorm2d(1.02 k, 0.007% Params, 16.38 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (17): ReLU(0, 0.000% Params, 8.19 KMac, 0.000% MACs, inplace=True)
    (18): Conv2d(2.36 M, 16.576% Params, 37.76 MMac, 0.410% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (19): BatchNorm2d(1.02 k, 0.007% Params, 16.38 KMac, 0.000% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): ReLU(0, 0.000% Params, 8.19 KMac, 0.000% MACs, inplace=True)
    (21): MaxPool2d(0, 0.000% Params, 8.19 KMac, 0.000% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (classifier): Sequential(
    797.2 k, 5.600% Params, 798.23 KMac, 0.009% MACs, 
    (0): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.5, inplace=False)
    (1): Linear(262.66 k, 1.845% Params, 262.66 KMac, 0.003% MACs, in_features=512, out_features=512, bias=True)
    (2): ReLU(0, 0.000% Params, 512.0 Mac, 0.000% MACs, inplace=True)
    (3): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.5, inplace=False)
    (4): Linear(262.66 k, 1.845% Params, 262.66 KMac, 0.003% MACs, in_features=512, out_features=512, bias=True)
    (5): ReLU(0, 0.000% Params, 512.0 Mac, 0.000% MACs, inplace=True)
    (6): Linear(271.89 k, 1.910% Params, 271.89 KMac, 0.003% MACs, in_features=512, out_features=530, bias=True)
  )
)
FLOPs: 9.21 GMac, Parameters: 14.24 M
Model 1 Params: 4279560, Model 2 Params: 14236058
: 532.36 ms
Epoch 0	Test (client-0):	Loss 0.9783 (0.9783)	Prec@1 81.641 (81.641)
 * Prec@1 78.178
Best Average Validation Accuracy is 78.17762379189814
Generating IR ...... (may take a while)
run the attack for training time
2.0 torch.Size([1, 8, 16, 16])
Epoch 1, Current LR: 0.0009990232305719944
the pred acc is: 3.5986159068352896
epoch [1/50], train_loss 0.0340 (0.0343), val_loss 0.0246 (0.0269)
Epoch 2, Current LR: 0.0009960967771506664
the pred acc is: 3.2756632013991904
epoch [2/50], train_loss 0.0309 (0.0260), val_loss 0.0224 (0.0246)
Epoch 3, Current LR: 0.0009912321891107007
the pred acc is: 3.9907727757952207
epoch [3/50], train_loss 0.0299 (0.0244), val_loss 0.0219 (0.0241)
Epoch 4, Current LR: 0.0009844486647586721
the pred acc is: 4.659746251001765
epoch [4/50], train_loss 0.0291 (0.0237), val_loss 0.0213 (0.0234)
Epoch 5, Current LR: 0.0009757729755661009
the pred acc is: 6.251441745362067
epoch [5/50], train_loss 0.0286 (0.0231), val_loss 0.0208 (0.0231)
Epoch 6, Current LR: 0.0009652393605146842
the pred acc is: 6.943483267853416
epoch [6/50], train_loss 0.0280 (0.0227), val_loss 0.0208 (0.0228)
Epoch 7, Current LR: 0.0009528893909706795
the pred acc is: 7.82006919634246
epoch [7/50], train_loss 0.0281 (0.0225), val_loss 0.0207 (0.0228)
Epoch 8, Current LR: 0.0009387718066217122
the pred acc is: 8.535178757593844
epoch [8/50], train_loss 0.0276 (0.0223), val_loss 0.0205 (0.0225)
Epoch 9, Current LR: 0.0009229423231234972
the pred acc is: 8.927335644318296
epoch [9/50], train_loss 0.0275 (0.0219), val_loss 0.0202 (0.0222)
Epoch 10, Current LR: 0.0009054634122155987
the pred acc is: 8.673587086071468
epoch [10/50], train_loss 0.0271 (0.0217), val_loss 0.0201 (0.0222)
Epoch 11, Current LR: 0.0008864040551740153
the pred acc is: 8.673587081451592
epoch [11/50], train_loss 0.0268 (0.0216), val_loss 0.0203 (0.0221)
Epoch 12, Current LR: 0.0008658394705735984
the pred acc is: 8.027681641100187
epoch [12/50], train_loss 0.0270 (0.0213), val_loss 0.0201 (0.0220)
Epoch 13, Current LR: 0.0008438508174347006
the pred acc is: 8.027681641100187
epoch [13/50], train_loss 0.0269 (0.0210), val_loss 0.0201 (0.0218)
Epoch 14, Current LR: 0.000820524874925601
the pred acc is: 8.627450982262106
epoch [14/50], train_loss 0.0271 (0.0210), val_loss 0.0203 (0.0220)
Epoch 15, Current LR: 0.000795953699884774
the pred acc is: 9.434832748822275
epoch [15/50], train_loss 0.0263 (0.0209), val_loss 0.0201 (0.0216)
Epoch 16, Current LR: 0.000770234263514603
the pred acc is: 10.149942343292764
epoch [16/50], train_loss 0.0264 (0.0206), val_loss 0.0201 (0.0216)
Epoch 17, Current LR: 0.0007434680686803488
the pred acc is: 9.411764698072563
epoch [17/50], train_loss 0.0260 (0.0202), val_loss 0.0200 (0.0218)
Epoch 18, Current LR: 0.0007157607493247108
the pred acc is: 9.873125703387057
epoch [18/50], train_loss 0.0257 (0.0201), val_loss 0.0200 (0.0218)
Epoch 19, Current LR: 0.0006872216535789154
the pred acc is: 10.080738176744015
epoch [19/50], train_loss 0.0259 (0.0200), val_loss 0.0202 (0.0220)
Epoch 20, Current LR: 0.0006579634122155987
the pred acc is: 9.596309104070256
epoch [20/50], train_loss 0.0251 (0.0198), val_loss 0.0200 (0.0219)
Epoch 21, Current LR: 0.0006281014941466028
the pred acc is: 10.242214531991996
epoch [21/50], train_loss 0.0244 (0.0196), val_loss 0.0202 (0.0218)
Epoch 22, Current LR: 0.0005977537507199335
the pred acc is: 10.080738148144786
epoch [22/50], train_loss 0.0240 (0.0192), val_loss 0.0199 (0.0217)
Epoch 23, Current LR: 0.0005670399506143305
the pred acc is: 9.573241043640806
epoch [23/50], train_loss 0.0237 (0.0189), val_loss 0.0194 (0.0214)
Epoch 24, Current LR: 0.0005360813071670099
the pred acc is: 9.642445205569679
epoch [24/50], train_loss 0.0235 (0.0188), val_loss 0.0199 (0.0218)
Epoch 25, Current LR: 0.0005049999999999998
the pred acc is: 10.588235278938056
epoch [25/50], train_loss 0.0233 (0.0187), val_loss 0.0197 (0.0219)
Epoch 26, Current LR: 0.0004739186928329897
the pred acc is: 10.173010372813048
epoch [26/50], train_loss 0.0225 (0.0185), val_loss 0.0195 (0.0218)
Epoch 27, Current LR: 0.0004429600493856692
the pred acc is: 10.334486706391614
epoch [27/50], train_loss 0.0227 (0.0188), val_loss 0.0191 (0.0211)
Epoch 28, Current LR: 0.0004122462492800661
the pred acc is: 10.88811993637437
epoch [28/50], train_loss 0.0223 (0.0185), val_loss 0.0187 (0.0208)
Epoch 29, Current LR: 0.0003818985058533967
the pred acc is: 11.026528252862317
epoch [29/50], train_loss 0.0217 (0.0178), val_loss 0.0190 (0.0207)
Epoch 30, Current LR: 0.00035203658778440103
the pred acc is: 10.772779682625812
epoch [30/50], train_loss 0.0213 (0.0174), val_loss 0.0188 (0.0206)
Epoch 31, Current LR: 0.00032277834642108444
the pred acc is: 11.672433678474118
epoch [31/50], train_loss 0.0213 (0.0171), val_loss 0.0189 (0.0205)
Epoch 32, Current LR: 0.0002942392506752889
the pred acc is: 10.980392151362894
epoch [32/50], train_loss 0.0210 (0.0168), val_loss 0.0191 (0.0206)
Epoch 33, Current LR: 0.00026653193131965077
the pred acc is: 11.8339100337221
epoch [33/50], train_loss 0.0208 (0.0166), val_loss 0.0190 (0.0208)
Epoch 34, Current LR: 0.00023976573648539642
the pred acc is: 11.118800446181424
epoch [34/50], train_loss 0.0202 (0.0165), val_loss 0.0189 (0.0208)
Epoch 35, Current LR: 0.00021404630011522574
the pred acc is: 11.603229526224982
epoch [35/50], train_loss 0.0198 (0.0162), val_loss 0.0188 (0.0207)
Epoch 36, Current LR: 0.00018947512507439847
the pred acc is: 11.234140728529212
epoch [36/50], train_loss 0.0198 (0.0160), val_loss 0.0192 (0.0208)
Epoch 37, Current LR: 0.000166149182565299
the pred acc is: 11.257208755299569
epoch [37/50], train_loss 0.0191 (0.0157), val_loss 0.0190 (0.0208)
Epoch 38, Current LR: 0.00014416052942640132
the pred acc is: 11.718569774913679
epoch [38/50], train_loss 0.0190 (0.0155), val_loss 0.0184 (0.0203)
Epoch 39, Current LR: 0.00012359594482598432
the pred acc is: 12.179930775608296
epoch [39/50], train_loss 0.0187 (0.0152), val_loss 0.0185 (0.0203)
Epoch 40, Current LR: 0.00010453658778440102
the pred acc is: 12.110726609059546
epoch [40/50], train_loss 0.0183 (0.0149), val_loss 0.0183 (0.0202)
Epoch 41, Current LR: 8.70576768765026e-05
the pred acc is: 12.0184544060607
epoch [41/50], train_loss 0.0180 (0.0147), val_loss 0.0182 (0.0201)
Epoch 42, Current LR: 7.12281933782875e-05
the pred acc is: 12.50288351470349
epoch [42/50], train_loss 0.0178 (0.0145), val_loss 0.0184 (0.0199)
Epoch 43, Current LR: 5.71106090293204e-05
the pred acc is: 13.63321796316039
epoch [43/50], train_loss 0.0177 (0.0143), val_loss 0.0183 (0.0198)
Epoch 44, Current LR: 4.4760639485315563e-05
the pred acc is: 13.379469428892916
epoch [44/50], train_loss 0.0174 (0.0141), val_loss 0.0181 (0.0197)
Epoch 45, Current LR: 3.422702443389899e-05
the pred acc is: 13.310265266964043
epoch [45/50], train_loss 0.0172 (0.0139), val_loss 0.0182 (0.0196)
Epoch 46, Current LR: 2.5551335241327665e-05
the pred acc is: 12.502883490724134
epoch [46/50], train_loss 0.0172 (0.0138), val_loss 0.0179 (0.0195)
Epoch 47, Current LR: 1.876781088929908e-05
the pred acc is: 12.66435984135224
epoch [47/50], train_loss 0.0171 (0.0136), val_loss 0.0179 (0.0194)
Epoch 48, Current LR: 1.3903222849333505e-05
the pred acc is: 13.01038063625701
epoch [48/50], train_loss 0.0168 (0.0135), val_loss 0.0177 (0.0194)
Epoch 49, Current LR: 1.0976769428005579e-05
the pred acc is: 13.033448643667887
epoch [49/50], train_loss 0.0169 (0.0134), val_loss 0.0176 (0.0193)
Epoch 50, Current LR: 1e-05
No valid Checkpoint Found!
new_saves/cifar10/None_infocons_sgm_lg1_thre0.125/pretrain_False_lambd_0_noise_0.01_epoch_240_bottleneck_noRELU_C8S1_log_1_ATstrength_0.3_lr_0.05_varthres_0.125/checkpoint_f_best.tar
the pred acc is: 13.284226878366745
epoch [50/50], train_loss 0.0169 (0.0134), val_loss 0.0179 (0.0193)
Best Validation Loss is 0.016760943457484245
MSE Loss on ALL Image is 0.0191 (Real Attack Results on the Target Client)
SSIM Loss on ALL Image is 0.6038 (Real Attack Results on the Target Client)
PSNR Loss on ALL Image is 17.1937 (Real Attack Results on the Target Client)
run the attack for inference time
2.0 torch.Size([1, 8, 16, 16])
Epoch 1, Current LR: 0.0009990232305719944
the pred acc is: 0.23094688221709006
epoch [1/50], train_loss 0.0501 (0.0629), val_loss 0.0476 (0.0482)
Epoch 2, Current LR: 0.0009960967771506664
the pred acc is: 2.309468838689509
epoch [2/50], train_loss 0.0356 (0.0410), val_loss 0.0338 (0.0345)
Epoch 3, Current LR: 0.0009912321891107007
the pred acc is: 2.3094688117091153
epoch [3/50], train_loss 0.0272 (0.0317), val_loss 0.0295 (0.0302)
Epoch 4, Current LR: 0.0009844486647586721
the pred acc is: 2.771362549162902
epoch [4/50], train_loss 0.0223 (0.0278), val_loss 0.0283 (0.0291)
Epoch 5, Current LR: 0.0009757729755661009
the pred acc is: 2.5404156939262057
epoch [5/50], train_loss 0.0188 (0.0253), val_loss 0.0300 (0.0303)
Epoch 6, Current LR: 0.0009652393605146842
the pred acc is: 3.695150121530264
epoch [6/50], train_loss 0.0176 (0.0236), val_loss 0.0275 (0.0285)
Epoch 7, Current LR: 0.0009528893909706795
the pred acc is: 3.695150121530264
epoch [7/50], train_loss 0.0182 (0.0228), val_loss 0.0273 (0.0281)
Epoch 8, Current LR: 0.0009387718066217122
the pred acc is: 4.157043842465442
epoch [8/50], train_loss 0.0184 (0.0224), val_loss 0.0281 (0.0287)
Epoch 9, Current LR: 0.0009229423231234972
the pred acc is: 2.5404157374252074
epoch [9/50], train_loss 0.0157 (0.0209), val_loss 0.0277 (0.0285)
Epoch 10, Current LR: 0.0009054634122155987
the pred acc is: 2.7713625761432956
epoch [10/50], train_loss 0.0152 (0.0194), val_loss 0.0289 (0.0290)
Epoch 11, Current LR: 0.0008864040551740153
the pred acc is: 4.387990768181535
epoch [11/50], train_loss 0.0146 (0.0190), val_loss 0.0278 (0.0281)
Epoch 12, Current LR: 0.0008658394705735984
the pred acc is: 4.157043842465442
epoch [12/50], train_loss 0.0149 (0.0190), val_loss 0.0279 (0.0280)
Epoch 13, Current LR: 0.0008438508174347006
the pred acc is: 4.849884451674534
epoch [13/50], train_loss 0.0147 (0.0182), val_loss 0.0278 (0.0283)
Epoch 14, Current LR: 0.000820524874925601
the pred acc is: 3.9260970037473544
epoch [14/50], train_loss 0.0138 (0.0177), val_loss 0.0261 (0.0271)
Epoch 15, Current LR: 0.000795953699884774
the pred acc is: 3.464203222794566
epoch [15/50], train_loss 0.0150 (0.0178), val_loss 0.0283 (0.0282)
Epoch 16, Current LR: 0.000770234263514603
the pred acc is: 4.387990811680536
epoch [16/50], train_loss 0.0137 (0.0172), val_loss 0.0287 (0.0290)
Epoch 17, Current LR: 0.0007434680686803488
the pred acc is: 4.618937623418231
epoch [17/50], train_loss 0.0133 (0.0171), val_loss 0.0258 (0.0266)
Epoch 18, Current LR: 0.0007157607493247108
the pred acc is: 5.773671980542894
epoch [18/50], train_loss 0.0128 (0.0162), val_loss 0.0264 (0.0275)
Epoch 19, Current LR: 0.0006872216535789154
the pred acc is: 5.77367205102229
epoch [19/50], train_loss 0.0130 (0.0162), val_loss 0.0268 (0.0281)
Epoch 20, Current LR: 0.0006579634122155987
the pred acc is: 5.311778216108714
epoch [20/50], train_loss 0.0126 (0.0163), val_loss 0.0271 (0.0286)
Epoch 21, Current LR: 0.0006281014941466028
the pred acc is: 3.464203239313174
epoch [21/50], train_loss 0.0125 (0.0162), val_loss 0.0278 (0.0285)
Epoch 22, Current LR: 0.0005977537507199335
the pred acc is: 5.080831404371019
epoch [22/50], train_loss 0.0129 (0.0158), val_loss 0.0265 (0.0276)
Epoch 23, Current LR: 0.0005670399506143305
the pred acc is: 4.849884522153929
epoch [23/50], train_loss 0.0123 (0.0151), val_loss 0.0288 (0.0290)
Epoch 24, Current LR: 0.0005360813071670099
the pred acc is: 4.618937569457444
epoch [24/50], train_loss 0.0113 (0.0150), val_loss 0.0283 (0.0281)
Epoch 25, Current LR: 0.0005049999999999998
the pred acc is: 4.618937569457444
epoch [25/50], train_loss 0.0108 (0.0147), val_loss 0.0266 (0.0273)
Epoch 26, Current LR: 0.0004739186928329897
the pred acc is: 6.466512768152955
epoch [26/50], train_loss 0.0107 (0.0138), val_loss 0.0268 (0.0273)
Epoch 27, Current LR: 0.0004429600493856692
the pred acc is: 3.926097047246356
epoch [27/50], train_loss 0.0096 (0.0130), val_loss 0.0274 (0.0276)
Epoch 28, Current LR: 0.0004122462492800661
the pred acc is: 4.618937639936839
epoch [28/50], train_loss 0.0095 (0.0128), val_loss 0.0267 (0.0273)
Epoch 29, Current LR: 0.0003818985058533967
the pred acc is: 4.849884576114716
epoch [29/50], train_loss 0.0094 (0.0126), val_loss 0.0265 (0.0271)
Epoch 30, Current LR: 0.00035203658778440103
the pred acc is: 3.926097047246356
epoch [30/50], train_loss 0.0092 (0.0125), val_loss 0.0283 (0.0282)
Epoch 31, Current LR: 0.00032277834642108444
the pred acc is: 4.849884489116713
epoch [31/50], train_loss 0.0091 (0.0124), val_loss 0.0268 (0.0273)
Epoch 32, Current LR: 0.0002942392506752889
the pred acc is: 5.311778297049894
epoch [32/50], train_loss 0.0091 (0.0123), val_loss 0.0266 (0.0275)
Epoch 33, Current LR: 0.00026653193131965077
the pred acc is: 6.23556581545647
epoch [33/50], train_loss 0.0086 (0.0120), val_loss 0.0265 (0.0275)
Epoch 34, Current LR: 0.00023976573648539642
the pred acc is: 4.387990795161928
epoch [34/50], train_loss 0.0084 (0.0117), val_loss 0.0273 (0.0278)
Epoch 35, Current LR: 0.00021404630011522574
the pred acc is: 5.77367205102229
epoch [35/50], train_loss 0.0081 (0.0114), val_loss 0.0270 (0.0277)
Epoch 36, Current LR: 0.00018947512507439847
the pred acc is: 6.697459525929863
epoch [36/50], train_loss 0.0081 (0.0113), val_loss 0.0263 (0.0271)
Epoch 37, Current LR: 0.000166149182565299
the pred acc is: 5.0808313338916244
epoch [37/50], train_loss 0.0080 (0.0111), val_loss 0.0267 (0.0275)
Epoch 38, Current LR: 0.00014416052942640132
the pred acc is: 6.235565744977074
epoch [38/50], train_loss 0.0079 (0.0111), val_loss 0.0276 (0.0285)
Epoch 39, Current LR: 0.00012359594482598432
the pred acc is: 7.852193883054526
epoch [39/50], train_loss 0.0079 (0.0110), val_loss 0.0285 (0.0291)
Epoch 40, Current LR: 0.00010453658778440102
the pred acc is: 6.00461893323938
epoch [40/50], train_loss 0.0074 (0.0109), val_loss 0.0281 (0.0288)
Epoch 41, Current LR: 8.70576768765026e-05
the pred acc is: 5.773672104983077
epoch [41/50], train_loss 0.0073 (0.0107), val_loss 0.0270 (0.0278)
Epoch 42, Current LR: 7.12281933782875e-05
the pred acc is: 6.004618987200167
epoch [42/50], train_loss 0.0072 (0.0103), val_loss 0.0270 (0.0277)
Epoch 43, Current LR: 5.71106090293204e-05
the pred acc is: 6.697459525929863
epoch [43/50], train_loss 0.0068 (0.0098), val_loss 0.0276 (0.0281)
Epoch 44, Current LR: 4.4760639485315563e-05
the pred acc is: 8.775981482402281
epoch [44/50], train_loss 0.0065 (0.0096), val_loss 0.0273 (0.0280)
Epoch 45, Current LR: 3.422702443389899e-05
the pred acc is: 7.390300172581133
epoch [45/50], train_loss 0.0064 (0.0094), val_loss 0.0275 (0.0280)
Epoch 46, Current LR: 2.5551335241327665e-05
the pred acc is: 8.083140835751012
epoch [46/50], train_loss 0.0064 (0.0093), val_loss 0.0271 (0.0278)
Epoch 47, Current LR: 1.876781088929908e-05
the pred acc is: 7.159353236403256
epoch [47/50], train_loss 0.0063 (0.0092), val_loss 0.0271 (0.0278)
Epoch 48, Current LR: 1.3903222849333505e-05
the pred acc is: 7.390300297021315
epoch [48/50], train_loss 0.0063 (0.0092), val_loss 0.0270 (0.0278)
Epoch 49, Current LR: 1.0976769428005579e-05
the pred acc is: 7.621247125277618
epoch [49/50], train_loss 0.0063 (0.0091), val_loss 0.0273 (0.0279)
Epoch 50, Current LR: 1e-05
No valid Checkpoint Found!
new_saves/cifar10/None_infocons_sgm_lg1_thre0.125/pretrain_False_lambd_0_noise_0.01_epoch_240_bottleneck_noRELU_C8S1_log_1_ATstrength_0.3_lr_0.05_varthres_0.125/checkpoint_f_best.tar
the pred acc is: 7.89538980651235
epoch [50/50], train_loss 0.0063 (0.0091), val_loss 0.0276 (0.0280)
Best Validation Loss is 0.025801587849855423
MSE Loss on ALL Image is 0.0257 (Real Attack Results on the Target Client)
SSIM Loss on ALL Image is 0.5317 (Real Attack Results on the Target Client)
PSNR Loss on ALL Image is 15.9095 (Real Attack Results on the Target Client)
== res_normN8C64 Training-based MIA performance Score with optimizer Adam, lr 0.001, loss type MSE on best epoch saved model ==
Reverse Intermediate activation at layer -1 (-1 is the smashed-data)
The tested model is: best
MIA performance Score training time is (MSE, SSIM, PSNR) averaging 1 times
0.019137082786880425, 0.6038253703194368, 17.193673333384723
MIA performance Score inference time is (MSE, SSIM, PSNR): 0.02567, 0.53168, 15.91